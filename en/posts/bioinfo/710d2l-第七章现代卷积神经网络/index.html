<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">

<link rel="icon" href="/favicon.ico" type="image/x-icon"> 
<title>D2L--第七章现代卷积神经网络 | Li&#39;s Bioinfo-Blog</title>
<meta name="keywords" content="深度学习, D2L">
<meta name="description" content="1. 深度卷积神经网络(AlexNet)
1.1 学习表征
LeNet提出后，卷积神经网络并未占据主流，而是往往由其它机器学习方法所超越，如SVM。一个主要的原因是输入数据的特征处理上。">
<meta name="author" content="Lishensuo">
<link rel="canonical" href="https://lishensuo.github.io/en/posts/bioinfo/710d2l-%E7%AC%AC%E4%B8%83%E7%AB%A0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.9e4de5e3ba61ea358168341aa7cdf70abfaafb7c697dfe8624af3ddff9a35c2f.css" integrity="sha256-nk3l47ph6jWBaDQap833Cr&#43;q&#43;3xpff6GJK893/mjXC8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js" integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://lishensuo.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://lishensuo.github.io/Q.gif">
<link rel="mask-icon" href="https://lishensuo.github.io/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lishensuo.github.io/en/posts/bioinfo/710d2l-%E7%AC%AC%E4%B8%83%E7%AB%A0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="D2L--第七章现代卷积神经网络" />
<meta property="og:description" content="1. 深度卷积神经网络(AlexNet)
1.1 学习表征
LeNet提出后，卷积神经网络并未占据主流，而是往往由其它机器学习方法所超越，如SVM。一个主要的原因是输入数据的特征处理上。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lishensuo.github.io/en/posts/bioinfo/710d2l-%E7%AC%AC%E4%B8%83%E7%AB%A0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-04T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-08-04T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="D2L--第七章现代卷积神经网络"/>
<meta name="twitter:description" content="1. 深度卷积神经网络(AlexNet)
1.1 学习表征
LeNet提出后，卷积神经网络并未占据主流，而是往往由其它机器学习方法所超越，如SVM。一个主要的原因是输入数据的特征处理上。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "分类",
      "item": "https://lishensuo.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📖 生信数据分析--分析流程，工具包等",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "D2L--第七章现代卷积神经网络",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/710d2l-%E7%AC%AC%E4%B8%83%E7%AB%A0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "D2L--第七章现代卷积神经网络",
  "name": "D2L--第七章现代卷积神经网络",
  "description": "1. 深度卷积神经网络(AlexNet) 1.1 学习表征 LeNet提出后，卷积神经网络并未占据主流，而是往往由其它机器学习方法所超越，如SVM。一个主要的原因是输入数据的特征处理上。\n",
  "keywords": [
    "深度学习", "D2L"
  ],
  "articleBody": "1. 深度卷积神经网络(AlexNet) 1.1 学习表征 LeNet提出后，卷积神经网络并未占据主流，而是往往由其它机器学习方法所超越，如SVM。一个主要的原因是输入数据的特征处理上。\nCNN模型是基于端到端的预测，由模型本身来学习、提取特征。例如直接从图片像素到分类结果的预测； SVM等经典机器学习模型则依赖于精细的特征工程，即使用经过人的手工精心设计的特征来建模。 在2012年，AlexNet模型取得了当年ImageNet挑战赛的冠军，标志着深层神经网络相关研究的起点。\n在CNN的底层（例如第一层、第二层等）中的每一个通道可以’理解’为对某种模式特征的提取表示，用于更高层的综合学习。\n此外，限制神经网络取得良好性能的因素还包括数据与硬件两方面——\n数据：深度模型需要大量的有标签数据才能显著优于基于凸优化的传统方法（如线性方法和核方法） 2009年，由斯坦福教授李飞飞小组发布了ImageNet数据集，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。 硬件：深度学习模型对计算资源要求很高，需要数百次迭代训练；每次迭代有需要许多线性代数层传递数据。 相比于CPU，GPU用于大量的计算核心，方便并行运算；此外也提供更高的浮点运算性能（FLOPS），并配备有高带宽的显存（VRAM）等优势。 1.2 AlexNet 本书在这里提供的是一个稍微精简版本的AlexNet\nAlexNet与LeNet架构非常相似，在以下方面进行了提升：\n模型设计：由8层组成，包括5个卷积层、3个全连接层 考虑ImageNet图像宽高显著多于MNIST，第1个卷积层的卷积核窗口为11×11，第二个为5×5，往后都是3×3； 卷积输出通道数也是LeNet的10倍以上； 在第1、第2、第5层卷积层后加入最大汇聚层； 两个全连接隐藏层有4096个输出，用于接近1GB的模型参数； 激活函数：使用ReLU激活函数相比于Sigmoid计算更加简单，且更适应多种参数初始化方法。 容量控制：使用Dropout暂退法控制了全连接层的模型复杂度。 如下的模型架构是为Fashion-MNIST数据集修改后的设计，主要体现在第一层卷积层的输入通道数为1。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from torch import nn from d2l import torch as d2l net = nn.Sequential( # 这里使用一个11*11的更大窗口来捕捉对象。 # 同时，步幅为4，以减少输出的高度和宽度。 # 另外，输出通道的数目远大于LeNet nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数 nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 使用三个连续的卷积层和较小的卷积窗口。 # 除了最后的卷积层，输出通道的数量进一步增加。 nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合 nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000 nn.Linear(4096, 10)) 以一个高度和宽度都为224的单通道输入数据为例 1 2 3 4 X = torch.randn(1, 1, 224, 224) for layer in net: X=layer(X) print(layer.__class__.__name__,'output shape:\\t',X.shape) 1.3 读取数据集 为了将Fashion-MNIST数据用于AlexNet模型框架，需要将像素分辨率重新设置为224×224 1 2 batch_size = 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) 1.4 训练AlexNet 1 2 3 4 lr, num_epochs = 0.01, 10 d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.333, train acc 0.879, test acc 0.849 # 5523.8 examples/sec on cuda:0 2 使用块的网络(VGG) AlexNet虽然证明深层网络有效，但未能提供通用的模板指导后续的设计； VGG由牛津大学的视觉几何组于2013年提出，可以简洁地实现更深更窄的网络，在2014年ImageNet中取得优异的表现。 2.1 VGG块 VGG块提出了一种经典的卷积神经网络的组成架构，包括如下：\n多个连续的卷积层 较小的3×3卷积核； 带填充以保持输出分辨率不变； 自定义输出通道。 ReLU激活函数； 步幅为2的2×2最大汇聚层，使得输出高宽减半 代码实现如下，可调整参数包括：\nnum_convs 块包含多少个卷积层； in_channels 输入通道数 out_channels 输出通道数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torch import nn from d2l import torch as d2l def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) 2.2 VGG网络 参考AlexNet，VGG同样可以分为两部分：\n第一部分由多个VGG块组成的卷积层部分； 第二部分由3个的全连接层组成。 原始VGG网络包含如下5个VGG块，共有5个卷积层；结合三个全连接层，因此又称为VGG-11。\n第一个块：1个卷积层，64个输出通道； 第二个块：1个卷积层，128个输出通道； 第三个块：2个卷积层，256个输出通道； 第四个块：2个卷积层，512个输出通道； 第五个块：2个卷积层，512个输出通道； 1 conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) 由于每个块的最后一层都是最大汇聚层，使得输出减半；同时增加输出通道数。这是经典的CNN设计思路。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def vgg(conv_arch): conv_blks = [] in_channels = 1 #Fashion-MNIST通道数为1 # 卷积层部分 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels #前者的输出通道数等于后者的输入通道数 return nn.Sequential( *conv_blks, nn.Flatten(), # 全连接层部分 # 224/2/2/2/2/2 = 7 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10)) net = vgg(conv_arch) X = torch.randn(size=(1, 1, 224, 224)) for blk in net: X = blk(X) print(blk.__class__.__name__,'output shape:\\t',X.shape) 2.3 训练模型 由于VGG-11比AlexNet计算量更大，因此这里构建了一个通道数较少的网络 1 2 3 4 5 6 7 8 9 ratio = 4 small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] net = vgg(small_conv_arch) lr, num_epochs, batch_size = 0.05, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.174, train acc 0.936, test acc 0.908 # 3872.4 examples/sec on cuda:0 3. 网络中的网络(NiN) 在前述介绍的网络组成中，在最后通常会加入全连接层，会导致引入大量的模型参数； 1×1卷积核可以起到混合通道的作用，有点类似MLP的全连接层； NiN网络在AlexNet的基础之上，使用了1×1卷积核，取代了上述全连接层部分的作用。 3.1 NiN块 NiN块提出了一种特殊的组成架构，包括如下：\n第一层为用户自定义的卷积层，以及ReLU激活函数； 第二、三层为1×1卷积核的卷积层，输出通道数不变，同样分别加入ReLU激活函数； 使用1×1卷积核时，相当于对同一位置，不同通道的元素进行全连接层\n代码实现如下：\n1 2 3 4 5 6 7 8 9 10 import torch from torch import nn from d2l import torch as d2l def nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) 3.2 NiN模型 由4个NiN块组成，每个NiN块中的卷积核窗口参考AlexNet设置为11×11，5×5，3×3，3×3 前3个NiN块后接一个最大池化层； 最后一个NiN块的输出通道数等于类别数，且后面接一个全局平均池化层，输出为1×1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 net = nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5), # 标签类别数是10 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1, 1)), # 将四维的输出转成二维的输出，其形状为(批量大小,10) nn.Flatten()) X = torch.rand(size=(1, 1, 224, 224)) for layer in net: X = layer(X) print(layer.__class__.__name__,'output shape:\\t', X.shape) 3.3 训练模型 1 2 3 4 5 6 lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.341, train acc 0.873, test acc 0.875 # 4756.1 examples/sec on cuda:0 4. 含并行连接的网络(GoogLeNet) 基于NiN中串联网络的思想，GoogLeNet在2014年ImageNet图像识别挑战赛的取得佳绩； 4.1 Inception块 如下图，Inception块由4条并行的路径组成，每个路径仅改变通道数，不改变高宽。\n第一条：1×1卷积 第二条：1×1卷积，加上3×3卷积(填充1) 第三条：1×1卷积，加上5×5卷积(填充2) 第四条：3×3最大汇聚(填充1)，加上1×1卷积 如下是定义Inception块的代码，其中可调参数均是输入以及每条路径中的通道数。\nin_channels 输入通道数; c1 第一条路径的输出通道数; c2 第二条路径每个卷积层的输出通道数; c3 第三条路径每个卷积层的输出通道数; c4 第四条路径每个卷积层的输出通道数; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l class Inception(nn.Module): # c1--c4是每条路径的输出通道数 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # 线路1，单1x1卷积层 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # 线路2，1x1卷积层后接3x3卷积层 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # 线路3，1x1卷积层后接5x5卷积层 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # 线路4，3x3最大汇聚层后接1x1卷积层 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # 在通道维度上连结输出 return torch.cat((p1, p2, p3, p4), dim=1) 4.2 GoogLeNet模型 如下示意图，GoogLeNet模型共有5个部分组成(从下到上)：\n第一部分：类似于AlexNet，一个卷积层加上一个最大汇聚层； 1 2 3 b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) 第二部分：两个卷积层加上一个最大汇聚层 1 2 3 4 5 b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) 第三部分：2个Inception块，加上最大汇聚层 e.g. 第一个卷积层：输入通道=192，输出通道=64+128+32+32=256 1 2 3 b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32), Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) 第四部分：5个Inception块，加上最大汇聚层 e.g. 第一个卷积层：输入通道=480，输出通道=192+208+48+64=512 1 2 3 4 5 6 b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64), Inception(512, 160, (112, 224), (24, 64), 64), Inception(512, 128, (128, 256), (24, 64), 64), Inception(512, 112, (144, 288), (32, 64), 64), Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) 第五部分：2个Inception块，加上全局平均汇聚层以及全连接层 e.g. 最后一层卷积层：输入通道=832，输出通道=384+384+128+128=1024 全局平均汇聚层以及Flatten将输出变为1024的特征向量，后面再根据类别数接一个全连接层 1 2 3 4 5 6 b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128), Inception(832, 384, (192, 384), (48, 128), 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()) net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10)) 查看模型架构（仍是高宽变小，通道数变多的思想） 1 2 3 4 X = torch.rand(size=(1, 1, 96, 96)) for layer in net: X = layer(X) print(layer.__class__.__name__,'output shape:\\t', X.shape) 4.3 训练模型 1 2 3 4 5 6 lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.249, train acc 0.905, test acc 0.827 # 3811.4 examples/sec on cuda:0 5. 批量规范化 加速深层神经网络的收敛 5.1 训练深层网络 批量规范化(Batch Normalization, BN)用于对特定神经网络层中，每次训练迭代的小批量输入数据进行’归一化’处理。\n这使得不同层之间的参数量级得到统一，防止模型参数的更新是为了补偿不同层之间的数据差异，从而针对性的对预测问题本身进行学习，加速模型收敛。\n具体实现其实也并不复杂：\n（1）首先对一个小批量B，计算其（feature）均值与方差\n（2）然后，进行均值为0，方差为1的归一化处理后，进一步进行拉伸与偏移。γ可以变换方差，β可以变换均值，均属于可学习的参数，从而拟合最适合的规范化分布。\n一种角度的解释是BN操作中会引入一定的噪声，控制了模型复杂度。因为随机抽样的小批量分布不能代表总体情况。\n5.2 批量规范化层 对于全连接层： BN操作位于全连接层与激活函数之间； 对于[小批量数，特征数]的输入数据，会对每一列特征进行BN操作。即每个特征学习的γ与β都是不同的； 对于卷积层 BN操作位于卷积层与激活函数之间； 对于多通道输出，会将每个通道作为一个特征，即计算小批量样本对于特定通道的，所有元素的均值与方差。 此外，BN操作在预测过程是估算特征在整个训练数据集的均值与方差，再进行规范化。 5.3 从零实现 首先定义一个函数，进行BN操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from torch import nn from d2l import torch as d2l def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式 if not torch.is_grad_enabled(): # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差 X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # 使用全连接层的情况，计算特征维上的均值和方差 mean = X.mean(dim=0) var = ((X - mean) ** 2).mean(dim=0) else: # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。 # 这里我们需要保持X的形状以便后面可以做广播运算 mean = X.mean(dim=(0, 2, 3), keepdim=True) var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # 训练模式下，用当前的均值和方差做标准化 X_hat = (X - mean) / torch.sqrt(var + eps) # 更新移动平均的均值和方差 moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # 缩放和移位 return Y, moving_mean.data, moving_var.data 然后定义一个BatchNorm层 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class BatchNorm(nn.Module): # num_features：完全连接层的输出数量或卷积层的输出通道数。 # num_dims：2表示完全连接层，4表示卷积层 def __init__(self, num_features, num_dims): super().__init__() if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0 self.gamma = nn.Parameter(torch.ones(shape)) self.beta = nn.Parameter(torch.zeros(shape)) # 非模型参数的变量初始化为0和1 self.moving_mean = torch.zeros(shape) self.moving_var = torch.ones(shape) def forward(self, X): # 如果X不在内存上，将moving_mean和moving_var # 复制到X所在显存上 if self.moving_mean.device != X.device: self.moving_mean = self.moving_mean.to(X.device) self.moving_var = self.moving_var.to(X.device) # 保存更新过的moving_mean和moving_var Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Y 5.4 使用批量规范化层的LeNet 定义模型 1 2 3 4 5 6 7 8 net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(), nn.Linear(84, 10)) 训练模型 1 2 3 4 5 6 lr, num_epochs, batch_size = 1.0, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.272, train acc 0.899, test acc 0.863 # 23764.9 examples/sec on cuda:0 查看模型BN层的γ与β参数 1 2 3 4 5 net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,)) # (tensor([3.3490, 3.2102, 4.1508, 1.7645, 2.7210, 0.5482], device='cuda:0', # grad_fn=), # tensor([ 0.3007, 2.3391, 4.3099, -0.4318, -0.8200, -0.5383], device='cuda:0', # grad_fn=)) 6. 残差网络(ResNet) 残差网络主要由何凯明等人提出，在2015年ImageNet挑战赛中取得了冠军。 6.1 函数类 （1）非嵌套函数类\n新模型 = 新添加的层 ← 原模型 在先前学习的深度神经网络中，每个神经网络层/块基本独立非嵌套关系，即前者的输出直接作为后者的输入。对于复杂的模型架构，有时新添加的层并不能使模型接近最优解，甚至可能更糟； （2）嵌套函数类\n新模型 = 新添加的层 + 原模型 如果新添加的层效果不明显，新模型仍然有机会基于原模型更新梯度。即新模型可能得出更优的解来拟合数据集（至少不会变差）。 6.2 残差块 如下左图，为一个正常的神经网络架构，若输出为f(x)。 由输入x，经神经网络层映射，得到输出f(x) 如下右图，为一个残差块架构，若输出为f(x)。 由输入x，经神经网络层与原数据共同组成f(x)。 ResNet网络中残差块共有两种，区别在于原模型数据的输入： 一种是原模型的x直接输入 另一种是原模型的x经1×1卷积层后再输入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l # input_channels 输入的通道 # num_channels 输出的通道 # use_1x1conv参数声明是否考虑对x进行1×1卷积 # strides 对于第一层卷积以及1×1卷积进行高宽缩减 class Residual(nn.Module): #@save def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y) 查看示例输出 1 2 3 4 5 6 7 8 9 10 11 # 输入通道为3，输出通道为3 blk = Residual(3,3) X = torch.rand(4, 3, 6, 6) # 4个样本 Y = blk(X) Y.shape # torch.Size([4, 3, 6, 6]) # 输入通道为3，输出通道为6，高宽减半（必须调用1×1卷积） blk = Residual(3,6, use_1x1conv=True, strides=2) blk(X).shape # torch.Size([4, 6, 3, 3]) 6.3 ResNet模型 ResNet模型由如下部分组成\n第1部分：卷积层+BN+最大汇聚层，64通道输出，高宽降低四倍 1 2 3 b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) 第2~5部分由4个block组成，每个block包含两个2残差块 除了第1个Block以外的第1个残差块都需要将高宽减半，通道数加倍 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def resnet_block(input_channels, num_channels, num_residuals, first_block=False): blk = [] for i in range(num_residuals): if i == 0 and not first_block: blk.append(Residual(input_channels, num_channels, #输出通道数是输入的2倍 use_1x1conv=True, strides=2)) else: blk.append(Residual(num_channels, num_channels)) #输入输出通道数相同 return blk b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True)) b3 = nn.Sequential(*resnet_block(64, 128, 2)) b4 = nn.Sequential(*resnet_block(128, 256, 2)) b5 = nn.Sequential(*resnet_block(256, 512, 2)) 最后的第6部分连接全局平均汇聚层以及全连接输出层 1 2 3 net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10)) 4个Block包含共包含8个残差块，共16个卷积层，加上第一个卷积层以及最后一个全连接层，共有18层，因此又称为ResNet-18。\n查看示例形状输出 1 2 3 4 X = torch.rand(size=(1, 1, 224, 224)) for layer in net: X = layer(X) print(layer.__class__.__name__,'output shape:\\t', X.shape) 6.4 训练模型 1 2 3 4 5 6 lr, num_epochs, batch_size = 0.05, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.010, train acc 0.998, test acc 0.904 # 10662.5 examples/sec on cuda:0 7. 稠密连接网络(DenseNet) DenseNet相当于是ResNet的逻辑扩展。 7.1 从ResNet到DenseNet ResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[,]表示）而不是如ResNet的简单相加。 稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。 7.2 稠密块 基本的“批量规范化、激活和卷积”架构 1 2 3 4 5 6 7 8 import torch from torch import nn from d2l import torch as d2l def conv_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)) #高宽不变 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道； 在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # num_convs 卷积层数, # input_channels 输入通道数, # num_channels 输出通道数 class DenseBlock(nn.Module): def __init__(self, num_convs, input_channels, num_channels): super(DenseBlock, self).__init__() layer = [] for i in range(num_convs): # 每多一个卷积层，其输入通道数不断增加 layer.append(conv_block( num_channels * i + input_channels, num_channels)) self.net = nn.Sequential(*layer) def forward(self, X): for blk in self.net: Y = blk(X) # 连接通道维度上每个块的输入和输出，即通道数增加 X = torch.cat((X, Y), dim=1) return X 示例展示：第一层输入通道数为3，第二层输入通道数是10+3，第二次输出通道数是10 + 10 + 3 1 2 3 4 5 blk = DenseBlock(2, 3, 10) X = torch.randn(4, 3, 8, 8) Y = blk(X) Y.shape # torch.Size([4, 23, 8, 8]) 7.3 过渡层 上述稠密层会导致通道数的不断累加、增多 过渡层用来控制模型复杂度，通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽。 1 2 3 4 5 6 7 8 9 def transition_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=1), nn.AvgPool2d(kernel_size=2, stride=2)) blk = transition_block(23, 10) blk(Y).shape # torch.Size([4, 10, 4, 4]) 7.4 DenseNet模型 架构基本类似ResNet。\n第一部分由单卷积层和最大汇聚层组成 1 2 3 4 b1 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) 后面再接4个稠密快，每个块由4个卷积层组成 1 2 3 4 5 6 7 8 9 10 11 12 # num_channels为当前的通道数 num_channels, growth_rate = 64, 32 num_convs_in_dense_blocks = [4, 4, 4, 4] blks = [] for i, num_convs in enumerate(num_convs_in_dense_blocks): blks.append(DenseBlock(num_convs, num_channels, growth_rate)) # 上一个稠密块的输出通道数 num_channels += num_convs * growth_rate # 在稠密块之间添加一个转换层，使通道数量减半 if i != len(num_convs_in_dense_blocks) - 1: blks.append(transition_block(num_channels, num_channels // 2)) num_channels = num_channels // 2 最后接上全局汇聚层和全连接层来输出结果。 1 2 3 4 5 6 net = nn.Sequential( b1, *blks, nn.BatchNorm2d(num_channels), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(num_channels, 10)) 7.5 训练模型 1 2 3 4 5 6 lr, num_epochs, batch_size = 0.1, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.140, train acc 0.949, test acc 0.887 # 8501.4 examples/sec on cuda:0 ",
  "wordCount" : "7080",
  "inLanguage": "en",
  "datePublished": "2024-08-04T00:00:00Z",
  "dateModified": "2024-08-04T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Lishensuo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lishensuo.github.io/en/posts/bioinfo/710d2l-%E7%AC%AC%E4%B8%83%E7%AB%A0%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Li's Bioinfo-Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lishensuo.github.io/img/Q.gif"
    }
  }
}
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lishensuo.github.io/en/" accesskey="h" title="Li&#39;s Bioinfo-Blog (Alt + H)">Li&#39;s Bioinfo-Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lishensuo.github.io/en/" title="主页">
                    <span>主页</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/posts" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/tags" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/about" title="关于">
                    <span>关于</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/search" title="搜索 (Alt &#43; /)" accesskey=/>
                    <span>搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lishensuo.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/">分类</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/bioinfo/">📖 生信数据分析--分析流程，工具包等</a></div>
    <h1 class="post-title">
      D2L--第七章现代卷积神经网络
    </h1>
    <div class="post-meta">













Create:&amp;nbsp;&lt;span title=&#39;2024-08-04 00:00:00 &#43;0000 UTC&#39;&gt;2024-08-04&lt;/span&gt;&amp;nbsp;|&amp;nbsp;Update:&amp;nbsp;2024-08-04&amp;nbsp;|&amp;nbsp;Words:&amp;nbsp;7080&amp;nbsp;|&amp;nbsp;15 min&amp;nbsp;|&amp;nbsp;Lishensuo

|  Viewers: <span id="busuanzi_value_page_pv"></span> 
	  
    </div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-%e6%b7%b1%e5%ba%a6%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9calexnet" aria-label="1. 深度卷积神经网络(AlexNet)">1. 深度卷积神经网络(AlexNet)</a><ul>
                            
                    <li>
                        <a href="#11-%e5%ad%a6%e4%b9%a0%e8%a1%a8%e5%be%81" aria-label="1.1 学习表征">1.1 学习表征</a></li>
                    <li>
                        <a href="#12-alexnet" aria-label="1.2 AlexNet">1.2 AlexNet</a></li>
                    <li>
                        <a href="#13-%e8%af%bb%e5%8f%96%e6%95%b0%e6%8d%ae%e9%9b%86" aria-label="1.3 读取数据集">1.3 读取数据集</a></li>
                    <li>
                        <a href="#14-%e8%ae%ad%e7%bb%83alexnet" aria-label="1.4 训练AlexNet">1.4 训练AlexNet</a></li></ul>
                    </li>
                    <li>
                        <a href="#2-%e4%bd%bf%e7%94%a8%e5%9d%97%e7%9a%84%e7%bd%91%e7%bb%9cvgg" aria-label="2 使用块的网络(VGG)">2 使用块的网络(VGG)</a><ul>
                            
                    <li>
                        <a href="#21-vgg%e5%9d%97" aria-label="2.1 VGG块">2.1 VGG块</a></li>
                    <li>
                        <a href="#22-vgg%e7%bd%91%e7%bb%9c" aria-label="2.2 VGG网络">2.2 VGG网络</a></li>
                    <li>
                        <a href="#23-%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b" aria-label="2.3 训练模型">2.3 训练模型</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e7%bd%91%e7%bb%9cnin" aria-label="3. 网络中的网络(NiN)">3. 网络中的网络(NiN)</a><ul>
                            
                    <li>
                        <a href="#31-nin%e5%9d%97" aria-label="3.1 NiN块">3.1 NiN块</a></li>
                    <li>
                        <a href="#32-nin%e6%a8%a1%e5%9e%8b" aria-label="3.2 NiN模型">3.2 NiN模型</a></li>
                    <li>
                        <a href="#33-%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b" aria-label="3.3 训练模型">3.3 训练模型</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-%e5%90%ab%e5%b9%b6%e8%a1%8c%e8%bf%9e%e6%8e%a5%e7%9a%84%e7%bd%91%e7%bb%9cgooglenet" aria-label="4. 含并行连接的网络(GoogLeNet)">4. 含并行连接的网络(GoogLeNet)</a><ul>
                            
                    <li>
                        <a href="#41-inception%e5%9d%97" aria-label="4.1 Inception块">4.1 Inception块</a></li>
                    <li>
                        <a href="#42-googlenet%e6%a8%a1%e5%9e%8b" aria-label="4.2 GoogLeNet模型">4.2 GoogLeNet模型</a></li>
                    <li>
                        <a href="#43-%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b" aria-label="4.3 训练模型">4.3 训练模型</a></li></ul>
                    </li>
                    <li>
                        <a href="#5-%e6%89%b9%e9%87%8f%e8%a7%84%e8%8c%83%e5%8c%96" aria-label="5. 批量规范化">5. 批量规范化</a><ul>
                            
                    <li>
                        <a href="#51-%e8%ae%ad%e7%bb%83%e6%b7%b1%e5%b1%82%e7%bd%91%e7%bb%9c" aria-label="5.1 训练深层网络">5.1 训练深层网络</a></li>
                    <li>
                        <a href="#52-%e6%89%b9%e9%87%8f%e8%a7%84%e8%8c%83%e5%8c%96%e5%b1%82" aria-label="5.2 批量规范化层">5.2 批量规范化层</a></li>
                    <li>
                        <a href="#53-%e4%bb%8e%e9%9b%b6%e5%ae%9e%e7%8e%b0" aria-label="5.3 从零实现">5.3 从零实现</a></li>
                    <li>
                        <a href="#54-%e4%bd%bf%e7%94%a8%e6%89%b9%e9%87%8f%e8%a7%84%e8%8c%83%e5%8c%96%e5%b1%82%e7%9a%84lenet" aria-label="5.4 使用批量规范化层的LeNet">5.4 使用批量规范化层的LeNet</a></li></ul>
                    </li>
                    <li>
                        <a href="#6-%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9cresnet" aria-label="6. 残差网络(ResNet)">6. 残差网络(ResNet)</a><ul>
                            
                    <li>
                        <a href="#61-%e5%87%bd%e6%95%b0%e7%b1%bb" aria-label="6.1 函数类">6.1 函数类</a></li>
                    <li>
                        <a href="#62-%e6%ae%8b%e5%b7%ae%e5%9d%97" aria-label="6.2 残差块">6.2 残差块</a></li>
                    <li>
                        <a href="#63-resnet%e6%a8%a1%e5%9e%8b" aria-label="6.3 ResNet模型">6.3 ResNet模型</a></li>
                    <li>
                        <a href="#64-%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b" aria-label="6.4 训练模型">6.4 训练模型</a></li></ul>
                    </li>
                    <li>
                        <a href="#7-%e7%a8%a0%e5%af%86%e8%bf%9e%e6%8e%a5%e7%bd%91%e7%bb%9cdensenet" aria-label="7. 稠密连接网络(DenseNet)">7. 稠密连接网络(DenseNet)</a><ul>
                            
                    <li>
                        <a href="#71-%e4%bb%8eresnet%e5%88%b0densenet" aria-label="7.1 从ResNet到DenseNet">7.1 从ResNet到DenseNet</a></li>
                    <li>
                        <a href="#72-%e7%a8%a0%e5%af%86%e5%9d%97" aria-label="7.2 稠密块">7.2 稠密块</a></li>
                    <li>
                        <a href="#73-%e8%bf%87%e6%b8%a1%e5%b1%82" aria-label="7.3 过渡层">7.3 过渡层</a></li>
                    <li>
                        <a href="#74-densenet%e6%a8%a1%e5%9e%8b" aria-label="7.4 DenseNet模型">7.4 DenseNet模型</a></li>
                    <li>
                        <a href="#75-%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b" aria-label="7.5 训练模型">7.5 训练模型</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>


  <div class="post-content"><h1 id="1-深度卷积神经网络alexnet">1. 深度卷积神经网络(AlexNet)<a hidden class="anchor" aria-hidden="true" href="#1-深度卷积神经网络alexnet">#</a></h1>
<h2 id="11-学习表征">1.1 学习表征<a hidden class="anchor" aria-hidden="true" href="#11-学习表征">#</a></h2>
<p>LeNet提出后，卷积神经网络并未占据主流，而是往往由其它机器学习方法所超越，如SVM。一个主要的原因是输入数据的特征处理上。</p>
<ul>
<li>CNN模型是基于端到端的预测，由模型本身来学习、提取特征。例如直接从图片像素到分类结果的预测；</li>
<li>SVM等经典机器学习模型则依赖于精细的特征工程，即使用经过人的手工精心设计的特征来建模。</li>
</ul>
<p>在2012年，AlexNet模型取得了当年ImageNet挑战赛的冠军，标志着深层神经网络相关研究的起点。</p>
<blockquote>
<p>在CNN的底层（例如第一层、第二层等）中的每一个通道可以&rsquo;理解&rsquo;为对某种模式特征的提取表示，用于更高层的综合学习。</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802120203408.png" alt="image-20240802120203408"  />
</p></blockquote>
<p>此外，限制神经网络取得良好性能的因素还包括<strong>数据与硬件</strong>两方面——</p>
<ul>
<li>数据：深度模型需要大量的有标签数据才能显著优于基于凸优化的传统方法（如线性方法和核方法）
<ul>
<li>2009年，由斯坦福教授李飞飞小组发布了ImageNet数据集，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。</li>
</ul>
</li>
<li>硬件：深度学习模型对计算资源要求很高，需要数百次迭代训练；每次迭代有需要许多线性代数层传递数据。
<ul>
<li>相比于CPU，GPU用于大量的计算核心，方便并行运算；此外也提供更高的浮点运算性能（FLOPS），并配备有高带宽的显存（VRAM）等优势。</li>
</ul>
</li>
</ul>
<h2 id="12-alexnet">1.2 AlexNet<a hidden class="anchor" aria-hidden="true" href="#12-alexnet">#</a></h2>
<blockquote>
<p>本书在这里提供的是一个稍微精简版本的AlexNet</p></blockquote>
<p>AlexNet与LeNet架构非常相似，在以下方面进行了提升：</p>
<ul>
<li>模型设计：由8层组成，包括5个卷积层、3个全连接层
<ul>
<li>考虑ImageNet图像宽高显著多于MNIST，第1个卷积层的卷积核窗口为11×11，第二个为5×5，往后都是3×3；</li>
<li>卷积输出通道数也是LeNet的10倍以上；</li>
<li>在第1、第2、第5层卷积层后加入最大汇聚层；</li>
<li>两个全连接隐藏层有4096个输出，用于接近1GB的模型参数；</li>
</ul>
</li>
<li>激活函数：使用ReLU激活函数相比于Sigmoid计算更加简单，且更适应多种参数初始化方法。</li>
<li>容量控制：使用Dropout暂退法控制了全连接层的模型复杂度。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802122156855.png" alt="image-20240802122156855"  />
</p>
<ul>
<li>如下的模型架构是为Fashion-MNIST数据集修改后的设计，主要体现在第一层卷积层的输入通道数为1。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net = nn.Sequential(
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 这里使用一个11*11的更大窗口来捕捉对象。</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 同时，步幅为4，以减少输出的高度和宽度。</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 另外，输出通道的数目远大于LeNet</span>
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">96</span>, kernel_size=<span style="color:#ff0;font-weight:bold">11</span>, stride=<span style="color:#ff0;font-weight:bold">4</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span>
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">96</span>, <span style="color:#ff0;font-weight:bold">256</span>, kernel_size=<span style="color:#ff0;font-weight:bold">5</span>, padding=<span style="color:#ff0;font-weight:bold">2</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 使用三个连续的卷积层和较小的卷积窗口。</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 除了最后的卷积层，输出通道的数量进一步增加。</span>
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">384</span>, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">384</span>, <span style="color:#ff0;font-weight:bold">384</span>, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">384</span>, <span style="color:#ff0;font-weight:bold">256</span>, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    nn.Flatten(),
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span>
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#ff0;font-weight:bold">6400</span>, <span style="color:#ff0;font-weight:bold">4096</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.Dropout(p=<span style="color:#ff0;font-weight:bold">0.5</span>),
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#ff0;font-weight:bold">4096</span>, <span style="color:#ff0;font-weight:bold">4096</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.Dropout(p=<span style="color:#ff0;font-weight:bold">0.5</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span>
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#ff0;font-weight:bold">4096</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>以一个高度和宽度都为224的单通道输入数据为例</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X = torch.randn(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">224</span>, <span style="color:#ff0;font-weight:bold">224</span>)
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> layer in net:
</span></span><span style="display:flex;"><span>    X=layer(X)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(layer.__class__.__name__,<span style="color:#0ff;font-weight:bold">&#39;output shape:</span><span style="color:#0ff;font-weight:bold">\t</span><span style="color:#0ff;font-weight:bold">&#39;</span>,X.shape)
</span></span></code></pre></td></tr></table>
</div>
</div><img src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802124919501.png" alt="image-20240802124919501" style="zoom: 67%;" />
<h2 id="13-读取数据集">1.3 读取数据集<a hidden class="anchor" aria-hidden="true" href="#13-读取数据集">#</a></h2>
<ul>
<li>为了将Fashion-MNIST数据用于AlexNet模型框架，需要将像素分辨率重新设置为224×224</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch_size = <span style="color:#ff0;font-weight:bold">128</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span style="color:#ff0;font-weight:bold">224</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="14-训练alexnet">1.4 训练AlexNet<a hidden class="anchor" aria-hidden="true" href="#14-训练alexnet">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>lr, num_epochs = <span style="color:#ff0;font-weight:bold">0.01</span>, <span style="color:#ff0;font-weight:bold">10</span>
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.333, train acc 0.879, test acc 0.849</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 5523.8 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="2-使用块的网络vgg">2 使用块的网络(VGG)<a hidden class="anchor" aria-hidden="true" href="#2-使用块的网络vgg">#</a></h1>
<ul>
<li>AlexNet虽然证明深层网络有效，但未能提供通用的模板指导后续的设计；</li>
<li>VGG由牛津大学的视觉几何组于2013年提出，可以简洁地实现更深更窄的网络，在2014年ImageNet中取得优异的表现。</li>
</ul>
<h2 id="21-vgg块">2.1 VGG块<a hidden class="anchor" aria-hidden="true" href="#21-vgg块">#</a></h2>
<p>VGG块提出了一种经典的卷积神经网络的组成架构，包括如下：</p>
<ul>
<li>多个连续的卷积层
<ul>
<li>较小的3×3卷积核；</li>
<li>带填充以保持输出分辨率不变；</li>
<li>自定义输出通道。</li>
</ul>
</li>
<li>ReLU激活函数；</li>
<li>步幅为2的2×2最大汇聚层，使得输出高宽减半</li>
</ul>
<p>代码实现如下，可调整参数包括：</p>
<ul>
<li>num_convs 块包含多少个卷积层；</li>
<li>in_channels 输入通道数</li>
<li>out_channels 输出通道数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> vgg_block(num_convs, in_channels, out_channels):
</span></span><span style="display:flex;"><span>    layers = []
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> _ in <span style="color:#fff;font-weight:bold">range</span>(num_convs):
</span></span><span style="display:flex;"><span>        layers.append(nn.Conv2d(in_channels, out_channels,
</span></span><span style="display:flex;"><span>                                kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>        layers.append(nn.ReLU())
</span></span><span style="display:flex;"><span>        in_channels = out_channels
</span></span><span style="display:flex;"><span>    layers.append(nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">2</span>,stride=<span style="color:#ff0;font-weight:bold">2</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> nn.Sequential(*layers)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="22-vgg网络">2.2 VGG网络<a hidden class="anchor" aria-hidden="true" href="#22-vgg网络">#</a></h2>
<p>参考AlexNet，VGG同样可以分为两部分：</p>
<ul>
<li>第一部分由多个VGG块组成的卷积层部分；</li>
<li>第二部分由3个的全连接层组成。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802135712008.png" alt="image-20240802135712008"  />
</p>
<p>原始VGG网络包含如下5个VGG块，共有5个卷积层；结合三个全连接层，因此又称为VGG-11。</p>
<ul>
<li>第一个块：1个卷积层，64个输出通道；</li>
<li>第二个块：1个卷积层，128个输出通道；</li>
<li>第三个块：2个卷积层，256个输出通道；</li>
<li>第四个块：2个卷积层，512个输出通道；</li>
<li>第五个块：2个卷积层，512个输出通道；</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conv_arch = ((<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">64</span>), (<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">128</span>), (<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">256</span>), (<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">512</span>), (<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">512</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>由于每个块的最后一层都是最大汇聚层，使得输出减半；同时增加输出通道数。这是经典的CNN设计思路。</p></blockquote>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> vgg(conv_arch):
</span></span><span style="display:flex;"><span>    conv_blks = []
</span></span><span style="display:flex;"><span>    in_channels = <span style="color:#ff0;font-weight:bold">1</span>  <span style="color:#007f7f">#Fashion-MNIST通道数为1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 卷积层部分</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> (num_convs, out_channels) in conv_arch:
</span></span><span style="display:flex;"><span>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
</span></span><span style="display:flex;"><span>        in_channels = out_channels <span style="color:#007f7f">#前者的输出通道数等于后者的输入通道数</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> nn.Sequential(
</span></span><span style="display:flex;"><span>        *conv_blks, nn.Flatten(),
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 全连接层部分</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 224/2/2/2/2/2 = 7</span>
</span></span><span style="display:flex;"><span>        nn.Linear(out_channels * <span style="color:#ff0;font-weight:bold">7</span> * <span style="color:#ff0;font-weight:bold">7</span>, <span style="color:#ff0;font-weight:bold">4096</span>), nn.ReLU(), nn.Dropout(<span style="color:#ff0;font-weight:bold">0.5</span>),
</span></span><span style="display:flex;"><span>        nn.Linear(<span style="color:#ff0;font-weight:bold">4096</span>, <span style="color:#ff0;font-weight:bold">4096</span>), nn.ReLU(), nn.Dropout(<span style="color:#ff0;font-weight:bold">0.5</span>),
</span></span><span style="display:flex;"><span>        nn.Linear(<span style="color:#ff0;font-weight:bold">4096</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net = vgg(conv_arch)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X = torch.randn(size=(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">224</span>, <span style="color:#ff0;font-weight:bold">224</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> blk in net:
</span></span><span style="display:flex;"><span>    X = blk(X)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(blk.__class__.__name__,<span style="color:#0ff;font-weight:bold">&#39;output shape:</span><span style="color:#0ff;font-weight:bold">\t</span><span style="color:#0ff;font-weight:bold">&#39;</span>,X.shape)
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802140636464.png" alt="image-20240802140636464"  />
</p>
<h2 id="23-训练模型">2.3 训练模型<a hidden class="anchor" aria-hidden="true" href="#23-训练模型">#</a></h2>
<ul>
<li>由于VGG-11比AlexNet计算量更大，因此这里构建了一个通道数较少的网络</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ratio = <span style="color:#ff0;font-weight:bold">4</span>
</span></span><span style="display:flex;"><span>small_conv_arch = [(pair[<span style="color:#ff0;font-weight:bold">0</span>], pair[<span style="color:#ff0;font-weight:bold">1</span>] // ratio) <span style="color:#fff;font-weight:bold">for</span> pair in conv_arch]
</span></span><span style="display:flex;"><span>net = vgg(small_conv_arch)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lr, num_epochs, batch_size = <span style="color:#ff0;font-weight:bold">0.05</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">128</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span style="color:#ff0;font-weight:bold">224</span>)
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.174, train acc 0.936, test acc 0.908</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 3872.4 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="3-网络中的网络nin">3. 网络中的网络(NiN)<a hidden class="anchor" aria-hidden="true" href="#3-网络中的网络nin">#</a></h1>
<ul>
<li>在前述介绍的网络组成中，在最后通常会加入全连接层，会导致引入大量的模型参数；</li>
<li>1×1卷积核可以起到混合通道的作用，有点类似MLP的全连接层；</li>
<li>NiN网络在AlexNet的基础之上，使用了1×1卷积核，取代了上述全连接层部分的作用。</li>
</ul>
<h2 id="31-nin块">3.1 NiN块<a hidden class="anchor" aria-hidden="true" href="#31-nin块">#</a></h2>
<p>NiN块提出了一种特殊的组成架构，包括如下：</p>
<ul>
<li>第一层为用户自定义的卷积层，以及ReLU激活函数；</li>
<li>第二、三层为1×1卷积核的卷积层，输出通道数不变，同样分别加入ReLU激活函数；</li>
</ul>
<blockquote>
<p>使用1×1卷积核时，相当于对同一位置，不同通道的元素进行全连接层</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802211038993.png" alt="image-20240802211038993"  />
</p></blockquote>
<p>代码实现如下：</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> nin_block(in_channels, out_channels, kernel_size, strides, padding):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> nn.Sequential(
</span></span><span style="display:flex;"><span>        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
</span></span><span style="display:flex;"><span>        nn.ReLU(),
</span></span><span style="display:flex;"><span>        nn.Conv2d(out_channels, out_channels, kernel_size=<span style="color:#ff0;font-weight:bold">1</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>        nn.Conv2d(out_channels, out_channels, kernel_size=<span style="color:#ff0;font-weight:bold">1</span>), nn.ReLU())
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="32-nin模型">3.2 NiN模型<a hidden class="anchor" aria-hidden="true" href="#32-nin模型">#</a></h2>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240802155334846.png" alt="image-20240802155334846"  />
</p>
<ul>
<li>由4个NiN块组成，每个NiN块中的卷积核窗口参考AlexNet设置为11×11，5×5，3×3，3×3</li>
<li>前3个NiN块后接一个最大池化层；</li>
<li>最后一个NiN块的输出通道数等于类别数，且后面接一个全局平均池化层，输出为1×1</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net = nn.Sequential(
</span></span><span style="display:flex;"><span>    nin_block(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">96</span>, kernel_size=<span style="color:#ff0;font-weight:bold">11</span>, strides=<span style="color:#ff0;font-weight:bold">4</span>, padding=<span style="color:#ff0;font-weight:bold">0</span>),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    nin_block(<span style="color:#ff0;font-weight:bold">96</span>, <span style="color:#ff0;font-weight:bold">256</span>, kernel_size=<span style="color:#ff0;font-weight:bold">5</span>, strides=<span style="color:#ff0;font-weight:bold">1</span>, padding=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    nin_block(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">384</span>, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, strides=<span style="color:#ff0;font-weight:bold">1</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    nn.Dropout(<span style="color:#ff0;font-weight:bold">0.5</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 标签类别数是10</span>
</span></span><span style="display:flex;"><span>    nin_block(<span style="color:#ff0;font-weight:bold">384</span>, <span style="color:#ff0;font-weight:bold">10</span>, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, strides=<span style="color:#ff0;font-weight:bold">1</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>),
</span></span><span style="display:flex;"><span>    nn.AdaptiveAvgPool2d((<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>)),
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span>
</span></span><span style="display:flex;"><span>    nn.Flatten())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X = torch.rand(size=(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">224</span>, <span style="color:#ff0;font-weight:bold">224</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> layer in net:
</span></span><span style="display:flex;"><span>    X = layer(X)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(layer.__class__.__name__,<span style="color:#0ff;font-weight:bold">&#39;output shape:</span><span style="color:#0ff;font-weight:bold">\t</span><span style="color:#0ff;font-weight:bold">&#39;</span>, X.shape)
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="C:%5cUsers%5cxiaoxin%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20240802211740264.png" alt="image-20240802211740264"  />
</p>
<h2 id="33-训练模型">3.3 训练模型<a hidden class="anchor" aria-hidden="true" href="#33-训练模型">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr, num_epochs, batch_size = <span style="color:#ff0;font-weight:bold">0.1</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">128</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span style="color:#ff0;font-weight:bold">224</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.341, train acc 0.873, test acc 0.875</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 4756.1 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="4-含并行连接的网络googlenet">4. 含并行连接的网络(GoogLeNet)<a hidden class="anchor" aria-hidden="true" href="#4-含并行连接的网络googlenet">#</a></h1>
<ul>
<li>基于NiN中串联网络的思想，GoogLeNet在2014年ImageNet图像识别挑战赛的取得佳绩；</li>
</ul>
<h2 id="41-inception块">4.1 Inception块<a hidden class="anchor" aria-hidden="true" href="#41-inception块">#</a></h2>
<p>如下图，Inception块由4条并行的路径组成，每个路径仅改变通道数，不改变高宽。</p>
<ul>
<li>第一条：1×1卷积</li>
<li>第二条：1×1卷积，加上3×3卷积(填充1)</li>
<li>第三条：1×1卷积，加上5×5卷积(填充2)</li>
<li>第四条：3×3最大汇聚(填充1)，加上1×1卷积</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803083313073.png" alt="image-20240803083313073"  />
</p>
<p>如下是定义Inception块的代码，其中可调参数均是输入以及每条路径中的通道数。</p>
<ul>
<li><strong>in_channels</strong> 输入通道数;</li>
<li><strong>c1</strong> 第一条路径的输出通道数;</li>
<li><strong>c2</strong> 第二条路径每个卷积层的输出通道数;</li>
<li><strong>c3</strong> 第三条路径每个卷积层的输出通道数;</li>
<li><strong>c4</strong> 第四条路径每个卷积层的输出通道数;</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch.nn <span style="color:#fff;font-weight:bold">import</span> functional <span style="color:#fff;font-weight:bold">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> Inception(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># c1--c4是每条路径的输出通道数</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, in_channels, c1, c2, c3, c4, **kwargs):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(Inception, <span style="color:#fff;font-weight:bold">self</span>).__init__(**kwargs)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 线路1，单1x1卷积层</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 线路2，1x1卷积层后接3x3卷积层</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p2_1 = nn.Conv2d(in_channels, c2[<span style="color:#ff0;font-weight:bold">0</span>], kernel_size=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p2_2 = nn.Conv2d(c2[<span style="color:#ff0;font-weight:bold">0</span>], c2[<span style="color:#ff0;font-weight:bold">1</span>], kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 线路3，1x1卷积层后接5x5卷积层</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p3_1 = nn.Conv2d(in_channels, c3[<span style="color:#ff0;font-weight:bold">0</span>], kernel_size=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p3_2 = nn.Conv2d(c3[<span style="color:#ff0;font-weight:bold">0</span>], c3[<span style="color:#ff0;font-weight:bold">1</span>], kernel_size=<span style="color:#ff0;font-weight:bold">5</span>, padding=<span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 线路4，3x3最大汇聚层后接1x1卷积层</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p4_1 = nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">1</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, x):
</span></span><span style="display:flex;"><span>        p1 = F.relu(<span style="color:#fff;font-weight:bold">self</span>.p1_1(x))
</span></span><span style="display:flex;"><span>        p2 = F.relu(<span style="color:#fff;font-weight:bold">self</span>.p2_2(F.relu(<span style="color:#fff;font-weight:bold">self</span>.p2_1(x))))
</span></span><span style="display:flex;"><span>        p3 = F.relu(<span style="color:#fff;font-weight:bold">self</span>.p3_2(F.relu(<span style="color:#fff;font-weight:bold">self</span>.p3_1(x))))
</span></span><span style="display:flex;"><span>        p4 = F.relu(<span style="color:#fff;font-weight:bold">self</span>.p4_2(<span style="color:#fff;font-weight:bold">self</span>.p4_1(x)))
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 在通道维度上连结输出</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> torch.cat((p1, p2, p3, p4), dim=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="42-googlenet模型">4.2 GoogLeNet模型<a hidden class="anchor" aria-hidden="true" href="#42-googlenet模型">#</a></h2>
<p>如下示意图，GoogLeNet模型共有5个部分组成(从下到上)：</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803085510921.png" alt="image-20240803085510921"  />
</p>
<ul>
<li>第一部分：类似于AlexNet，一个卷积层加上一个最大汇聚层；</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b1 = nn.Sequential(nn.Conv2d(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">64</span>, kernel_size=<span style="color:#ff0;font-weight:bold">7</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">3</span>),
</span></span><span style="display:flex;"><span>                   nn.ReLU(),
</span></span><span style="display:flex;"><span>                   nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>第二部分：两个卷积层加上一个最大汇聚层</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b2 = nn.Sequential(nn.Conv2d(<span style="color:#ff0;font-weight:bold">64</span>, <span style="color:#ff0;font-weight:bold">64</span>, kernel_size=<span style="color:#ff0;font-weight:bold">1</span>),
</span></span><span style="display:flex;"><span>                   nn.ReLU(),
</span></span><span style="display:flex;"><span>                   nn.Conv2d(<span style="color:#ff0;font-weight:bold">64</span>, <span style="color:#ff0;font-weight:bold">192</span>, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>),
</span></span><span style="display:flex;"><span>                   nn.ReLU(),
</span></span><span style="display:flex;"><span>                   nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>第三部分：2个Inception块，加上最大汇聚层
<ul>
<li>e.g. 第一个卷积层：输入通道=192，输出通道=64+128+32+32=256</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b3 = nn.Sequential(Inception(<span style="color:#ff0;font-weight:bold">192</span>, <span style="color:#ff0;font-weight:bold">64</span>, (<span style="color:#ff0;font-weight:bold">96</span>, <span style="color:#ff0;font-weight:bold">128</span>), (<span style="color:#ff0;font-weight:bold">16</span>, <span style="color:#ff0;font-weight:bold">32</span>), <span style="color:#ff0;font-weight:bold">32</span>),
</span></span><span style="display:flex;"><span>                   Inception(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">128</span>, (<span style="color:#ff0;font-weight:bold">128</span>, <span style="color:#ff0;font-weight:bold">192</span>), (<span style="color:#ff0;font-weight:bold">32</span>, <span style="color:#ff0;font-weight:bold">96</span>), <span style="color:#ff0;font-weight:bold">64</span>),
</span></span><span style="display:flex;"><span>                   nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>第四部分：5个Inception块，加上最大汇聚层
<ul>
<li>e.g. 第一个卷积层：输入通道=480，输出通道=192+208+48+64=512</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b4 = nn.Sequential(Inception(<span style="color:#ff0;font-weight:bold">480</span>, <span style="color:#ff0;font-weight:bold">192</span>, (<span style="color:#ff0;font-weight:bold">96</span>, <span style="color:#ff0;font-weight:bold">208</span>), (<span style="color:#ff0;font-weight:bold">16</span>, <span style="color:#ff0;font-weight:bold">48</span>), <span style="color:#ff0;font-weight:bold">64</span>),
</span></span><span style="display:flex;"><span>                   Inception(<span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">160</span>, (<span style="color:#ff0;font-weight:bold">112</span>, <span style="color:#ff0;font-weight:bold">224</span>), (<span style="color:#ff0;font-weight:bold">24</span>, <span style="color:#ff0;font-weight:bold">64</span>), <span style="color:#ff0;font-weight:bold">64</span>),
</span></span><span style="display:flex;"><span>                   Inception(<span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">128</span>, (<span style="color:#ff0;font-weight:bold">128</span>, <span style="color:#ff0;font-weight:bold">256</span>), (<span style="color:#ff0;font-weight:bold">24</span>, <span style="color:#ff0;font-weight:bold">64</span>), <span style="color:#ff0;font-weight:bold">64</span>),
</span></span><span style="display:flex;"><span>                   Inception(<span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">112</span>, (<span style="color:#ff0;font-weight:bold">144</span>, <span style="color:#ff0;font-weight:bold">288</span>), (<span style="color:#ff0;font-weight:bold">32</span>, <span style="color:#ff0;font-weight:bold">64</span>), <span style="color:#ff0;font-weight:bold">64</span>),
</span></span><span style="display:flex;"><span>                   Inception(<span style="color:#ff0;font-weight:bold">528</span>, <span style="color:#ff0;font-weight:bold">256</span>, (<span style="color:#ff0;font-weight:bold">160</span>, <span style="color:#ff0;font-weight:bold">320</span>), (<span style="color:#ff0;font-weight:bold">32</span>, <span style="color:#ff0;font-weight:bold">128</span>), <span style="color:#ff0;font-weight:bold">128</span>),
</span></span><span style="display:flex;"><span>                   nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>第五部分：2个Inception块，加上全局平均汇聚层以及全连接层
<ul>
<li>e.g. 最后一层卷积层：输入通道=832，输出通道=384+384+128+128=1024</li>
<li>全局平均汇聚层以及Flatten将输出变为1024的特征向量，后面再根据类别数接一个全连接层</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b5 = nn.Sequential(Inception(<span style="color:#ff0;font-weight:bold">832</span>, <span style="color:#ff0;font-weight:bold">256</span>, (<span style="color:#ff0;font-weight:bold">160</span>, <span style="color:#ff0;font-weight:bold">320</span>), (<span style="color:#ff0;font-weight:bold">32</span>, <span style="color:#ff0;font-weight:bold">128</span>), <span style="color:#ff0;font-weight:bold">128</span>),
</span></span><span style="display:flex;"><span>                   Inception(<span style="color:#ff0;font-weight:bold">832</span>, <span style="color:#ff0;font-weight:bold">384</span>, (<span style="color:#ff0;font-weight:bold">192</span>, <span style="color:#ff0;font-weight:bold">384</span>), (<span style="color:#ff0;font-weight:bold">48</span>, <span style="color:#ff0;font-weight:bold">128</span>), <span style="color:#ff0;font-weight:bold">128</span>),
</span></span><span style="display:flex;"><span>                   nn.AdaptiveAvgPool2d((<span style="color:#ff0;font-weight:bold">1</span>,<span style="color:#ff0;font-weight:bold">1</span>)),
</span></span><span style="display:flex;"><span>                   nn.Flatten())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span style="color:#ff0;font-weight:bold">1024</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>查看模型架构（仍是高宽变小，通道数变多的思想）</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X = torch.rand(size=(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">96</span>, <span style="color:#ff0;font-weight:bold">96</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> layer in net:
</span></span><span style="display:flex;"><span>    X = layer(X)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(layer.__class__.__name__,<span style="color:#0ff;font-weight:bold">&#39;output shape:</span><span style="color:#0ff;font-weight:bold">\t</span><span style="color:#0ff;font-weight:bold">&#39;</span>, X.shape)
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="C:%5cUsers%5cxiaoxin%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20240803091141694.png" alt="image-20240803091141694"  />
</p>
<h2 id="43-训练模型">4.3 训练模型<a hidden class="anchor" aria-hidden="true" href="#43-训练模型">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr, num_epochs, batch_size = <span style="color:#ff0;font-weight:bold">0.1</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">128</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span style="color:#ff0;font-weight:bold">96</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.249, train acc 0.905, test acc 0.827</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 3811.4 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="5-批量规范化">5. 批量规范化<a hidden class="anchor" aria-hidden="true" href="#5-批量规范化">#</a></h1>
<ul>
<li>加速深层神经网络的收敛</li>
</ul>
<h2 id="51-训练深层网络">5.1 训练深层网络<a hidden class="anchor" aria-hidden="true" href="#51-训练深层网络">#</a></h2>
<ul>
<li>
<p>批量规范化(Batch Normalization, BN)用于对特定神经网络层中，每次训练迭代的小批量输入数据进行&rsquo;归一化&rsquo;处理。</p>
</li>
<li>
<p>这使得不同层之间的参数量级得到统一，防止模型参数的更新是为了补偿不同层之间的数据差异，从而针对性的对预测问题本身进行学习，加速模型收敛。</p>
</li>
<li>
<p>具体实现其实也并不复杂：</p>
</li>
</ul>
<p>（1）首先对一个小批量B，计算其（feature）均值与方差</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803134949274.png" alt="image-20240803134949274"  />
</p>
<p>（2）然后，进行均值为0，方差为1的归一化处理后，进一步进行拉伸与偏移。γ可以变换方差，β可以变换均值，均属于可学习的参数，从而拟合最适合的规范化分布。</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803135335299.png" alt="image-20240803135335299"  />
</p>
<blockquote>
<p>一种角度的解释是BN操作中会引入一定的噪声，控制了模型复杂度。因为随机抽样的小批量分布不能代表总体情况。</p></blockquote>
<h2 id="52-批量规范化层">5.2 批量规范化层<a hidden class="anchor" aria-hidden="true" href="#52-批量规范化层">#</a></h2>
<ul>
<li>对于全连接层：
<ul>
<li>BN操作位于全连接层与激活函数之间；</li>
<li>对于[小批量数，特征数]的输入数据，会对每一列特征进行BN操作。即每个特征学习的γ与β都是不同的；</li>
</ul>
</li>
<li>对于卷积层
<ul>
<li>BN操作位于卷积层与激活函数之间；</li>
<li>对于多通道输出，会将每个通道作为一个特征，即计算小批量样本对于特定通道的，所有元素的均值与方差。</li>
</ul>
</li>
<li>此外，BN操作在预测过程是估算特征在整个训练数据集的均值与方差，再进行规范化。</li>
</ul>
<h2 id="53-从零实现">5.3 从零实现<a hidden class="anchor" aria-hidden="true" href="#53-从零实现">#</a></h2>
<ul>
<li>首先定义一个函数，进行BN操作</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> not torch.is_grad_enabled():
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span>
</span></span><span style="display:flex;"><span>        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">assert</span> <span style="color:#fff;font-weight:bold">len</span>(X.shape) in (<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">4</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">len</span>(X.shape) == <span style="color:#ff0;font-weight:bold">2</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 使用全连接层的情况，计算特征维上的均值和方差</span>
</span></span><span style="display:flex;"><span>            mean = X.mean(dim=<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>            var = ((X - mean) ** <span style="color:#ff0;font-weight:bold">2</span>).mean(dim=<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span>
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 这里我们需要保持X的形状以便后面可以做广播运算</span>
</span></span><span style="display:flex;"><span>            mean = X.mean(dim=(<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">3</span>), keepdim=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>            var = ((X - mean) ** <span style="color:#ff0;font-weight:bold">2</span>).mean(dim=(<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">3</span>), keepdim=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 训练模式下，用当前的均值和方差做标准化</span>
</span></span><span style="display:flex;"><span>        X_hat = (X - mean) / torch.sqrt(var + eps)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 更新移动平均的均值和方差</span>
</span></span><span style="display:flex;"><span>        moving_mean = momentum * moving_mean + (<span style="color:#ff0;font-weight:bold">1.0</span> - momentum) * mean
</span></span><span style="display:flex;"><span>        moving_var = momentum * moving_var + (<span style="color:#ff0;font-weight:bold">1.0</span> - momentum) * var
</span></span><span style="display:flex;"><span>    Y = gamma * X_hat + beta  <span style="color:#007f7f"># 缩放和移位</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> Y, moving_mean.data, moving_var.data
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>然后定义一个BatchNorm层</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> BatchNorm(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># num_features：完全连接层的输出数量或卷积层的输出通道数。</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># num_dims：2表示完全连接层，4表示卷积层</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, num_features, num_dims):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>().__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> num_dims == <span style="color:#ff0;font-weight:bold">2</span>:
</span></span><span style="display:flex;"><span>            shape = (<span style="color:#ff0;font-weight:bold">1</span>, num_features)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            shape = (<span style="color:#ff0;font-weight:bold">1</span>, num_features, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.gamma = nn.Parameter(torch.ones(shape))
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.beta = nn.Parameter(torch.zeros(shape))
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 非模型参数的变量初始化为0和1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.moving_mean = torch.zeros(shape)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.moving_var = torch.ones(shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, X):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 如果X不在内存上，将moving_mean和moving_var</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 复制到X所在显存上</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">self</span>.moving_mean.device != X.device:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">self</span>.moving_mean = <span style="color:#fff;font-weight:bold">self</span>.moving_mean.to(X.device)
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">self</span>.moving_var = <span style="color:#fff;font-weight:bold">self</span>.moving_var.to(X.device)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 保存更新过的moving_mean和moving_var</span>
</span></span><span style="display:flex;"><span>        Y, <span style="color:#fff;font-weight:bold">self</span>.moving_mean, <span style="color:#fff;font-weight:bold">self</span>.moving_var = batch_norm(
</span></span><span style="display:flex;"><span>            X, <span style="color:#fff;font-weight:bold">self</span>.gamma, <span style="color:#fff;font-weight:bold">self</span>.beta, <span style="color:#fff;font-weight:bold">self</span>.moving_mean,
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">self</span>.moving_var, eps=<span style="color:#ff0;font-weight:bold">1e-5</span>, momentum=<span style="color:#ff0;font-weight:bold">0.9</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> Y
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="54-使用批量规范化层的lenet">5.4 使用批量规范化层的LeNet<a hidden class="anchor" aria-hidden="true" href="#54-使用批量规范化层的lenet">#</a></h2>
<ul>
<li>定义模型</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net = nn.Sequential(
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">6</span>, kernel_size=<span style="color:#ff0;font-weight:bold">5</span>), BatchNorm(<span style="color:#ff0;font-weight:bold">6</span>, num_dims=<span style="color:#ff0;font-weight:bold">4</span>), nn.Sigmoid(),
</span></span><span style="display:flex;"><span>    nn.AvgPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">2</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>),
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">6</span>, <span style="color:#ff0;font-weight:bold">16</span>, kernel_size=<span style="color:#ff0;font-weight:bold">5</span>), BatchNorm(<span style="color:#ff0;font-weight:bold">16</span>, num_dims=<span style="color:#ff0;font-weight:bold">4</span>), nn.Sigmoid(),
</span></span><span style="display:flex;"><span>    nn.AvgPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">2</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>), nn.Flatten(),
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#ff0;font-weight:bold">16</span>*<span style="color:#ff0;font-weight:bold">4</span>*<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">120</span>), BatchNorm(<span style="color:#ff0;font-weight:bold">120</span>, num_dims=<span style="color:#ff0;font-weight:bold">2</span>), nn.Sigmoid(),
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#ff0;font-weight:bold">120</span>, <span style="color:#ff0;font-weight:bold">84</span>), BatchNorm(<span style="color:#ff0;font-weight:bold">84</span>, num_dims=<span style="color:#ff0;font-weight:bold">2</span>), nn.Sigmoid(),
</span></span><span style="display:flex;"><span>    nn.Linear(<span style="color:#ff0;font-weight:bold">84</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>训练模型</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr, num_epochs, batch_size = <span style="color:#ff0;font-weight:bold">1.0</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.272, train acc 0.899, test acc 0.863</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 23764.9 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>查看模型BN层的γ与β参数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net[<span style="color:#ff0;font-weight:bold">1</span>].gamma.reshape((-<span style="color:#ff0;font-weight:bold">1</span>,)), net[<span style="color:#ff0;font-weight:bold">1</span>].beta.reshape((-<span style="color:#ff0;font-weight:bold">1</span>,))
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># (tensor([3.3490, 3.2102, 4.1508, 1.7645, 2.7210, 0.5482], device=&#39;cuda:0&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         grad_fn=&lt;ViewBackward0&gt;),</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#  tensor([ 0.3007,  2.3391,  4.3099, -0.4318, -0.8200, -0.5383], device=&#39;cuda:0&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         grad_fn=&lt;ViewBackward0&gt;))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="6-残差网络resnet">6. 残差网络(ResNet)<a hidden class="anchor" aria-hidden="true" href="#6-残差网络resnet">#</a></h1>
<ul>
<li>残差网络主要由何凯明等人提出，在2015年ImageNet挑战赛中取得了冠军。</li>
</ul>
<h2 id="61-函数类">6.1 函数类<a hidden class="anchor" aria-hidden="true" href="#61-函数类">#</a></h2>
<p>（1）非嵌套函数类</p>
<ul>
<li>新模型 = 新添加的层 ← 原模型</li>
<li>在先前学习的深度神经网络中，每个神经网络层/块基本独立非嵌套关系，即前者的输出直接作为后者的输入。对于复杂的模型架构，有时新添加的层并不能使模型接近最优解，甚至可能更糟；</li>
</ul>
<p>（2）嵌套函数类</p>
<ul>
<li>新模型 = 新添加的层 + 原模型</li>
<li>如果新添加的层效果不明显，新模型仍然有机会基于原模型更新梯度。即新模型可能得出更优的解来拟合数据集（至少不会变差）。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803170211365.png" alt="image-20240803170211365"  />
</p>
<h2 id="62-残差块">6.2 残差块<a hidden class="anchor" aria-hidden="true" href="#62-残差块">#</a></h2>
<ul>
<li>如下左图，为一个正常的神经网络架构，若输出为f(x)。
<ul>
<li>由输入x，经神经网络层映射，得到输出f(x)</li>
</ul>
</li>
<li>如下右图，为一个残差块架构，若输出为f(x)。
<ul>
<li>由输入x，经神经网络层与原数据共同组成f(x)。</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803173550957.png" alt="image-20240803173550957"  />
</p>
<ul>
<li>ResNet网络中残差块共有两种，区别在于原模型数据的输入：
<ul>
<li>一种是原模型的x直接输入</li>
<li>另一种是原模型的x经1×1卷积层后再输入</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803200259327.png" alt="image-20240803200259327"  />
</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch.nn <span style="color:#fff;font-weight:bold">import</span> functional <span style="color:#fff;font-weight:bold">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># input_channels 输入的通道</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># num_channels 输出的通道</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># use_1x1conv参数声明是否考虑对x进行1×1卷积</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># strides 对于第一层卷积以及1×1卷积进行高宽缩减</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> Residual(nn.Module):  <span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, input_channels, num_channels,
</span></span><span style="display:flex;"><span>                 use_1x1conv=<span style="color:#fff;font-weight:bold">False</span>, strides=<span style="color:#ff0;font-weight:bold">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>().__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.conv1 = nn.Conv2d(input_channels, num_channels,
</span></span><span style="display:flex;"><span>                               kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>, stride=strides)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.conv2 = nn.Conv2d(num_channels, num_channels,
</span></span><span style="display:flex;"><span>                               kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> use_1x1conv:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">self</span>.conv3 = nn.Conv2d(input_channels, num_channels,
</span></span><span style="display:flex;"><span>                                   kernel_size=<span style="color:#ff0;font-weight:bold">1</span>, stride=strides)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">self</span>.conv3 = <span style="color:#fff;font-weight:bold">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.bn1 = nn.BatchNorm2d(num_channels)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.bn2 = nn.BatchNorm2d(num_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, X):
</span></span><span style="display:flex;"><span>        Y = F.relu(<span style="color:#fff;font-weight:bold">self</span>.bn1(<span style="color:#fff;font-weight:bold">self</span>.conv1(X)))
</span></span><span style="display:flex;"><span>        Y = <span style="color:#fff;font-weight:bold">self</span>.bn2(<span style="color:#fff;font-weight:bold">self</span>.conv2(Y))
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">self</span>.conv3:
</span></span><span style="display:flex;"><span>            X = <span style="color:#fff;font-weight:bold">self</span>.conv3(X)
</span></span><span style="display:flex;"><span>        Y += X
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> F.relu(Y)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>查看示例输出</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 输入通道为3，输出通道为3</span>
</span></span><span style="display:flex;"><span>blk = Residual(<span style="color:#ff0;font-weight:bold">3</span>,<span style="color:#ff0;font-weight:bold">3</span>) 
</span></span><span style="display:flex;"><span>X = torch.rand(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">6</span>, <span style="color:#ff0;font-weight:bold">6</span>) <span style="color:#007f7f"># 4个样本</span>
</span></span><span style="display:flex;"><span>Y = blk(X)
</span></span><span style="display:flex;"><span>Y.shape 
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([4, 3, 6, 6])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 输入通道为3，输出通道为6，高宽减半（必须调用1×1卷积）</span>
</span></span><span style="display:flex;"><span>blk = Residual(<span style="color:#ff0;font-weight:bold">3</span>,<span style="color:#ff0;font-weight:bold">6</span>, use_1x1conv=<span style="color:#fff;font-weight:bold">True</span>, strides=<span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>blk(X).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([4, 6, 3, 3])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="63-resnet模型">6.3 ResNet模型<a hidden class="anchor" aria-hidden="true" href="#63-resnet模型">#</a></h2>
<p>ResNet模型由如下部分组成</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803201636063.png" alt="image-20240803201636063"  />
</p>
<ul>
<li>第1部分：卷积层+BN+最大汇聚层，64通道输出，高宽降低四倍</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>b1 = nn.Sequential(nn.Conv2d(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">64</span>, kernel_size=<span style="color:#ff0;font-weight:bold">7</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">3</span>),
</span></span><span style="display:flex;"><span>                   nn.BatchNorm2d(<span style="color:#ff0;font-weight:bold">64</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>                   nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>第2~5部分由4个block组成，每个block包含两个2残差块
<ul>
<li>除了第1个Block以外的第1个残差块都需要将高宽减半，通道数加倍</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> resnet_block(input_channels, num_channels, num_residuals,
</span></span><span style="display:flex;"><span>                 first_block=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    blk = []
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> i in <span style="color:#fff;font-weight:bold">range</span>(num_residuals):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> i == <span style="color:#ff0;font-weight:bold">0</span> and not first_block:
</span></span><span style="display:flex;"><span>            blk.append(Residual(input_channels, num_channels, <span style="color:#007f7f">#输出通道数是输入的2倍</span>
</span></span><span style="display:flex;"><span>                                use_1x1conv=<span style="color:#fff;font-weight:bold">True</span>, strides=<span style="color:#ff0;font-weight:bold">2</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            blk.append(Residual(num_channels, num_channels)) <span style="color:#007f7f">#输入输出通道数相同</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> blk
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b2 = nn.Sequential(*resnet_block(<span style="color:#ff0;font-weight:bold">64</span>, <span style="color:#ff0;font-weight:bold">64</span>, <span style="color:#ff0;font-weight:bold">2</span>, first_block=<span style="color:#fff;font-weight:bold">True</span>))
</span></span><span style="display:flex;"><span>b3 = nn.Sequential(*resnet_block(<span style="color:#ff0;font-weight:bold">64</span>, <span style="color:#ff0;font-weight:bold">128</span>, <span style="color:#ff0;font-weight:bold">2</span>))
</span></span><span style="display:flex;"><span>b4 = nn.Sequential(*resnet_block(<span style="color:#ff0;font-weight:bold">128</span>, <span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">2</span>))
</span></span><span style="display:flex;"><span>b5 = nn.Sequential(*resnet_block(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">2</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>最后的第6部分连接全局平均汇聚层以及全连接输出层</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net = nn.Sequential(b1, b2, b3, b4, b5,
</span></span><span style="display:flex;"><span>                    nn.AdaptiveAvgPool2d((<span style="color:#ff0;font-weight:bold">1</span>,<span style="color:#ff0;font-weight:bold">1</span>)),
</span></span><span style="display:flex;"><span>                    nn.Flatten(), nn.Linear(<span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>4个Block包含共包含8个残差块，共16个卷积层，加上第一个卷积层以及最后一个全连接层，共有18层，因此又称为ResNet-18。</p></blockquote>
<ul>
<li>查看示例形状输出</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X = torch.rand(size=(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">224</span>, <span style="color:#ff0;font-weight:bold">224</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> layer in net:
</span></span><span style="display:flex;"><span>    X = layer(X)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(layer.__class__.__name__,<span style="color:#0ff;font-weight:bold">&#39;output shape:</span><span style="color:#0ff;font-weight:bold">\t</span><span style="color:#0ff;font-weight:bold">&#39;</span>, X.shape)
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="C:%5cUsers%5cxiaoxin%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20240803203729416.png" alt="image-20240803203729416"  />
</p>
<h2 id="64-训练模型">6.4 训练模型<a hidden class="anchor" aria-hidden="true" href="#64-训练模型">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr, num_epochs, batch_size = <span style="color:#ff0;font-weight:bold">0.05</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span style="color:#ff0;font-weight:bold">96</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.010, train acc 0.998, test acc 0.904</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 10662.5 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="7-稠密连接网络densenet">7. 稠密连接网络(DenseNet)<a hidden class="anchor" aria-hidden="true" href="#7-稠密连接网络densenet">#</a></h1>
<ul>
<li>DenseNet相当于是ResNet的逻辑扩展。</li>
</ul>
<h2 id="71-从resnet到densenet">7.1 从ResNet到DenseNet<a hidden class="anchor" aria-hidden="true" href="#71-从resnet到densenet">#</a></h2>
<ul>
<li>ResNet和DenseNet的关键区别在于，DenseNet输出是<em>连接</em>（用图中的[,]表示）而不是如ResNet的简单相加。</li>
<li>稠密网络主要由2部分构成：<em>稠密块</em>（dense block）和<em>过渡层</em>（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240803211424419.png" alt="image-20240803211424419"  />
</p>
<h2 id="72-稠密块">7.2 稠密块<a hidden class="anchor" aria-hidden="true" href="#72-稠密块">#</a></h2>
<ul>
<li>基本的“批量规范化、激活和卷积”架构</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> conv_block(input_channels, num_channels):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> nn.Sequential(
</span></span><span style="display:flex;"><span>        nn.BatchNorm2d(input_channels), nn.ReLU(),
</span></span><span style="display:flex;"><span>        nn.Conv2d(input_channels, num_channels, kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>)) <span style="color:#007f7f">#高宽不变</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>一个<em>稠密块</em>由多个卷积块组成，每个卷积块使用相同数量的输出通道；</li>
<li>在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># num_convs 卷积层数, </span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># input_channels 输入通道数, </span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># num_channels 输出通道数</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> DenseBlock(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, num_convs, input_channels, num_channels):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(DenseBlock, <span style="color:#fff;font-weight:bold">self</span>).__init__()
</span></span><span style="display:flex;"><span>        layer = []
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> i in <span style="color:#fff;font-weight:bold">range</span>(num_convs):
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 每多一个卷积层，其输入通道数不断增加</span>
</span></span><span style="display:flex;"><span>            layer.append(conv_block(
</span></span><span style="display:flex;"><span>                num_channels * i + input_channels, num_channels))
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.net = nn.Sequential(*layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, X):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> blk in <span style="color:#fff;font-weight:bold">self</span>.net:
</span></span><span style="display:flex;"><span>            Y = blk(X)
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 连接通道维度上每个块的输入和输出，即通道数增加</span>
</span></span><span style="display:flex;"><span>            X = torch.cat((X, Y), dim=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> X
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>示例展示：第一层输入通道数为3，第二层输入通道数是10+3，第二次输出通道数是10 + 10 + 3</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>blk = DenseBlock(<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">10</span>) 
</span></span><span style="display:flex;"><span>X = torch.randn(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">8</span>)
</span></span><span style="display:flex;"><span>Y = blk(X)
</span></span><span style="display:flex;"><span>Y.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([4, 23, 8, 8])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="73-过渡层">7.3 过渡层<a hidden class="anchor" aria-hidden="true" href="#73-过渡层">#</a></h2>
<ul>
<li>上述稠密层会导致通道数的不断累加、增多</li>
<li>过渡层用来控制模型复杂度，通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> transition_block(input_channels, num_channels):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> nn.Sequential(
</span></span><span style="display:flex;"><span>        nn.BatchNorm2d(input_channels), nn.ReLU(),
</span></span><span style="display:flex;"><span>        nn.Conv2d(input_channels, num_channels, kernel_size=<span style="color:#ff0;font-weight:bold">1</span>),
</span></span><span style="display:flex;"><span>        nn.AvgPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">2</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>blk = transition_block(<span style="color:#ff0;font-weight:bold">23</span>, <span style="color:#ff0;font-weight:bold">10</span>)
</span></span><span style="display:flex;"><span>blk(Y).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([4, 10, 4, 4])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="74-densenet模型">7.4 DenseNet模型<a hidden class="anchor" aria-hidden="true" href="#74-densenet模型">#</a></h2>
<p>架构基本类似ResNet。</p>
<ul>
<li>第一部分由单卷积层和最大汇聚层组成</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b1 = nn.Sequential(
</span></span><span style="display:flex;"><span>    nn.Conv2d(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">64</span>, kernel_size=<span style="color:#ff0;font-weight:bold">7</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">3</span>),
</span></span><span style="display:flex;"><span>    nn.BatchNorm2d(<span style="color:#ff0;font-weight:bold">64</span>), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.MaxPool2d(kernel_size=<span style="color:#ff0;font-weight:bold">3</span>, stride=<span style="color:#ff0;font-weight:bold">2</span>, padding=<span style="color:#ff0;font-weight:bold">1</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>后面再接4个稠密快，每个块由4个卷积层组成</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># num_channels为当前的通道数</span>
</span></span><span style="display:flex;"><span>num_channels, growth_rate = <span style="color:#ff0;font-weight:bold">64</span>, <span style="color:#ff0;font-weight:bold">32</span>
</span></span><span style="display:flex;"><span>num_convs_in_dense_blocks = [<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">4</span>]
</span></span><span style="display:flex;"><span>blks = []
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> i, num_convs in <span style="color:#fff;font-weight:bold">enumerate</span>(num_convs_in_dense_blocks):
</span></span><span style="display:flex;"><span>    blks.append(DenseBlock(num_convs, num_channels, growth_rate))
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 上一个稠密块的输出通道数</span>
</span></span><span style="display:flex;"><span>    num_channels += num_convs * growth_rate
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 在稠密块之间添加一个转换层，使通道数量减半</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> i != <span style="color:#fff;font-weight:bold">len</span>(num_convs_in_dense_blocks) - <span style="color:#ff0;font-weight:bold">1</span>:
</span></span><span style="display:flex;"><span>        blks.append(transition_block(num_channels, num_channels // <span style="color:#ff0;font-weight:bold">2</span>))
</span></span><span style="display:flex;"><span>        num_channels = num_channels // <span style="color:#ff0;font-weight:bold">2</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>最后接上全局汇聚层和全连接层来输出结果。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net = nn.Sequential(
</span></span><span style="display:flex;"><span>    b1, *blks,
</span></span><span style="display:flex;"><span>    nn.BatchNorm2d(num_channels), nn.ReLU(),
</span></span><span style="display:flex;"><span>    nn.AdaptiveAvgPool2d((<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>)),
</span></span><span style="display:flex;"><span>    nn.Flatten(),
</span></span><span style="display:flex;"><span>    nn.Linear(num_channels, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="75-训练模型">7.5 训练模型<a hidden class="anchor" aria-hidden="true" href="#75-训练模型">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr, num_epochs, batch_size = <span style="color:#ff0;font-weight:bold">0.1</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span style="color:#ff0;font-weight:bold">96</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.140, train acc 0.949, test acc 0.887</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 8501.4 examples/sec on cuda:0</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lishensuo.github.io/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li>
      <li><a href="https://lishensuo.github.io/en/tags/d2l/">D2L</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lishensuo.github.io/en/posts/bioinfo/709d2l-%E7%AC%AC%E5%85%AD%E7%AB%A0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
    <span class="title">« Prev Page</span>
    <br>
    <span>D2L--第六章卷积神经网络</span>
  </a>
  <a class="next" href="https://lishensuo.github.io/en/posts/bioinfo/711d2l-%E7%AC%AC%E5%85%AB%E7%AB%A0%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
    <span class="title">Next Page »</span>
    <br>
    <span>D2L--第八章循环神经网络</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://lishensuo.github.io/en/">Li&#39;s Bioinfo-Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
		<br/>您是本站第 <span id="busuanzi_value_site_uv"></span> 位访问者，总浏览量为 <span id="busuanzi_value_site_pv"></span> 次
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script type="text/javascript"
async
src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\[\[','\]\]']],
processEscapes: true,
processEnvironments: true,
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
TeX: { equationNumbers: { autoNumber: "AMS" },
extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});

MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>

<style>
code.has-jax {
font: inherit;
font-size: 100%;
background: inherit;
border: inherit;
color: #515151;
}
</style></body>
</html>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">

<link rel="icon" href="/favicon.ico" type="image/x-icon"> 
<title>D2L--第十四及十五章BERT模型 | Li&#39;s Bioinfo-Blog</title>
<meta name="keywords" content="深度学习, D2L">
<meta name="description" content="1. 基础介绍

BERT（来自Transformers的双向编码器表示）基于Transformer编码器进行预训练(Pre-train)，从而对输入词元进行上下文表示。
而在针对具体的自然语言处理任务的训练(Fine-tuning)时，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。


2. 输入表示
BERT输入序列的Embedding嵌入，表示为词元嵌入、段嵌入和位置嵌入的矩阵加和。">
<meta name="author" content="Lishensuo">
<link rel="canonical" href="https://lishensuo.github.io/en/posts/bioinfo/714d2l-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%8F%8A%E5%8D%81%E4%BA%94%E7%AB%A0bert%E6%A8%A1%E5%9E%8B/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.9e4de5e3ba61ea358168341aa7cdf70abfaafb7c697dfe8624af3ddff9a35c2f.css" integrity="sha256-nk3l47ph6jWBaDQap833Cr&#43;q&#43;3xpff6GJK893/mjXC8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js" integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://lishensuo.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://lishensuo.github.io/Q.gif">
<link rel="mask-icon" href="https://lishensuo.github.io/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lishensuo.github.io/en/posts/bioinfo/714d2l-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%8F%8A%E5%8D%81%E4%BA%94%E7%AB%A0bert%E6%A8%A1%E5%9E%8B/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="D2L--第十四及十五章BERT模型" />
<meta property="og:description" content="1. 基础介绍

BERT（来自Transformers的双向编码器表示）基于Transformer编码器进行预训练(Pre-train)，从而对输入词元进行上下文表示。
而在针对具体的自然语言处理任务的训练(Fine-tuning)时，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。


2. 输入表示
BERT输入序列的Embedding嵌入，表示为词元嵌入、段嵌入和位置嵌入的矩阵加和。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lishensuo.github.io/en/posts/bioinfo/714d2l-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%8F%8A%E5%8D%81%E4%BA%94%E7%AB%A0bert%E6%A8%A1%E5%9E%8B/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-17T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-08-17T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="D2L--第十四及十五章BERT模型"/>
<meta name="twitter:description" content="1. 基础介绍

BERT（来自Transformers的双向编码器表示）基于Transformer编码器进行预训练(Pre-train)，从而对输入词元进行上下文表示。
而在针对具体的自然语言处理任务的训练(Fine-tuning)时，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。


2. 输入表示
BERT输入序列的Embedding嵌入，表示为词元嵌入、段嵌入和位置嵌入的矩阵加和。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "分类",
      "item": "https://lishensuo.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📖 生信数据分析--分析流程，工具包等",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "D2L--第十四及十五章BERT模型",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/714d2l-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%8F%8A%E5%8D%81%E4%BA%94%E7%AB%A0bert%E6%A8%A1%E5%9E%8B/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "D2L--第十四及十五章BERT模型",
  "name": "D2L--第十四及十五章BERT模型",
  "description": "1. 基础介绍 BERT（来自Transformers的双向编码器表示）基于Transformer编码器进行预训练(Pre-train)，从而对输入词元进行上下文表示。 而在针对具体的自然语言处理任务的训练(Fine-tuning)时，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。 2. 输入表示 BERT输入序列的Embedding嵌入，表示为词元嵌入、段嵌入和位置嵌入的矩阵加和。\n",
  "keywords": [
    "深度学习", "D2L"
  ],
  "articleBody": "1. 基础介绍 BERT（来自Transformers的双向编码器表示）基于Transformer编码器进行预训练(Pre-train)，从而对输入词元进行上下文表示。 而在针对具体的自然语言处理任务的训练(Fine-tuning)时，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。 2. 输入表示 BERT输入序列的Embedding嵌入，表示为词元嵌入、段嵌入和位置嵌入的矩阵加和。\n词元嵌入：词元索引的独热编码的特征表示 片段嵌入：标记来自第一个文本句，还是第二个文本句 每个序列开头加是一个’‘特殊类别词元。 BERT输入序列可以包括1个或者2个文本句子，每个文本句后加一个’‘特殊分隔词元 1 2 3 4 5 6 7 8 9 10 11 12 13 #@save def get_tokens_and_segments(tokens_a, tokens_b=None): \"\"\"获取输入序列的词元及其片段索引\"\"\" tokens = [''] + tokens_a + [''] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + [''] segments += [1] * (len(tokens_b) + 1) return tokens, segments get_tokens_and_segments([\"aa\"],['b']) # (['', 'aa', '', 'b', ''], [0, 0, 0, 1, 1]) 位置嵌入：与Transformer不同，BERT使用了可学习的位置编码 获取序列的输入表示后，就可以简单地应用到Transformer的编码器中 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #@save class BERTEncoder(nn.Module): \"\"\"BERT编码器\"\"\" def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs): super(BERTEncoder, self).__init__(**kwargs) self.token_embedding = nn.Embedding(vocab_size, num_hiddens) #词元嵌入 self.segment_embedding = nn.Embedding(2, num_hiddens) #段嵌入 # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数（用多少截多少） self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens)) # n个Transformer编码层 self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(f\"{i}\", d2l.EncoderBlock( key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, True)) def forward(self, tokens, segments, valid_lens): # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens） X = self.token_embedding(tokens) + self.segment_embedding(segments) X = X + self.pos_embedding.data[:, :X.shape[1], :] for blk in self.blks: X = blk(X, valid_lens) return X 示例 1 2 3 4 5 6 7 8 9 10 11 12 13 # 实例化一个encoder vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4 norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2 encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout) # demo输入：批量大小为2，序列长度为8 tokens = torch.randint(0, vocab_size, (2, 8)) # 每个序列的段嵌入 segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]]) encoded_X = encoder(tokens, segments, None) encoded_X.shape # torch.Size([2, 8, 768]) 3. 预训练任务 上述的encoder前向推断给出了输入文本的每个词元和插入的特殊标记“”及“\u003c\\seq\u003e”的BERT表示。 基于此，来计算预训练BERT的两个损失函数，从而以自监督(无标签)训练方式，学习词元的embedding 3.1 掩蔽语言模型 Masked language modeling\nBERT随机掩蔽(masked)词元，并使用来自双向上下文的词元预测掩蔽词元。\n这里的双向主要是指在学习特定词元的表示时，可以同时看到该词元前面和后面的序列信息。 这区别于GPT模型从左到右的学习预测方式。 具体来说，在该预训练中，将随机选择15%的词元作为预测的掩蔽/掩码词元。对于每个被选中的词元，会如下处理：\n80%可能变为特殊的““词元（例如，“this movie is great”变为“this movie is\u003c\\mask\u003e”； 10%时间为随机词元(噪音)（例如，“this movie is great”变为“this movie is drink”）； 10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”）。 如下定义一个MaskLM类来预测上述的掩蔽标记\n输入：Encoder对词元的编码表示，用于预测的词元位置 输出：这些位置的预测结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #@save class MaskLM(nn.Module): \"\"\"BERT的掩蔽语言模型任务\"\"\" def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs): super(MaskLM, self).__init__(**kwargs) # 使用具有单隐藏层的MLP进行预测 self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.LayerNorm(num_hiddens), nn.Linear(num_hiddens, vocab_size)) def forward(self, X, pred_positions): num_pred_positions = pred_positions.shape[1] #每个序列的masked词元数 pred_positions = pred_positions.reshape(-1) #总共的masked词元数 batch_size = X.shape[0] batch_idx = torch.arange(0, batch_size) # 假设batch_size=2，num_pred_positions=3 # 那么batch_idx是np.array（[0,0,0,1,1,1]） batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) masked_X = X[batch_idx, pred_positions] masked_X = masked_X.reshape((batch_size, num_pred_positions, -1)) mlm_Y_hat = self.mlp(masked_X) return mlm_Y_hat 示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 mlm = MaskLM(vocab_size, num_hiddens) mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]]) mlm_Y_hat = mlm(encoded_X, mlm_positions) mlm_Y_hat.shape # torch.Size([2, 3, 10000]) # 10000为词表大小 #计算与真实标签的loss mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]]) loss = nn.CrossEntropyLoss(reduction='none') mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1)) mlm_l.shape # torch.Size([6]) 3.2 下一步预测 Next sentence prediction\n当BERT的输入序列包含两个句子时，可以预测两个句子是否存在上下文逻辑关系 此时，可以使用序列开头的词元嵌入表示整个序列，进行建模 1 2 3 4 5 6 7 8 9 10 #@save class NextSentencePred(nn.Module): \"\"\"BERT的下一句预测任务\"\"\" def __init__(self, num_inputs, **kwargs): super(NextSentencePred, self).__init__(**kwargs) self.output = nn.Linear(num_inputs, 2) def forward(self, X): # X的形状：(batchsize,num_hiddens) 每个序列的词元表示 return self.output(X) 示例 1 2 3 4 5 6 7 8 9 10 11 12 13 encoded_X = torch.flatten(encoded_X, start_dim=1) # torch.Size([2, 6144]) 这里应该只是模拟合适的形状 # NSP的输入形状:(batchsize，num_hiddens) nsp = NextSentencePred(encoded_X.shape[-1]) nsp_Y_hat = nsp(encoded_X) nsp_Y_hat.shape # torch.Size([2, 2]) # 每个样本得到两个概率预测结果 nsp_y = torch.tensor([0, 1]) #真实标签 nsp_l = loss(nsp_Y_hat, nsp_y) nsp_l.shape # encoded_X.shape 3.3 综合代码 基于上述定义的BERTEncoder类，以及两个预训练任务MaskLM和NextSentencePred来定义BERTModel\n输入的是词元相关信息 输出的是编码后BERT表示encoded_X、以及两个任务的预测结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #@save class BERTModel(nn.Module): \"\"\"BERT模型\"\"\" def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768): super(BERTModel, self).__init__() self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size) self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features) self.nsp = NextSentencePred(nsp_in_features) # 为nsp任务添加的一层中间隐藏层, 输入为encoder的cls输出， # 输出是nsp的输入 self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh()) def forward(self, tokens, segments, valid_lens=None, pred_positions=None): encoded_X = self.encoder(tokens, segments, valid_lens) if pred_positions is not None: mlm_Y_hat = self.mlm(encoded_X, pred_positions) else: mlm_Y_hat = None # 用于下一句预测的多层感知机分类器的隐藏层，0是“”标记的索引 nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :])) return encoded_X, mlm_Y_hat, nsp_Y_hat 4. 训练数据集 示例数据WikiText-2 教材中提供的下载链接失效了，在讨论区有小伙伴及时保存了，目前已下载上传到Github 将该文件手动上传到../data/目录下，再解压 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import os import random import torch from d2l import torch as d2l #@save d2l.DATA_HUB['wikitext-2'] = ( 'https://s3.amazonaws.com/research.metamind.io/wikitext/' 'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe') # 上述链接失效了，收到上传到服务器中 data_dir = '../data/wikitext-2' ls ../data/wikitext-2 # wiki.test.tokens wiki.train.tokens wiki.valid.tokens 预处理 一行代表一个段落 句号为分隔符，将一段拆成多个句子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #@save def _read_wiki(data_dir): file_name = os.path.join(data_dir, 'wiki.train.tokens') with open(file_name, 'r') as f: lines = f.readlines() # 大写字母转换为小写字母 paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) \u003e= 2] random.shuffle(paragraphs) return paragraphs # 示例 # 词元化处理 paragraphs = [d2l.tokenize( paragraph, token='word') for paragraph in _read_wiki(data_dir)] # 3层嵌套的list # 第一层：段落 # 第二层：句子 # 第三层：词元 paragraphs[0][0][:3] # ['families', 'of', '45'] # 第一段的第一句的前三个词元 4.1 NSP数据处理 将两句话合并作为一个样本序列 若两句话来自于同一段落的上下文，则为阳性；其余为阴性。 将两种情况序列设置各占50% 如下函数，50%概率返回两个连续的句子（阳性），50%的概率不会 1 2 3 4 5 6 7 8 9 #@save def _get_next_sentence(sentence, next_sentence, paragraphs): if random.random() \u003c 0.5: is_next = True else: # 阴性：随机选择第二个句子 next_sentence = random.choice(random.choice(paragraphs)) is_next = False return sentence, next_sentence, is_next 如下函数，将对每个段落生成具有两个句子的阳性或阴性序列 返回一个list，包含该段落所有的样本序列，片段嵌入，以及是否连续句子的逻辑符 1 2 3 4 5 6 7 8 9 10 11 12 #@save def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len): nsp_data_from_paragraph = [] for i in range(len(paragraph) - 1): tokens_a, tokens_b, is_next = _get_next_sentence( paragraph[i], paragraph[i + 1], paragraphs) # 考虑1个''词元和2个''词元 if len(tokens_a) + len(tokens_b) + 3 \u003e max_len: continue tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b) nsp_data_from_paragraph.append((tokens, segments, is_next)) return nsp_data_from_paragraph 4.2 MLM数据处理 这里所说的序列就是指4.1步骤得到的合并两个句子的词元序列\n为每个输入序列做掩码处理，并记录被替换的位置，以及正确的标签 这里的位置就是每个样本序列的词元的位置（不是索引） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #@save def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab): # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“”或随机词元 mlm_input_tokens = [token for token in tokens] pred_positions_and_labels = [] # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测 random.shuffle(candidate_pred_positions) for mlm_pred_position in candidate_pred_positions: if len(pred_positions_and_labels) \u003e= num_mlm_preds: break masked_token = None # 80%的时间：将词替换为“”词元 if random.random() \u003c 0.8: masked_token = '' else: # 10%的时间：保持词不变 if random.random() \u003c 0.5: masked_token = tokens[mlm_pred_position] # 10%的时间：用随机词替换该词 else: masked_token = random.choice(vocab.idx_to_token) mlm_input_tokens[mlm_pred_position] = masked_token pred_positions_and_labels.append( (mlm_pred_position, tokens[mlm_pred_position])) return mlm_input_tokens, pred_positions_and_labels 输入：词元序列以及相应的词表 输出：序列索引信息，以及待预测的位置信息以及正确的标签 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #@save def _get_mlm_data_from_tokens(tokens, vocab): candidate_pred_positions = [] # tokens是一个字符串列表 for i, token in enumerate(tokens): # 在遮蔽语言模型任务中不会预测特殊词元 if token in ['', '']: continue candidate_pred_positions.append(i) # 遮蔽语言模型任务中预测15%的随机词元 num_mlm_preds = max(1, round(len(tokens) * 0.15)) mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens( tokens, candidate_pred_positions, num_mlm_preds, vocab) pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0]) pred_positions = [v[0] for v in pred_positions_and_labels] mlm_pred_labels = [v[1] for v in pred_positions_and_labels] return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels] 4.3 添加填充词元 为每个序列添加填充词元’’，从而保证输入序列的长度固定，并记录有效长度 同时，序列相关的其它标记信息也需要随之补充到固定长度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #@save def _pad_bert_inputs(examples, max_len, vocab): max_num_mlm_preds = round(max_len * 0.15) all_token_ids, all_segments, valid_lens, = [], [], [] all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], [] nsp_labels = [] for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples: all_token_ids.append(torch.tensor(token_ids + [vocab['']] * ( max_len - len(token_ids)), dtype=torch.long)) all_segments.append(torch.tensor(segments + [0] * ( max_len - len(segments)), dtype=torch.long)) # 片段标记补长 # valid_lens不包括''的计数 valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32)) # 预测词元标记补长: 固定长度的15% - 有效长度的15% all_pred_positions.append(torch.tensor(pred_positions + [0] * ( max_num_mlm_preds - len(pred_positions)), dtype=torch.long)) # 填充词元的预测将通过乘以0权重在损失中过滤掉 all_mlm_weights.append( torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * ( max_num_mlm_preds - len(pred_positions)), dtype=torch.float32)) all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * ( max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long)) # 预测词元真实值，也需要补长 nsp_labels.append(torch.tensor(is_next, dtype=torch.long)) return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels) 4.4 综合处理 all_token_ids: 输入序列的列表 all_segments: 输入序列片段标记的列表 valid_lens: 每个序列有效长度的列表 all_pred_positions, all_mlm_weights, all_mlm_labels: mlm任务中预测词元的信息 nsp_labels: 是否为连续句子对的逻辑符 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #@save class _WikiTextDataset(torch.utils.data.Dataset): def __init__(self, paragraphs, max_len): # 输入paragraphs[i]是代表段落的句子字符串列表； # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表 paragraphs = [d2l.tokenize( paragraph, token='word') for paragraph in paragraphs] sentences = [sentence for paragraph in paragraphs for sentence in paragraph] self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[ '', '', '', '']) # 获取下一句子预测任务的数据 examples = [] for paragraph in paragraphs: examples.extend(_get_nsp_data_from_paragraph( paragraph, paragraphs, self.vocab, max_len)) # 获取遮蔽语言模型任务的数据 examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next in examples] # 填充输入 (self.all_token_ids, self.all_segments, self.valid_lens, self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs( examples, max_len, self.vocab) def __getitem__(self, idx): return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx], self.all_pred_positions[idx], self.all_mlm_weights[idx], self.all_mlm_labels[idx], self.nsp_labels[idx]) def __len__(self): return len(self.all_token_ids) 制作成数据迭代器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #@save def load_data_wiki(batch_size, max_len): \"\"\"加载WikiText-2数据集\"\"\" num_workers = d2l.get_dataloader_workers() # data_dir = d2l.download_extract('wikitext-2', 'wikitext-2') data_dir = '../data/wikitext-2' paragraphs = _read_wiki(data_dir) train_set = _WikiTextDataset(paragraphs, max_len) train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers) return train_iter, train_set.vocab # 词元序列最大长度设置为64 batch_size, max_len = 512, 64 train_iter, vocab = load_data_wiki(batch_size, max_len) for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter: print(tokens_X.shape, segments_X.shape, valid_lens_x.shape, pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape, nsp_y.shape) break # torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) # torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512]) pred_positions_X[0,:] # tensor([ 4, 6, 18, 28, 35, 37, 39, 49, 0, 0]) mlm_Y[0,:] # tensor([3003, 443, 239, 1792, 48, 6, 814, 18, 0, 0]) 5. 预训练BERT 5.1 预训练BERT 1 2 3 import torch from torch import nn from d2l import torch as d2l 实例化模型 1 2 3 4 5 6 7 8 batch_size, max_len = 512, 64 train_iter, vocab = d2l.load_data_wiki(batch_size, max_len) net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128], ffn_num_input=128, ffn_num_hiddens=256, num_heads=2, num_layers=2, dropout=0.2, key_size=128, query_size=128, value_size=128, hid_in_features=128, mlm_in_features=128, nsp_in_features=128) 损失函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 loss = nn.CrossEntropyLoss() #@save def _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y): # 前向传播 _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X, valid_lens_x.reshape(-1), pred_positions_X) # 计算遮蔽语言模型损失 mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\ mlm_weights_X.reshape(-1, 1) mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8) # 计算下一句子预测任务的损失 nsp_l = loss(nsp_Y_hat, nsp_y) l = mlm_l + nsp_l return mlm_l, nsp_l, l GPU训练模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 devices = d2l.try_all_gpus() def train_bert(train_iter, net, loss, vocab_size, devices, num_steps): net = nn.DataParallel(net, device_ids=devices).to(devices[0]) trainer = torch.optim.Adam(net.parameters(), lr=0.01) step, timer = 0, d2l.Timer() animator = d2l.Animator(xlabel='step', ylabel='loss', xlim=[1, num_steps], legend=['mlm', 'nsp']) # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数 metric = d2l.Accumulator(4) num_steps_reached = False while step \u003c num_steps and not num_steps_reached: for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\ mlm_weights_X, mlm_Y, nsp_y in train_iter: tokens_X = tokens_X.to(devices[0]) segments_X = segments_X.to(devices[0]) valid_lens_x = valid_lens_x.to(devices[0]) pred_positions_X = pred_positions_X.to(devices[0]) mlm_weights_X = mlm_weights_X.to(devices[0]) mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0]) trainer.zero_grad() timer.start() mlm_l, nsp_l, l = _get_batch_loss_bert( net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) l.backward() trainer.step() metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1) timer.stop() animator.add(step + 1, (metric[0] / metric[3], metric[1] / metric[3])) step += 1 if step == num_steps: num_steps_reached = True break print(f'MLM loss {metric[0] / metric[3]:.3f}, ' f'NSP loss {metric[1] / metric[3]:.3f}') print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on ' f'{str(devices)}') # 结果 train_bert(train_iter, net, loss, len(vocab), devices, 50) # MLM loss 5.816, NSP loss 0.757 # 6688.3 sentence pairs/sec on [device(type='cuda', index=0), device(type='cuda', index=1)] 5.2 用BERT表示文本 给定输入句子，经BERT编码后，返回每个词元的嵌入表示 1 2 3 4 5 6 7 def get_bert_encoding(net, tokens_a, tokens_b=None): tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b) token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0) segments = torch.tensor(segments, device=devices[0]).unsqueeze(0) valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0) encoded_X, _, _ = net(token_ids, segments, valid_len) return encoded_X 句子1：a crane is flying, 加上头的以及尾的，共有6个词元 1 2 3 4 5 6 7 8 9 10 11 12 tokens_a = ['a', 'crane', 'is', 'flying'] encoded_text = get_bert_encoding(net, tokens_a) # 词元：'','a','crane','is','flying','' encoded_text.shape # torch.Size([1, 6, 128]) encoded_text_cls = encoded_text[:, 0, :] encoded_text_crane = encoded_text[:, 2, :] encoded_text_cls.shape # torch.Size([1, 128]) encoded_text_crane[0][:3] # tensor([-0.3421, -0.6974, -1.9238], device='cuda:0', grad_fn=) 句子2: a crane driver came 1 2 3 4 5 tokens_a2 = ['a', 'crane', 'driver', 'came'] encoded_text2 = get_bert_encoding(net, tokens_a2) encoded_text_crane2 = encoded_text2[:, 2, :] encoded_text_crane2[0][:3] # tensor([-0.3659, -0.6927, -1.8660], device='cuda:0', grad_fn=) 6. 微调BERT 6.1 SNLI数据集 一对文本序列（前提与假设）通常可认为存在3种逻辑关系： （1）蕴含 entailment：前提可以推出假设 （2）矛盾contradiction：二者逻辑互相矛盾 （3）中性neutral：无法确定 斯坦福自然语言推断(Stanford natural language inferrence, SNLI)数据集包含了50多万个带标签的英语句子组成的集合。 1 2 3 4 5 6 7 8 9 # #@save # d2l.DATA_HUB['SNLI'] = ( # 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip', # '9fcde07509c7e87ec61c640c1b2753d9041758e4') # data_dir = d2l.download_extract('SNLI') # 下载snli_1.0.zip到 ../data，解压后，重命名为SNLI data_dir = '../data/SNLI' 定义如下代码，读取该数据集 分为训练集与测试集，前者约有55万对；后者约有1万对 每个数据集返回a list of 3 list，每个子list分别包括所有的前提序列，假设序列，以及二者的标签。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #@save def read_snli(data_dir, is_train): \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\" def extract_text(s): # 删除我们不会使用的信息 s = re.sub('\\\\(', '', s) s = re.sub('\\\\)', '', s) # 用一个空格替换两个或多个连续的空格 s = re.sub('\\\\s{2,}', ' ', s) return s.strip() label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2} file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt') with open(file_name, 'r') as f: rows = [row.split('\\t') for row in f.readlines()[1:]] premises = [extract_text(row[1]) for row in rows if row[0] in label_set] hypotheses = [extract_text(row[2]) for row in rows if row[0] \\ in label_set] labels = [label_set[row[0]] for row in rows if row[0] in label_set] return premises, hypotheses, labels train_data = read_snli(data_dir, is_train=True) len(train_data), len(train_data[0]) # (3, 549367) for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]): print('前提：', x0) print('假设：', x1) print('标签：', y) # 前提： A person on a horse jumps over a broken down airplane . # 假设： A person is training his horse for a competition . # 标签： 2 # 前提： A person on a horse jumps over a broken down airplane . # 假设： A person is at a diner , ordering an omelette . # 标签： 1 # 前提： A person on a horse jumps over a broken down airplane . # 假设： A person is outdoors , on a horse . # 标签： 0 6.2 加载预训练模型 1 2 3 4 5 6 import json import multiprocessing import os import torch from torch import nn from d2l import torch as d2l 教材提供了两个预训练好的bert模型，分别是base和small两个版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip', # '225d66f04cae318b841a13d32af3acc165f253ac') # d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip', # 'c72329e68a732bef0452e4b96a1c341c8910f81f') def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices): # data_dir = d2l.download_extract(pretrained_model) # http://d2l-data.s3-accelerate.amazonaws.com/bert.small.torch.zip data_dir = '../data/bert.small' # 定义空词表以加载预定义词表 vocab = d2l.Vocab() vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json'))) vocab.token_to_idx = {token: idx for idx, token in enumerate( vocab.idx_to_token)} bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256], ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, num_heads=4, num_layers=2, dropout=0.2, max_len=max_len, key_size=256, query_size=256, value_size=256, hid_in_features=256, mlm_in_features=256, nsp_in_features=256) # 加载预训练BERT参数 bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params'))) return bert, vocab 6.3 数据预处理 根据6.1 ，合并一对文本序列为一个完整的输入序列；以及相应的片段标记，有效长度。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class SNLIBERTDataset(torch.utils.data.Dataset): def __init__(self, dataset, max_len, vocab=None): all_premise_hypothesis_tokens = [[ p_tokens, h_tokens] for p_tokens, h_tokens in zip( *[d2l.tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])] self.labels = torch.tensor(dataset[2]) self.vocab = vocab self.max_len = max_len (self.all_token_ids, self.all_segments, self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens) print('read ' + str(len(self.all_token_ids)) + ' examples') # 定义_mp_worker，多线程预处理， def _preprocess(self, all_premise_hypothesis_tokens): pool = multiprocessing.Pool(4) # 使用4个进程 out = pool.map(self._mp_worker, all_premise_hypothesis_tokens) all_token_ids = [ token_ids for token_ids, segments, valid_len in out] all_segments = [segments for token_ids, segments, valid_len in out] valid_lens = [valid_len for token_ids, segments, valid_len in out] return (torch.tensor(all_token_ids, dtype=torch.long), torch.tensor(all_segments, dtype=torch.long), torch.tensor(valid_lens)) # 每个线程的处理细节 def _mp_worker(self, premise_hypothesis_tokens): p_tokens, h_tokens = premise_hypothesis_tokens self._truncate_pair_of_tokens(p_tokens, h_tokens) tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens) token_ids = self.vocab[tokens] + [self.vocab['']] \\ * (self.max_len - len(tokens)) segments = segments + [0] * (self.max_len - len(segments)) valid_len = len(tokens) return token_ids, segments, valid_len # 保证原始的p_tokens加上h_tokens的词元数小于最大长度 def _truncate_pair_of_tokens(self, p_tokens, h_tokens): # 为BERT输入中的''、''和''词元保留位置 while len(p_tokens) + len(h_tokens) \u003e self.max_len - 3: if len(p_tokens) \u003e len(h_tokens): p_tokens.pop() else: h_tokens.pop() def __getitem__(self, idx): return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx]), self.labels[idx] def __len__(self): return len(self.all_token_ids) 小批量加载数据 1 2 3 4 5 6 7 8 9 10 11 # 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512 batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers() data_dir = d2l.download_extract('SNLI') train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab) test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab) train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers) test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers) # read 549367 examples # read 9824 examples 6.4 微调BERT 在原有的基础上，添加额外的MLP。该MLP由两个全连接层组成（如下的self.hidden与self.output） 基于bert输出中，对于输入序列的词元的嵌入表示进行微调、预测。 （1）定义前向传播，实例化模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 class BERTClassifier(nn.Module): def __init__(self, bert): super(BERTClassifier, self).__init__() self.encoder = bert.encoder self.hidden = bert.hidden self.output = nn.Linear(256, 3) def forward(self, inputs): tokens_X, segments_X, valid_lens_x = inputs encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x) return self.output(self.hidden(encoded_X[:, 0, :])) net = BERTClassifier(bert) MaskLM类和NextSentencePred类这两个损失函数与微调下游应用无关，因此当BERT微调时，MaskLM和NextSentencePred中采用的多层感知机的参数不会更新（陈旧的，staled）。\n（2）训练微调模型\n1 2 3 4 5 6 lr, num_epochs = 1e-4, 5 trainer = torch.optim.Adam(net.parameters(), lr=lr) loss = nn.CrossEntropyLoss(reduction='none') d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices) # loss 0.519, train acc 0.791, test acc 0.783 # 7622.9 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)] ",
  "wordCount" : "6809",
  "inLanguage": "en",
  "datePublished": "2024-08-17T00:00:00Z",
  "dateModified": "2024-08-17T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Lishensuo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lishensuo.github.io/en/posts/bioinfo/714d2l-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%8F%8A%E5%8D%81%E4%BA%94%E7%AB%A0bert%E6%A8%A1%E5%9E%8B/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Li's Bioinfo-Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lishensuo.github.io/img/Q.gif"
    }
  }
}
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lishensuo.github.io/en/" accesskey="h" title="Li&#39;s Bioinfo-Blog (Alt + H)">Li&#39;s Bioinfo-Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lishensuo.github.io/en/" title="主页">
                    <span>主页</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/posts" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/tags" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/about" title="关于">
                    <span>关于</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/search" title="搜索 (Alt &#43; /)" accesskey=/>
                    <span>搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lishensuo.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/">分类</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/bioinfo/">📖 生信数据分析--分析流程，工具包等</a></div>
    <h1 class="post-title">
      D2L--第十四及十五章BERT模型
    </h1>
    <div class="post-meta">













Create:&amp;nbsp;&lt;span title=&#39;2024-08-17 00:00:00 &#43;0000 UTC&#39;&gt;2024-08-17&lt;/span&gt;&amp;nbsp;|&amp;nbsp;Update:&amp;nbsp;2024-08-17&amp;nbsp;|&amp;nbsp;Words:&amp;nbsp;6809&amp;nbsp;|&amp;nbsp;14 min&amp;nbsp;|&amp;nbsp;Lishensuo

|  Viewers: <span id="busuanzi_value_page_pv"></span> 
	  
    </div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-%e5%9f%ba%e7%a1%80%e4%bb%8b%e7%bb%8d" aria-label="1. 基础介绍">1. 基础介绍</a></li>
                    <li>
                        <a href="#2-%e8%be%93%e5%85%a5%e8%a1%a8%e7%a4%ba" aria-label="2. 输入表示">2. 输入表示</a></li>
                    <li>
                        <a href="#3-%e9%a2%84%e8%ae%ad%e7%bb%83%e4%bb%bb%e5%8a%a1" aria-label="3. 预训练任务">3. 预训练任务</a><ul>
                            
                    <li>
                        <a href="#31-%e6%8e%a9%e8%94%bd%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b" aria-label="3.1 掩蔽语言模型">3.1 掩蔽语言模型</a></li>
                    <li>
                        <a href="#32-%e4%b8%8b%e4%b8%80%e6%ad%a5%e9%a2%84%e6%b5%8b" aria-label="3.2 下一步预测">3.2 下一步预测</a></li>
                    <li>
                        <a href="#33-%e7%bb%bc%e5%90%88%e4%bb%a3%e7%a0%81" aria-label="3.3 综合代码">3.3 综合代码</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e9%9b%86" aria-label="4. 训练数据集">4. 训练数据集</a><ul>
                            
                    <li>
                        <a href="#41-nsp%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" aria-label="4.1 NSP数据处理">4.1 NSP数据处理</a></li>
                    <li>
                        <a href="#42-mlm%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" aria-label="4.2 MLM数据处理">4.2 MLM数据处理</a></li>
                    <li>
                        <a href="#43-%e6%b7%bb%e5%8a%a0%e5%a1%ab%e5%85%85%e8%af%8d%e5%85%83" aria-label="4.3 添加填充词元">4.3 添加填充词元</a></li>
                    <li>
                        <a href="#44-%e7%bb%bc%e5%90%88%e5%a4%84%e7%90%86" aria-label="4.4 综合处理">4.4 综合处理</a></li></ul>
                    </li>
                    <li>
                        <a href="#5-%e9%a2%84%e8%ae%ad%e7%bb%83bert" aria-label="5. 预训练BERT">5. 预训练BERT</a><ul>
                            
                    <li>
                        <a href="#51-%e9%a2%84%e8%ae%ad%e7%bb%83bert" aria-label="5.1 预训练BERT">5.1 预训练BERT</a></li>
                    <li>
                        <a href="#52-%e7%94%a8bert%e8%a1%a8%e7%a4%ba%e6%96%87%e6%9c%ac" aria-label="5.2 用BERT表示文本">5.2 用BERT表示文本</a></li></ul>
                    </li>
                    <li>
                        <a href="#6-%e5%be%ae%e8%b0%83bert" aria-label="6. 微调BERT">6. 微调BERT</a><ul>
                            
                    <li>
                        <a href="#61-snli%e6%95%b0%e6%8d%ae%e9%9b%86" aria-label="6.1 SNLI数据集">6.1 SNLI数据集</a></li>
                    <li>
                        <a href="#62-%e5%8a%a0%e8%bd%bd%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b" aria-label="6.2 加载预训练模型">6.2 加载预训练模型</a></li>
                    <li>
                        <a href="#63-%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86" aria-label="6.3 数据预处理">6.3 数据预处理</a></li>
                    <li>
                        <a href="#64-%e5%be%ae%e8%b0%83bert" aria-label="6.4 微调BERT">6.4 微调BERT</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>


  <div class="post-content"><h1 id="1-基础介绍">1. 基础介绍<a hidden class="anchor" aria-hidden="true" href="#1-基础介绍">#</a></h1>
<ul>
<li>BERT（来自Transformers的双向编码器表示）基于<strong>Transformer编码器</strong>进行预训练(Pre-train)，从而对输入词元进行上下文表示。</li>
<li>而在针对具体的自然语言处理任务的训练(Fine-tuning)时，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。</li>
</ul>
<img src="https://miro.medium.com/v2/resize:fit:1134/1*FzPes8JAlfFHi-7lR5yWKQ.png" alt="使用Pre-training的方法與時機| by Jia-Yau Shiau | 軟體之心| Medium | AI Blog TW" style="zoom:67%;" />
<h1 id="2-输入表示">2. 输入表示<a hidden class="anchor" aria-hidden="true" href="#2-输入表示">#</a></h1>
<p>BERT输入序列的Embedding嵌入，表示为词元嵌入、段嵌入和位置嵌入的矩阵加和。</p>
<ul>
<li><strong>词元嵌入</strong>：词元索引的独热编码的特征表示</li>
<li><strong>片段嵌入</strong>：标记来自第一个文本句，还是第二个文本句
<ul>
<li>每个序列开头加是一个&rsquo;&lt;cls&gt;&lsquo;特殊类别词元。</li>
<li>BERT输入序列可以包括1个或者2个文本句子，每个文本句后加一个&rsquo;&lt;sep&gt;&lsquo;特殊分隔词元</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> get_tokens_and_segments(tokens_a, tokens_b=<span style="color:#fff;font-weight:bold">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;获取输入序列的词元及其片段索引&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    tokens = [<span style="color:#0ff;font-weight:bold">&#39;&lt;cls&gt;&#39;</span>] + tokens_a + [<span style="color:#0ff;font-weight:bold">&#39;&lt;sep&gt;&#39;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 0和1分别标记片段A和B</span>
</span></span><span style="display:flex;"><span>    segments = [<span style="color:#ff0;font-weight:bold">0</span>] * (<span style="color:#fff;font-weight:bold">len</span>(tokens_a) + <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> tokens_b is not <span style="color:#fff;font-weight:bold">None</span>:
</span></span><span style="display:flex;"><span>        tokens += tokens_b + [<span style="color:#0ff;font-weight:bold">&#39;&lt;sep&gt;&#39;</span>]
</span></span><span style="display:flex;"><span>        segments += [<span style="color:#ff0;font-weight:bold">1</span>] * (<span style="color:#fff;font-weight:bold">len</span>(tokens_b) + <span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> tokens, segments
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>get_tokens_and_segments([<span style="color:#0ff;font-weight:bold">&#34;aa&#34;</span>],[<span style="color:#0ff;font-weight:bold">&#39;b&#39;</span>])
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ([&#39;&lt;cls&gt;&#39;, &#39;aa&#39;, &#39;&lt;sep&gt;&#39;, &#39;b&#39;, &#39;&lt;sep&gt;&#39;], [0, 0, 0, 1, 1])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>位置嵌入</strong>：与Transformer不同，BERT使用了可学习的位置编码</li>
</ul>
<p><img loading="lazy" src="C:%5cUsers%5cxiaoxin%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20240815083258457.png" alt="image-20240815083258457"  />
</p>
<ul>
<li>获取序列的输入表示后，就可以简单地应用到Transformer的编码器中</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> BERTEncoder(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;BERT编码器&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, vocab_size, num_hiddens, norm_shape, ffn_num_input,
</span></span><span style="display:flex;"><span>                 ffn_num_hiddens, num_heads, num_layers, dropout,
</span></span><span style="display:flex;"><span>                 max_len=<span style="color:#ff0;font-weight:bold">1000</span>, key_size=<span style="color:#ff0;font-weight:bold">768</span>, query_size=<span style="color:#ff0;font-weight:bold">768</span>, value_size=<span style="color:#ff0;font-weight:bold">768</span>,
</span></span><span style="display:flex;"><span>                 **kwargs):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(BERTEncoder, <span style="color:#fff;font-weight:bold">self</span>).__init__(**kwargs)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.token_embedding = nn.Embedding(vocab_size, num_hiddens) <span style="color:#007f7f">#词元嵌入</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.segment_embedding = nn.Embedding(<span style="color:#ff0;font-weight:bold">2</span>, num_hiddens) <span style="color:#007f7f">#段嵌入</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数（用多少截多少）</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.pos_embedding = nn.Parameter(torch.randn(<span style="color:#ff0;font-weight:bold">1</span>, max_len, num_hiddens))
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># n个Transformer编码层</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.blks = nn.Sequential()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> i in <span style="color:#fff;font-weight:bold">range</span>(num_layers):
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">self</span>.blks.add_module(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;</span><span style="color:#0ff;font-weight:bold">{</span>i<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>, d2l.EncoderBlock(
</span></span><span style="display:flex;"><span>                key_size, query_size, value_size, num_hiddens, norm_shape,
</span></span><span style="display:flex;"><span>                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span style="color:#fff;font-weight:bold">True</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, tokens, segments, valid_lens):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
</span></span><span style="display:flex;"><span>        X = <span style="color:#fff;font-weight:bold">self</span>.token_embedding(tokens) + <span style="color:#fff;font-weight:bold">self</span>.segment_embedding(segments)
</span></span><span style="display:flex;"><span>        X = X + <span style="color:#fff;font-weight:bold">self</span>.pos_embedding.data[:, :X.shape[<span style="color:#ff0;font-weight:bold">1</span>], :]
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> blk in <span style="color:#fff;font-weight:bold">self</span>.blks:
</span></span><span style="display:flex;"><span>            X = blk(X, valid_lens)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> X
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>示例</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 实例化一个encoder</span>
</span></span><span style="display:flex;"><span>vocab_size, num_hiddens, ffn_num_hiddens, num_heads = <span style="color:#ff0;font-weight:bold">10000</span>, <span style="color:#ff0;font-weight:bold">768</span>, <span style="color:#ff0;font-weight:bold">1024</span>, <span style="color:#ff0;font-weight:bold">4</span>
</span></span><span style="display:flex;"><span>norm_shape, ffn_num_input, num_layers, dropout = [<span style="color:#ff0;font-weight:bold">768</span>], <span style="color:#ff0;font-weight:bold">768</span>, <span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">0.2</span>
</span></span><span style="display:flex;"><span>encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
</span></span><span style="display:flex;"><span>                      ffn_num_hiddens, num_heads, num_layers, dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># demo输入：批量大小为2，序列长度为8</span>
</span></span><span style="display:flex;"><span>tokens = torch.randint(<span style="color:#ff0;font-weight:bold">0</span>, vocab_size, (<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">8</span>))
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 每个序列的段嵌入</span>
</span></span><span style="display:flex;"><span>segments = torch.tensor([[<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>], [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>]])
</span></span><span style="display:flex;"><span>encoded_X = encoder(tokens, segments, <span style="color:#fff;font-weight:bold">None</span>)
</span></span><span style="display:flex;"><span>encoded_X.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([2, 8, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="3-预训练任务">3. 预训练任务<a hidden class="anchor" aria-hidden="true" href="#3-预训练任务">#</a></h1>
<ul>
<li>上述的encoder前向推断给出了输入文本的每个词元和插入的特殊标记“&lt;cls&gt;”及“&lt;\seq&gt;”的BERT表示。</li>
<li>基于此，来计算预训练BERT的两个损失函数，从而以自监督(无标签)训练方式，学习词元的embedding</li>
</ul>
<h2 id="31-掩蔽语言模型">3.1 掩蔽语言模型<a hidden class="anchor" aria-hidden="true" href="#31-掩蔽语言模型">#</a></h2>
<p>Masked language modeling</p>
<p>BERT随机掩蔽(masked)词元，并使用来自双向上下文的词元预测掩蔽词元。</p>
<ul>
<li>这里的双向主要是指在学习特定词元的表示时，可以同时看到该词元前面和后面的序列信息。</li>
<li>这区别于GPT模型从左到右的学习预测方式。</li>
</ul>
<img src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png" alt="MLM — Sentence Transformers documentation" style="zoom:67%;" />
<p>具体来说，在该预训练中，将随机选择15%的词元作为预测的掩蔽/掩码词元。对于每个被选中的词元，会如下处理：</p>
<ul>
<li>80%可能变为特殊的“&lt;mask&gt;“词元（例如，“this movie is great”变为“this movie is&lt;\mask&gt;”；</li>
<li>10%时间为随机词元(噪音)（例如，“this movie is great”变为“this movie is drink”）；</li>
<li>10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”）。</li>
</ul>
<p>如下定义一个MaskLM类来预测上述的掩蔽标记</p>
<ul>
<li>输入：Encoder对词元的编码表示，用于预测的词元位置</li>
<li>输出：这些位置的预测结果</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> MaskLM(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;BERT的掩蔽语言模型任务&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, vocab_size, num_hiddens, num_inputs=<span style="color:#ff0;font-weight:bold">768</span>, **kwargs):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(MaskLM, <span style="color:#fff;font-weight:bold">self</span>).__init__(**kwargs)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 使用具有单隐藏层的MLP进行预测</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
</span></span><span style="display:flex;"><span>                                 nn.ReLU(),
</span></span><span style="display:flex;"><span>                                 nn.LayerNorm(num_hiddens),
</span></span><span style="display:flex;"><span>                                 nn.Linear(num_hiddens, vocab_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, X, pred_positions):
</span></span><span style="display:flex;"><span>        num_pred_positions = pred_positions.shape[<span style="color:#ff0;font-weight:bold">1</span>] <span style="color:#007f7f">#每个序列的masked词元数</span>
</span></span><span style="display:flex;"><span>        pred_positions = pred_positions.reshape(-<span style="color:#ff0;font-weight:bold">1</span>) <span style="color:#007f7f">#总共的masked词元数</span>
</span></span><span style="display:flex;"><span>        batch_size = X.shape[<span style="color:#ff0;font-weight:bold">0</span>]
</span></span><span style="display:flex;"><span>        batch_idx = torch.arange(<span style="color:#ff0;font-weight:bold">0</span>, batch_size)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 假设batch_size=2，num_pred_positions=3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span>
</span></span><span style="display:flex;"><span>        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)
</span></span><span style="display:flex;"><span>        masked_X = X[batch_idx, pred_positions]
</span></span><span style="display:flex;"><span>        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>        mlm_Y_hat = <span style="color:#fff;font-weight:bold">self</span>.mlp(masked_X)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> mlm_Y_hat
</span></span></code></pre></td></tr></table>
</div>
</div><p>示例</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mlm = MaskLM(vocab_size, num_hiddens)
</span></span><span style="display:flex;"><span>mlm_positions = torch.tensor([[<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2</span>], [<span style="color:#ff0;font-weight:bold">6</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">5</span>]])
</span></span><span style="display:flex;"><span>mlm_Y_hat = mlm(encoded_X, mlm_positions)
</span></span><span style="display:flex;"><span>mlm_Y_hat.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([2, 3, 10000])</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 10000为词表大小</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#计算与真实标签的loss</span>
</span></span><span style="display:flex;"><span>mlm_Y = torch.tensor([[<span style="color:#ff0;font-weight:bold">7</span>, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">9</span>], [<span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">20</span>, <span style="color:#ff0;font-weight:bold">30</span>]])
</span></span><span style="display:flex;"><span>loss = nn.CrossEntropyLoss(reduction=<span style="color:#0ff;font-weight:bold">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>mlm_l = loss(mlm_Y_hat.reshape((-<span style="color:#ff0;font-weight:bold">1</span>, vocab_size)), mlm_Y.reshape(-<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>mlm_l.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([6])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="32-下一步预测">3.2 下一步预测<a hidden class="anchor" aria-hidden="true" href="#32-下一步预测">#</a></h2>
<p>Next sentence prediction</p>
<ul>
<li>当BERT的输入序列包含两个句子时，可以预测两个句子是否存在上下文逻辑关系</li>
<li>此时，可以使用序列开头的<code>&lt;cls&gt;</code>词元嵌入表示整个序列，进行建模</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> NextSentencePred(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;BERT的下一句预测任务&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, num_inputs, **kwargs):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(NextSentencePred, <span style="color:#fff;font-weight:bold">self</span>).__init__(**kwargs)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.output = nn.Linear(num_inputs, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, X):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># X的形状：(batchsize,num_hiddens) 每个序列的&lt;cls&gt;词元表示</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> <span style="color:#fff;font-weight:bold">self</span>.output(X)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>示例</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>encoded_X = torch.flatten(encoded_X, start_dim=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([2, 6144]) 这里应该只是模拟合适的形状</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># NSP的输入形状:(batchsize，num_hiddens)</span>
</span></span><span style="display:flex;"><span>nsp = NextSentencePred(encoded_X.shape[-<span style="color:#ff0;font-weight:bold">1</span>])
</span></span><span style="display:flex;"><span>nsp_Y_hat = nsp(encoded_X)
</span></span><span style="display:flex;"><span>nsp_Y_hat.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([2, 2])</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 每个样本得到两个概率预测结果</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>nsp_y = torch.tensor([<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>]) <span style="color:#007f7f">#真实标签</span>
</span></span><span style="display:flex;"><span>nsp_l = loss(nsp_Y_hat, nsp_y)
</span></span><span style="display:flex;"><span>nsp_l.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># encoded_X.shape</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="33-综合代码">3.3 综合代码<a hidden class="anchor" aria-hidden="true" href="#33-综合代码">#</a></h2>
<p>基于上述定义的BERTEncoder类，以及两个预训练任务MaskLM和NextSentencePred来定义BERTModel</p>
<ul>
<li>输入的是词元相关信息</li>
<li>输出的是编码后BERT表示encoded_X、以及两个任务的预测结果</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> BERTModel(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;BERT模型&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, vocab_size, num_hiddens, norm_shape, ffn_num_input,
</span></span><span style="display:flex;"><span>                 ffn_num_hiddens, num_heads, num_layers, dropout,
</span></span><span style="display:flex;"><span>                 max_len=<span style="color:#ff0;font-weight:bold">1000</span>, key_size=<span style="color:#ff0;font-weight:bold">768</span>, query_size=<span style="color:#ff0;font-weight:bold">768</span>, value_size=<span style="color:#ff0;font-weight:bold">768</span>,
</span></span><span style="display:flex;"><span>                 hid_in_features=<span style="color:#ff0;font-weight:bold">768</span>, mlm_in_features=<span style="color:#ff0;font-weight:bold">768</span>,
</span></span><span style="display:flex;"><span>                 nsp_in_features=<span style="color:#ff0;font-weight:bold">768</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(BERTModel, <span style="color:#fff;font-weight:bold">self</span>).__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
</span></span><span style="display:flex;"><span>                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
</span></span><span style="display:flex;"><span>                    dropout, max_len=max_len, key_size=key_size,
</span></span><span style="display:flex;"><span>                    query_size=query_size, value_size=value_size)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.nsp = NextSentencePred(nsp_in_features)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 为nsp任务添加的一层中间隐藏层, 输入为encoder的cls输出，</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 输出是nsp的输入</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
</span></span><span style="display:flex;"><span>                                    nn.Tanh())
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, tokens, segments, valid_lens=<span style="color:#fff;font-weight:bold">None</span>,
</span></span><span style="display:flex;"><span>                pred_positions=<span style="color:#fff;font-weight:bold">None</span>):
</span></span><span style="display:flex;"><span>        encoded_X = <span style="color:#fff;font-weight:bold">self</span>.encoder(tokens, segments, valid_lens)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> pred_positions is not <span style="color:#fff;font-weight:bold">None</span>:
</span></span><span style="display:flex;"><span>            mlm_Y_hat = <span style="color:#fff;font-weight:bold">self</span>.mlm(encoded_X, pred_positions)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            mlm_Y_hat = <span style="color:#fff;font-weight:bold">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span>
</span></span><span style="display:flex;"><span>        nsp_Y_hat = <span style="color:#fff;font-weight:bold">self</span>.nsp(<span style="color:#fff;font-weight:bold">self</span>.hidden(encoded_X[:, <span style="color:#ff0;font-weight:bold">0</span>, :]))
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="4-训练数据集">4. 训练数据集<a hidden class="anchor" aria-hidden="true" href="#4-训练数据集">#</a></h1>
<ul>
<li>示例数据WikiText-2</li>
<li>教材中提供的下载链接失效了，在讨论区有小伙伴及时保存了，目前已下载上传到Github</li>
<li>将该文件手动上传到<code>../data/</code>目录下，再解压</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span>d2l.DATA_HUB[<span style="color:#0ff;font-weight:bold">&#39;wikitext-2&#39;</span>] = (
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#39;https://s3.amazonaws.com/research.metamind.io/wikitext/&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#39;wikitext-2-v1.zip&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;3c914d17d80b1459be871a5039ac23e752a53cbe&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 上述链接失效了，收到上传到服务器中</span>
</span></span><span style="display:flex;"><span>data_dir = <span style="color:#0ff;font-weight:bold">&#39;../data/wikitext-2&#39;</span>
</span></span><span style="display:flex;"><span>ls ../data/wikitext-<span style="color:#ff0;font-weight:bold">2</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># wiki.test.tokens  wiki.train.tokens  wiki.valid.tokens</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>预处理
<ul>
<li>一行代表一个段落</li>
<li>句号为分隔符，将一段拆成多个句子</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _read_wiki(data_dir):
</span></span><span style="display:flex;"><span>    file_name = os.path.join(data_dir, <span style="color:#0ff;font-weight:bold">&#39;wiki.train.tokens&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">with</span> <span style="color:#fff;font-weight:bold">open</span>(file_name, <span style="color:#0ff;font-weight:bold">&#39;r&#39;</span>) <span style="color:#fff;font-weight:bold">as</span> f:
</span></span><span style="display:flex;"><span>        lines = f.readlines()
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 大写字母转换为小写字母</span>
</span></span><span style="display:flex;"><span>    paragraphs = [line.strip().lower().split(<span style="color:#0ff;font-weight:bold">&#39; . &#39;</span>)
</span></span><span style="display:flex;"><span>                  <span style="color:#fff;font-weight:bold">for</span> line in lines <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">len</span>(line.split(<span style="color:#0ff;font-weight:bold">&#39; . &#39;</span>)) &gt;= <span style="color:#ff0;font-weight:bold">2</span>]
</span></span><span style="display:flex;"><span>    random.shuffle(paragraphs)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> paragraphs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 示例</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 词元化处理</span>
</span></span><span style="display:flex;"><span>paragraphs = [d2l.tokenize(
</span></span><span style="display:flex;"><span>    paragraph, token=<span style="color:#0ff;font-weight:bold">&#39;word&#39;</span>) <span style="color:#fff;font-weight:bold">for</span> paragraph in _read_wiki(data_dir)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 3层嵌套的list</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 第一层：段落</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 第二层：句子</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 第三层：词元</span>
</span></span><span style="display:flex;"><span>paragraphs[<span style="color:#ff0;font-weight:bold">0</span>][<span style="color:#ff0;font-weight:bold">0</span>][:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># [&#39;families&#39;, &#39;of&#39;, &#39;45&#39;]</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 第一段的第一句的前三个词元</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="41-nsp数据处理">4.1 NSP数据处理<a hidden class="anchor" aria-hidden="true" href="#41-nsp数据处理">#</a></h2>
<ul>
<li>将两句话合并作为一个样本序列
<ul>
<li>若两句话来自于同一段落的上下文，则为阳性；其余为阴性。</li>
<li>将两种情况序列设置各占50%</li>
</ul>
</li>
<li>如下函数，50%概率返回两个连续的句子（阳性），50%的概率不会</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _get_next_sentence(sentence, next_sentence, paragraphs):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> random.random() &lt; <span style="color:#ff0;font-weight:bold">0.5</span>:
</span></span><span style="display:flex;"><span>        is_next = <span style="color:#fff;font-weight:bold">True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 阴性：随机选择第二个句子</span>
</span></span><span style="display:flex;"><span>        next_sentence = random.choice(random.choice(paragraphs))
</span></span><span style="display:flex;"><span>        is_next = <span style="color:#fff;font-weight:bold">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> sentence, next_sentence, is_next
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>如下函数，将对每个段落生成具有两个句子的阳性或阴性序列
<ul>
<li>返回一个list，包含该段落所有的样本序列，片段嵌入，以及是否连续句子的逻辑符</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
</span></span><span style="display:flex;"><span>    nsp_data_from_paragraph = []
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> i in <span style="color:#fff;font-weight:bold">range</span>(<span style="color:#fff;font-weight:bold">len</span>(paragraph) - <span style="color:#ff0;font-weight:bold">1</span>):
</span></span><span style="display:flex;"><span>        tokens_a, tokens_b, is_next = _get_next_sentence(
</span></span><span style="display:flex;"><span>            paragraph[i], paragraph[i + <span style="color:#ff0;font-weight:bold">1</span>], paragraphs)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 考虑1个&#39;&lt;cls&gt;&#39;词元和2个&#39;&lt;sep&gt;&#39;词元</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">len</span>(tokens_a) + <span style="color:#fff;font-weight:bold">len</span>(tokens_b) + <span style="color:#ff0;font-weight:bold">3</span> &gt; max_len:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">continue</span>
</span></span><span style="display:flex;"><span>        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
</span></span><span style="display:flex;"><span>        nsp_data_from_paragraph.append((tokens, segments, is_next))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> nsp_data_from_paragraph
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="42-mlm数据处理">4.2 MLM数据处理<a hidden class="anchor" aria-hidden="true" href="#42-mlm数据处理">#</a></h2>
<blockquote>
<p>这里所说的序列就是指4.1步骤得到的合并两个句子的词元序列</p></blockquote>
<ul>
<li>为每个输入序列做掩码处理，并记录被替换的位置，以及正确的标签
<ul>
<li>这里的位置就是每个样本序列的词元的位置（不是索引）</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
</span></span><span style="display:flex;"><span>                        vocab):
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“&lt;mask&gt;”或随机词元</span>
</span></span><span style="display:flex;"><span>    mlm_input_tokens = [token <span style="color:#fff;font-weight:bold">for</span> token in tokens]
</span></span><span style="display:flex;"><span>    pred_positions_and_labels = []
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测</span>
</span></span><span style="display:flex;"><span>    random.shuffle(candidate_pred_positions)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> mlm_pred_position in candidate_pred_positions:
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">len</span>(pred_positions_and_labels) &gt;= num_mlm_preds:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">break</span>
</span></span><span style="display:flex;"><span>        masked_token = <span style="color:#fff;font-weight:bold">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 80%的时间：将词替换为“&lt;mask&gt;”词元</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> random.random() &lt; <span style="color:#ff0;font-weight:bold">0.8</span>:
</span></span><span style="display:flex;"><span>            masked_token = <span style="color:#0ff;font-weight:bold">&#39;&lt;mask&gt;&#39;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 10%的时间：保持词不变</span>
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">if</span> random.random() &lt; <span style="color:#ff0;font-weight:bold">0.5</span>:
</span></span><span style="display:flex;"><span>                masked_token = tokens[mlm_pred_position]
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 10%的时间：用随机词替换该词</span>
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>                masked_token = random.choice(vocab.idx_to_token)
</span></span><span style="display:flex;"><span>        mlm_input_tokens[mlm_pred_position] = masked_token
</span></span><span style="display:flex;"><span>        pred_positions_and_labels.append(
</span></span><span style="display:flex;"><span>            (mlm_pred_position, tokens[mlm_pred_position]))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> mlm_input_tokens, pred_positions_and_labels
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>输入：词元序列以及相应的词表</li>
<li>输出：序列索引信息，以及待预测的位置信息以及正确的标签</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _get_mlm_data_from_tokens(tokens, vocab):
</span></span><span style="display:flex;"><span>    candidate_pred_positions = []
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># tokens是一个字符串列表</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> i, token in <span style="color:#fff;font-weight:bold">enumerate</span>(tokens):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 在遮蔽语言模型任务中不会预测特殊词元</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> token in [<span style="color:#0ff;font-weight:bold">&#39;&lt;cls&gt;&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;&lt;sep&gt;&#39;</span>]:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">continue</span>
</span></span><span style="display:flex;"><span>        candidate_pred_positions.append(i)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 遮蔽语言模型任务中预测15%的随机词元</span>
</span></span><span style="display:flex;"><span>    num_mlm_preds = <span style="color:#fff;font-weight:bold">max</span>(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#fff;font-weight:bold">round</span>(<span style="color:#fff;font-weight:bold">len</span>(tokens) * <span style="color:#ff0;font-weight:bold">0.15</span>))
</span></span><span style="display:flex;"><span>    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
</span></span><span style="display:flex;"><span>        tokens, candidate_pred_positions, num_mlm_preds, vocab)
</span></span><span style="display:flex;"><span>    pred_positions_and_labels = <span style="color:#fff;font-weight:bold">sorted</span>(pred_positions_and_labels,
</span></span><span style="display:flex;"><span>                                       key=<span style="color:#fff;font-weight:bold">lambda</span> x: x[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>    pred_positions = [v[<span style="color:#ff0;font-weight:bold">0</span>] <span style="color:#fff;font-weight:bold">for</span> v in pred_positions_and_labels]
</span></span><span style="display:flex;"><span>    mlm_pred_labels = [v[<span style="color:#ff0;font-weight:bold">1</span>] <span style="color:#fff;font-weight:bold">for</span> v in pred_positions_and_labels]
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="43-添加填充词元">4.3 添加填充词元<a hidden class="anchor" aria-hidden="true" href="#43-添加填充词元">#</a></h2>
<ul>
<li>为每个序列添加填充词元&rsquo;&lt;pad&gt;&rsquo;，从而保证输入序列的长度固定，并记录有效长度</li>
<li>同时，序列相关的其它标记信息也需要随之补充到固定长度</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _pad_bert_inputs(examples, max_len, vocab):
</span></span><span style="display:flex;"><span>    max_num_mlm_preds = <span style="color:#fff;font-weight:bold">round</span>(max_len * <span style="color:#ff0;font-weight:bold">0.15</span>)
</span></span><span style="display:flex;"><span>    all_token_ids, all_segments, valid_lens,  = [], [], []
</span></span><span style="display:flex;"><span>    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
</span></span><span style="display:flex;"><span>    nsp_labels = []
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> (token_ids, pred_positions, mlm_pred_label_ids, segments,
</span></span><span style="display:flex;"><span>         is_next) in examples:
</span></span><span style="display:flex;"><span>        all_token_ids.append(torch.tensor(token_ids + [vocab[<span style="color:#0ff;font-weight:bold">&#39;&lt;pad&gt;&#39;</span>]] * (
</span></span><span style="display:flex;"><span>            max_len - <span style="color:#fff;font-weight:bold">len</span>(token_ids)), dtype=torch.long))
</span></span><span style="display:flex;"><span>        all_segments.append(torch.tensor(segments + [<span style="color:#ff0;font-weight:bold">0</span>] * (
</span></span><span style="display:flex;"><span>            max_len - <span style="color:#fff;font-weight:bold">len</span>(segments)), dtype=torch.long)) <span style="color:#007f7f"># 片段标记补长</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># valid_lens不包括&#39;&lt;pad&gt;&#39;的计数</span>
</span></span><span style="display:flex;"><span>        valid_lens.append(torch.tensor(<span style="color:#fff;font-weight:bold">len</span>(token_ids), dtype=torch.float32))
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 预测词元标记补长: 固定长度的15% - 有效长度的15%</span>
</span></span><span style="display:flex;"><span>        all_pred_positions.append(torch.tensor(pred_positions + [<span style="color:#ff0;font-weight:bold">0</span>] * (
</span></span><span style="display:flex;"><span>            max_num_mlm_preds - <span style="color:#fff;font-weight:bold">len</span>(pred_positions)), dtype=torch.long)) 
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 填充词元的预测将通过乘以0权重在损失中过滤掉</span>
</span></span><span style="display:flex;"><span>        all_mlm_weights.append(
</span></span><span style="display:flex;"><span>            torch.tensor([<span style="color:#ff0;font-weight:bold">1.0</span>] * <span style="color:#fff;font-weight:bold">len</span>(mlm_pred_label_ids) + [<span style="color:#ff0;font-weight:bold">0.0</span>] * (
</span></span><span style="display:flex;"><span>                max_num_mlm_preds - <span style="color:#fff;font-weight:bold">len</span>(pred_positions)),
</span></span><span style="display:flex;"><span>                dtype=torch.float32)) 
</span></span><span style="display:flex;"><span>        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [<span style="color:#ff0;font-weight:bold">0</span>] * (
</span></span><span style="display:flex;"><span>            max_num_mlm_preds - <span style="color:#fff;font-weight:bold">len</span>(mlm_pred_label_ids)), dtype=torch.long)) <span style="color:#007f7f"># 预测词元真实值，也需要补长</span>
</span></span><span style="display:flex;"><span>        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> (all_token_ids, all_segments, valid_lens, all_pred_positions,
</span></span><span style="display:flex;"><span>            all_mlm_weights, all_mlm_labels, nsp_labels)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="44-综合处理">4.4 综合处理<a hidden class="anchor" aria-hidden="true" href="#44-综合处理">#</a></h2>
<ul>
<li>all_token_ids: 输入序列的列表</li>
<li>all_segments: 输入序列片段标记的列表</li>
<li>valid_lens: 每个序列有效长度的列表</li>
<li>all_pred_positions, all_mlm_weights, all_mlm_labels: mlm任务中预测词元的信息</li>
<li>nsp_labels: 是否为连续句子对的逻辑符</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">34
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> _WikiTextDataset(torch.utils.data.Dataset):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, paragraphs, max_len):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 输入paragraphs[i]是代表段落的句子字符串列表；</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表</span>
</span></span><span style="display:flex;"><span>        paragraphs = [d2l.tokenize(
</span></span><span style="display:flex;"><span>            paragraph, token=<span style="color:#0ff;font-weight:bold">&#39;word&#39;</span>) <span style="color:#fff;font-weight:bold">for</span> paragraph in paragraphs]
</span></span><span style="display:flex;"><span>        sentences = [sentence <span style="color:#fff;font-weight:bold">for</span> paragraph in paragraphs
</span></span><span style="display:flex;"><span>                     <span style="color:#fff;font-weight:bold">for</span> sentence in paragraph]
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.vocab = d2l.Vocab(sentences, min_freq=<span style="color:#ff0;font-weight:bold">5</span>, reserved_tokens=[
</span></span><span style="display:flex;"><span>            <span style="color:#0ff;font-weight:bold">&#39;&lt;pad&gt;&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;&lt;mask&gt;&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;&lt;cls&gt;&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;&lt;sep&gt;&#39;</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 获取下一句子预测任务的数据</span>
</span></span><span style="display:flex;"><span>        examples = []
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> paragraph in paragraphs:
</span></span><span style="display:flex;"><span>            examples.extend(_get_nsp_data_from_paragraph(
</span></span><span style="display:flex;"><span>                paragraph, paragraphs, <span style="color:#fff;font-weight:bold">self</span>.vocab, max_len))
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 获取遮蔽语言模型任务的数据</span>
</span></span><span style="display:flex;"><span>        examples = [(_get_mlm_data_from_tokens(tokens, <span style="color:#fff;font-weight:bold">self</span>.vocab)
</span></span><span style="display:flex;"><span>                      + (segments, is_next))
</span></span><span style="display:flex;"><span>                     <span style="color:#fff;font-weight:bold">for</span> tokens, segments, is_next in examples]
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 填充输入</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#fff;font-weight:bold">self</span>.all_token_ids, <span style="color:#fff;font-weight:bold">self</span>.all_segments, <span style="color:#fff;font-weight:bold">self</span>.valid_lens,
</span></span><span style="display:flex;"><span>         <span style="color:#fff;font-weight:bold">self</span>.all_pred_positions, <span style="color:#fff;font-weight:bold">self</span>.all_mlm_weights,
</span></span><span style="display:flex;"><span>         <span style="color:#fff;font-weight:bold">self</span>.all_mlm_labels, <span style="color:#fff;font-weight:bold">self</span>.nsp_labels) = _pad_bert_inputs(
</span></span><span style="display:flex;"><span>            examples, max_len, <span style="color:#fff;font-weight:bold">self</span>.vocab)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __getitem__(<span style="color:#fff;font-weight:bold">self</span>, idx):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> (<span style="color:#fff;font-weight:bold">self</span>.all_token_ids[idx], <span style="color:#fff;font-weight:bold">self</span>.all_segments[idx],
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">self</span>.valid_lens[idx], <span style="color:#fff;font-weight:bold">self</span>.all_pred_positions[idx],
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">self</span>.all_mlm_weights[idx], <span style="color:#fff;font-weight:bold">self</span>.all_mlm_labels[idx],
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">self</span>.nsp_labels[idx])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __len__(<span style="color:#fff;font-weight:bold">self</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> <span style="color:#fff;font-weight:bold">len</span>(<span style="color:#fff;font-weight:bold">self</span>.all_token_ids)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>制作成数据迭代器</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> load_data_wiki(batch_size, max_len):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;加载WikiText-2数据集&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    num_workers = d2l.get_dataloader_workers()
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># data_dir = d2l.download_extract(&#39;wikitext-2&#39;, &#39;wikitext-2&#39;)</span>
</span></span><span style="display:flex;"><span>    data_dir = <span style="color:#0ff;font-weight:bold">&#39;../data/wikitext-2&#39;</span>
</span></span><span style="display:flex;"><span>    paragraphs = _read_wiki(data_dir)
</span></span><span style="display:flex;"><span>    train_set = _WikiTextDataset(paragraphs, max_len)
</span></span><span style="display:flex;"><span>    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
</span></span><span style="display:flex;"><span>                                        shuffle=<span style="color:#fff;font-weight:bold">True</span>, num_workers=num_workers)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> train_iter, train_set.vocab
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 词元序列最大长度设置为64</span>
</span></span><span style="display:flex;"><span>batch_size, max_len = <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">64</span>
</span></span><span style="display:flex;"><span>train_iter, vocab = load_data_wiki(batch_size, max_len)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
</span></span><span style="display:flex;"><span>     mlm_Y, nsp_y) in train_iter:
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(tokens_X.shape, segments_X.shape, valid_lens_x.shape,
</span></span><span style="display:flex;"><span>          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,
</span></span><span style="display:flex;"><span>          nsp_y.shape)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">break</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) </span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pred_positions_X[<span style="color:#ff0;font-weight:bold">0</span>,:]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([ 4,  6, 18, 28, 35, 37, 39, 49,  0,  0])</span>
</span></span><span style="display:flex;"><span>mlm_Y[<span style="color:#ff0;font-weight:bold">0</span>,:]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([3003,  443,  239, 1792,   48,    6,  814,   18,    0,    0])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="5-预训练bert">5. 预训练BERT<a hidden class="anchor" aria-hidden="true" href="#5-预训练bert">#</a></h1>
<h2 id="51-预训练bert">5.1 预训练BERT<a hidden class="anchor" aria-hidden="true" href="#51-预训练bert">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>实例化模型</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch_size, max_len = <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">64</span>
</span></span><span style="display:flex;"><span>train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net = d2l.BERTModel(<span style="color:#fff;font-weight:bold">len</span>(vocab), num_hiddens=<span style="color:#ff0;font-weight:bold">128</span>, norm_shape=[<span style="color:#ff0;font-weight:bold">128</span>],
</span></span><span style="display:flex;"><span>                    ffn_num_input=<span style="color:#ff0;font-weight:bold">128</span>, ffn_num_hiddens=<span style="color:#ff0;font-weight:bold">256</span>, num_heads=<span style="color:#ff0;font-weight:bold">2</span>,
</span></span><span style="display:flex;"><span>                    num_layers=<span style="color:#ff0;font-weight:bold">2</span>, dropout=<span style="color:#ff0;font-weight:bold">0.2</span>, key_size=<span style="color:#ff0;font-weight:bold">128</span>, query_size=<span style="color:#ff0;font-weight:bold">128</span>,
</span></span><span style="display:flex;"><span>                    value_size=<span style="color:#ff0;font-weight:bold">128</span>, hid_in_features=<span style="color:#ff0;font-weight:bold">128</span>, mlm_in_features=<span style="color:#ff0;font-weight:bold">128</span>,
</span></span><span style="display:flex;"><span>                    nsp_in_features=<span style="color:#ff0;font-weight:bold">128</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>损失函数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loss = nn.CrossEntropyLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
</span></span><span style="display:flex;"><span>                         segments_X, valid_lens_x,
</span></span><span style="display:flex;"><span>                         pred_positions_X, mlm_weights_X,
</span></span><span style="display:flex;"><span>                         mlm_Y, nsp_y):
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 前向传播</span>
</span></span><span style="display:flex;"><span>    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
</span></span><span style="display:flex;"><span>                                  valid_lens_x.reshape(-<span style="color:#ff0;font-weight:bold">1</span>),
</span></span><span style="display:flex;"><span>                                  pred_positions_X)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 计算遮蔽语言模型损失</span>
</span></span><span style="display:flex;"><span>    mlm_l = loss(mlm_Y_hat.reshape(-<span style="color:#ff0;font-weight:bold">1</span>, vocab_size), mlm_Y.reshape(-<span style="color:#ff0;font-weight:bold">1</span>)) *\
</span></span><span style="display:flex;"><span>    mlm_weights_X.reshape(-<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + <span style="color:#ff0;font-weight:bold">1e-8</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 计算下一句子预测任务的损失</span>
</span></span><span style="display:flex;"><span>    nsp_l = loss(nsp_Y_hat, nsp_y)
</span></span><span style="display:flex;"><span>    l = mlm_l + nsp_l
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> mlm_l, nsp_l, l
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>GPU训练模型</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">45
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>devices = d2l.try_all_gpus()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
</span></span><span style="display:flex;"><span>    net = nn.DataParallel(net, device_ids=devices).to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>    trainer = torch.optim.Adam(net.parameters(), lr=<span style="color:#ff0;font-weight:bold">0.01</span>)
</span></span><span style="display:flex;"><span>    step, timer = <span style="color:#ff0;font-weight:bold">0</span>, d2l.Timer()
</span></span><span style="display:flex;"><span>    animator = d2l.Animator(xlabel=<span style="color:#0ff;font-weight:bold">&#39;step&#39;</span>, ylabel=<span style="color:#0ff;font-weight:bold">&#39;loss&#39;</span>,
</span></span><span style="display:flex;"><span>                            xlim=[<span style="color:#ff0;font-weight:bold">1</span>, num_steps], legend=[<span style="color:#0ff;font-weight:bold">&#39;mlm&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;nsp&#39;</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数</span>
</span></span><span style="display:flex;"><span>    metric = d2l.Accumulator(<span style="color:#ff0;font-weight:bold">4</span>)
</span></span><span style="display:flex;"><span>    num_steps_reached = <span style="color:#fff;font-weight:bold">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">while</span> step &lt; num_steps and not num_steps_reached:
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> tokens_X, segments_X, valid_lens_x, pred_positions_X,\
</span></span><span style="display:flex;"><span>            mlm_weights_X, mlm_Y, nsp_y in train_iter:
</span></span><span style="display:flex;"><span>            tokens_X = tokens_X.to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>            segments_X = segments_X.to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>            valid_lens_x = valid_lens_x.to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>            pred_positions_X = pred_positions_X.to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>            mlm_weights_X = mlm_weights_X.to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>            mlm_Y, nsp_y = mlm_Y.to(devices[<span style="color:#ff0;font-weight:bold">0</span>]), nsp_y.to(devices[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span>            trainer.zero_grad()
</span></span><span style="display:flex;"><span>            timer.start()
</span></span><span style="display:flex;"><span>            mlm_l, nsp_l, l = _get_batch_loss_bert(
</span></span><span style="display:flex;"><span>                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
</span></span><span style="display:flex;"><span>                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
</span></span><span style="display:flex;"><span>            l.backward()
</span></span><span style="display:flex;"><span>            trainer.step()
</span></span><span style="display:flex;"><span>            metric.add(mlm_l, nsp_l, tokens_X.shape[<span style="color:#ff0;font-weight:bold">0</span>], <span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>            timer.stop()
</span></span><span style="display:flex;"><span>            animator.add(step + <span style="color:#ff0;font-weight:bold">1</span>,
</span></span><span style="display:flex;"><span>                         (metric[<span style="color:#ff0;font-weight:bold">0</span>] / metric[<span style="color:#ff0;font-weight:bold">3</span>], metric[<span style="color:#ff0;font-weight:bold">1</span>] / metric[<span style="color:#ff0;font-weight:bold">3</span>]))
</span></span><span style="display:flex;"><span>            step += <span style="color:#ff0;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">if</span> step == num_steps:
</span></span><span style="display:flex;"><span>                num_steps_reached = <span style="color:#fff;font-weight:bold">True</span>
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#39;MLM loss </span><span style="color:#0ff;font-weight:bold">{</span>metric[<span style="color:#ff0;font-weight:bold">0</span>] / metric[<span style="color:#ff0;font-weight:bold">3</span>]<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.3f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, &#39;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#39;NSP loss </span><span style="color:#0ff;font-weight:bold">{</span>metric[<span style="color:#ff0;font-weight:bold">1</span>] / metric[<span style="color:#ff0;font-weight:bold">3</span>]<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.3f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#39;</span><span style="color:#0ff;font-weight:bold">{</span>metric[<span style="color:#ff0;font-weight:bold">2</span>] / timer.sum()<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.1f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> sentence pairs/sec on &#39;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#39;</span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">str</span>(devices)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 结果</span>
</span></span><span style="display:flex;"><span>train_bert(train_iter, net, loss, <span style="color:#fff;font-weight:bold">len</span>(vocab), devices, <span style="color:#ff0;font-weight:bold">50</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># MLM loss 5.816, NSP loss 0.757</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 6688.3 sentence pairs/sec on [device(type=&#39;cuda&#39;, index=0), device(type=&#39;cuda&#39;, index=1)]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="52-用bert表示文本">5.2 用BERT表示文本<a hidden class="anchor" aria-hidden="true" href="#52-用bert表示文本">#</a></h2>
<ul>
<li>给定输入句子，经BERT编码后，返回每个词元的嵌入表示</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> get_bert_encoding(net, tokens_a, tokens_b=<span style="color:#fff;font-weight:bold">None</span>):
</span></span><span style="display:flex;"><span>    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
</span></span><span style="display:flex;"><span>    token_ids = torch.tensor(vocab[tokens], device=devices[<span style="color:#ff0;font-weight:bold">0</span>]).unsqueeze(<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>    segments = torch.tensor(segments, device=devices[<span style="color:#ff0;font-weight:bold">0</span>]).unsqueeze(<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>    valid_len = torch.tensor(<span style="color:#fff;font-weight:bold">len</span>(tokens), device=devices[<span style="color:#ff0;font-weight:bold">0</span>]).unsqueeze(<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>    encoded_X, _, _ = net(token_ids, segments, valid_len)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> encoded_X
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>句子1：<code>a crane is flying</code>, 加上头的<code>&lt;cls&gt;</code>以及尾的<code>&lt;sep&gt;</code>，共有6个词元</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokens_a = [<span style="color:#0ff;font-weight:bold">&#39;a&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;crane&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;is&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;flying&#39;</span>]
</span></span><span style="display:flex;"><span>encoded_text = get_bert_encoding(net, tokens_a)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 词元：&#39;&lt;cls&gt;&#39;,&#39;a&#39;,&#39;crane&#39;,&#39;is&#39;,&#39;flying&#39;,&#39;&lt;sep&gt;&#39;</span>
</span></span><span style="display:flex;"><span>encoded_text.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([1, 6, 128])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>encoded_text_cls = encoded_text[:, <span style="color:#ff0;font-weight:bold">0</span>, :]
</span></span><span style="display:flex;"><span>encoded_text_crane = encoded_text[:, <span style="color:#ff0;font-weight:bold">2</span>, :]
</span></span><span style="display:flex;"><span>encoded_text_cls.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([1, 128])</span>
</span></span><span style="display:flex;"><span>encoded_text_crane[<span style="color:#ff0;font-weight:bold">0</span>][:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([-0.3421, -0.6974, -1.9238], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>句子2: <code>a crane driver came</code></li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokens_a2 = [<span style="color:#0ff;font-weight:bold">&#39;a&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;crane&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;driver&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;came&#39;</span>]
</span></span><span style="display:flex;"><span>encoded_text2 = get_bert_encoding(net, tokens_a2)
</span></span><span style="display:flex;"><span>encoded_text_crane2 = encoded_text2[:, <span style="color:#ff0;font-weight:bold">2</span>, :]
</span></span><span style="display:flex;"><span>encoded_text_crane2[<span style="color:#ff0;font-weight:bold">0</span>][:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([-0.3659, -0.6927, -1.8660], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="6-微调bert">6. 微调BERT<a hidden class="anchor" aria-hidden="true" href="#6-微调bert">#</a></h1>
<h2 id="61-snli数据集">6.1 SNLI数据集<a hidden class="anchor" aria-hidden="true" href="#61-snli数据集">#</a></h2>
<ul>
<li>一对文本序列（前提与假设）通常可认为存在3种逻辑关系：
<ul>
<li>（1）蕴含 entailment：前提可以推出假设</li>
<li>（2）矛盾contradiction：二者逻辑互相矛盾</li>
<li>（3）中性neutral：无法确定</li>
</ul>
</li>
<li>斯坦福自然语言推断(Stanford natural language inferrence, SNLI)数据集包含了50多万个带标签的英语句子组成的集合。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># #@save</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># d2l.DATA_HUB[&#39;SNLI&#39;] = (</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#     &#39;https://nlp.stanford.edu/projects/snli/snli_1.0.zip&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#     &#39;9fcde07509c7e87ec61c640c1b2753d9041758e4&#39;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># data_dir = d2l.download_extract(&#39;SNLI&#39;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 下载snli_1.0.zip到 ../data，解压后，重命名为SNLI</span>
</span></span><span style="display:flex;"><span>data_dir = <span style="color:#0ff;font-weight:bold">&#39;../data/SNLI&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>定义如下代码，读取该数据集
<ul>
<li>分为训练集与测试集，前者约有55万对；后者约有1万对</li>
<li>每个数据集返回a list of 3 list，每个子list分别包括所有的前提序列，假设序列，以及二者的标签。</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">39
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">#@save</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> read_snli(data_dir, is_train):
</span></span><span style="display:flex;"><span>    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;将SNLI数据集解析为前提、假设和标签&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> extract_text(s):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 删除我们不会使用的信息</span>
</span></span><span style="display:flex;"><span>        s = re.sub(<span style="color:#0ff;font-weight:bold">&#39;</span><span style="color:#0ff;font-weight:bold">\\</span><span style="color:#0ff;font-weight:bold">(&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;&#39;</span>, s)
</span></span><span style="display:flex;"><span>        s = re.sub(<span style="color:#0ff;font-weight:bold">&#39;</span><span style="color:#0ff;font-weight:bold">\\</span><span style="color:#0ff;font-weight:bold">)&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;&#39;</span>, s)
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 用一个空格替换两个或多个连续的空格</span>
</span></span><span style="display:flex;"><span>        s = re.sub(<span style="color:#0ff;font-weight:bold">&#39;</span><span style="color:#0ff;font-weight:bold">\\</span><span style="color:#0ff;font-weight:bold">s{2,}&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39; &#39;</span>, s)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> s.strip()
</span></span><span style="display:flex;"><span>    label_set = {<span style="color:#0ff;font-weight:bold">&#39;entailment&#39;</span>: <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#0ff;font-weight:bold">&#39;contradiction&#39;</span>: <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#0ff;font-weight:bold">&#39;neutral&#39;</span>: <span style="color:#ff0;font-weight:bold">2</span>}
</span></span><span style="display:flex;"><span>    file_name = os.path.join(data_dir, <span style="color:#0ff;font-weight:bold">&#39;snli_1.0_train.txt&#39;</span>
</span></span><span style="display:flex;"><span>                             <span style="color:#fff;font-weight:bold">if</span> is_train <span style="color:#fff;font-weight:bold">else</span> <span style="color:#0ff;font-weight:bold">&#39;snli_1.0_test.txt&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">with</span> <span style="color:#fff;font-weight:bold">open</span>(file_name, <span style="color:#0ff;font-weight:bold">&#39;r&#39;</span>) <span style="color:#fff;font-weight:bold">as</span> f:
</span></span><span style="display:flex;"><span>        rows = [row.split(<span style="color:#0ff;font-weight:bold">&#39;</span><span style="color:#0ff;font-weight:bold">\t</span><span style="color:#0ff;font-weight:bold">&#39;</span>) <span style="color:#fff;font-weight:bold">for</span> row in f.readlines()[<span style="color:#ff0;font-weight:bold">1</span>:]]
</span></span><span style="display:flex;"><span>    premises = [extract_text(row[<span style="color:#ff0;font-weight:bold">1</span>]) <span style="color:#fff;font-weight:bold">for</span> row in rows <span style="color:#fff;font-weight:bold">if</span> row[<span style="color:#ff0;font-weight:bold">0</span>] in label_set]
</span></span><span style="display:flex;"><span>    hypotheses = [extract_text(row[<span style="color:#ff0;font-weight:bold">2</span>]) <span style="color:#fff;font-weight:bold">for</span> row in rows <span style="color:#fff;font-weight:bold">if</span> row[<span style="color:#ff0;font-weight:bold">0</span>] \
</span></span><span style="display:flex;"><span>                in label_set]
</span></span><span style="display:flex;"><span>    labels = [label_set[row[<span style="color:#ff0;font-weight:bold">0</span>]] <span style="color:#fff;font-weight:bold">for</span> row in rows <span style="color:#fff;font-weight:bold">if</span> row[<span style="color:#ff0;font-weight:bold">0</span>] in label_set]
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> premises, hypotheses, labels
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_data = read_snli(data_dir, is_train=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">len</span>(train_data), <span style="color:#fff;font-weight:bold">len</span>(train_data[<span style="color:#ff0;font-weight:bold">0</span>])
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># (3, 549367)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> x0, x1, y in <span style="color:#fff;font-weight:bold">zip</span>(train_data[<span style="color:#ff0;font-weight:bold">0</span>][:<span style="color:#ff0;font-weight:bold">3</span>], train_data[<span style="color:#ff0;font-weight:bold">1</span>][:<span style="color:#ff0;font-weight:bold">3</span>], train_data[<span style="color:#ff0;font-weight:bold">2</span>][:<span style="color:#ff0;font-weight:bold">3</span>]):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;前提：&#39;</span>, x0)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;假设：&#39;</span>, x1)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;标签：&#39;</span>, y)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 前提： A person on a horse jumps over a broken down airplane .</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 假设： A person is training his horse for a competition .</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 标签： 2</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 前提： A person on a horse jumps over a broken down airplane .</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 假设： A person is at a diner , ordering an omelette .</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 标签： 1</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 前提： A person on a horse jumps over a broken down airplane .</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 假设： A person is outdoors , on a horse .</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 标签： 0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="62-加载预训练模型">6.2 加载预训练模型<a hidden class="anchor" aria-hidden="true" href="#62-加载预训练模型">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> multiprocessing
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>教材提供了两个预训练好的bert模型，分别是base和small两个版本</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># d2l.DATA_HUB[&#39;bert.base&#39;] = (d2l.DATA_URL + &#39;bert.base.torch.zip&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#                              &#39;225d66f04cae318b841a13d32af3acc165f253ac&#39;)</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># d2l.DATA_HUB[&#39;bert.small&#39;] = (d2l.DATA_URL + &#39;bert.small.torch.zip&#39;,</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#                               &#39;c72329e68a732bef0452e4b96a1c341c8910f81f&#39;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,
</span></span><span style="display:flex;"><span>                          num_heads, num_layers, dropout, max_len, devices):
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># data_dir = d2l.download_extract(pretrained_model)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># http://d2l-data.s3-accelerate.amazonaws.com/bert.small.torch.zip</span>
</span></span><span style="display:flex;"><span>    data_dir = <span style="color:#0ff;font-weight:bold">&#39;../data/bert.small&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 定义空词表以加载预定义词表</span>
</span></span><span style="display:flex;"><span>    vocab = d2l.Vocab()
</span></span><span style="display:flex;"><span>    vocab.idx_to_token = json.load(<span style="color:#fff;font-weight:bold">open</span>(os.path.join(data_dir,
</span></span><span style="display:flex;"><span>        <span style="color:#0ff;font-weight:bold">&#39;vocab.json&#39;</span>)))
</span></span><span style="display:flex;"><span>    vocab.token_to_idx = {token: idx <span style="color:#fff;font-weight:bold">for</span> idx, token in <span style="color:#fff;font-weight:bold">enumerate</span>(
</span></span><span style="display:flex;"><span>        vocab.idx_to_token)}
</span></span><span style="display:flex;"><span>    bert = d2l.BERTModel(<span style="color:#fff;font-weight:bold">len</span>(vocab), num_hiddens, norm_shape=[<span style="color:#ff0;font-weight:bold">256</span>],
</span></span><span style="display:flex;"><span>                         ffn_num_input=<span style="color:#ff0;font-weight:bold">256</span>, ffn_num_hiddens=ffn_num_hiddens,
</span></span><span style="display:flex;"><span>                         num_heads=<span style="color:#ff0;font-weight:bold">4</span>, num_layers=<span style="color:#ff0;font-weight:bold">2</span>, dropout=<span style="color:#ff0;font-weight:bold">0.2</span>,
</span></span><span style="display:flex;"><span>                         max_len=max_len, key_size=<span style="color:#ff0;font-weight:bold">256</span>, query_size=<span style="color:#ff0;font-weight:bold">256</span>,
</span></span><span style="display:flex;"><span>                         value_size=<span style="color:#ff0;font-weight:bold">256</span>, hid_in_features=<span style="color:#ff0;font-weight:bold">256</span>,
</span></span><span style="display:flex;"><span>                         mlm_in_features=<span style="color:#ff0;font-weight:bold">256</span>, nsp_in_features=<span style="color:#ff0;font-weight:bold">256</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 加载预训练BERT参数</span>
</span></span><span style="display:flex;"><span>    bert.load_state_dict(torch.load(os.path.join(data_dir,
</span></span><span style="display:flex;"><span>                                                 <span style="color:#0ff;font-weight:bold">&#39;pretrained.params&#39;</span>)))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> bert, vocab
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="63-数据预处理">6.3 数据预处理<a hidden class="anchor" aria-hidden="true" href="#63-数据预处理">#</a></h2>
<ul>
<li>根据6.1 ，合并一对文本序列为一个完整的输入序列；以及相应的片段标记，有效长度。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">54
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> SNLIBERTDataset(torch.utils.data.Dataset):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, dataset, max_len, vocab=<span style="color:#fff;font-weight:bold">None</span>):
</span></span><span style="display:flex;"><span>        all_premise_hypothesis_tokens = [[
</span></span><span style="display:flex;"><span>            p_tokens, h_tokens] <span style="color:#fff;font-weight:bold">for</span> p_tokens, h_tokens in <span style="color:#fff;font-weight:bold">zip</span>(
</span></span><span style="display:flex;"><span>            *[d2l.tokenize([s.lower() <span style="color:#fff;font-weight:bold">for</span> s in sentences])
</span></span><span style="display:flex;"><span>              <span style="color:#fff;font-weight:bold">for</span> sentences in dataset[:<span style="color:#ff0;font-weight:bold">2</span>]])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.labels = torch.tensor(dataset[<span style="color:#ff0;font-weight:bold">2</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.vocab = vocab
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.max_len = max_len
</span></span><span style="display:flex;"><span>        (<span style="color:#fff;font-weight:bold">self</span>.all_token_ids, <span style="color:#fff;font-weight:bold">self</span>.all_segments,
</span></span><span style="display:flex;"><span>         <span style="color:#fff;font-weight:bold">self</span>.valid_lens) = <span style="color:#fff;font-weight:bold">self</span>._preprocess(all_premise_hypothesis_tokens)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;read &#39;</span> + <span style="color:#fff;font-weight:bold">str</span>(<span style="color:#fff;font-weight:bold">len</span>(<span style="color:#fff;font-weight:bold">self</span>.all_token_ids)) + <span style="color:#0ff;font-weight:bold">&#39; examples&#39;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>	<span style="color:#007f7f"># 定义_mp_worker，多线程预处理，</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> _preprocess(<span style="color:#fff;font-weight:bold">self</span>, all_premise_hypothesis_tokens):
</span></span><span style="display:flex;"><span>        pool = multiprocessing.Pool(<span style="color:#ff0;font-weight:bold">4</span>)  <span style="color:#007f7f"># 使用4个进程</span>
</span></span><span style="display:flex;"><span>        out = pool.map(<span style="color:#fff;font-weight:bold">self</span>._mp_worker, all_premise_hypothesis_tokens)
</span></span><span style="display:flex;"><span>        all_token_ids = [
</span></span><span style="display:flex;"><span>            token_ids <span style="color:#fff;font-weight:bold">for</span> token_ids, segments, valid_len in out]
</span></span><span style="display:flex;"><span>        all_segments = [segments <span style="color:#fff;font-weight:bold">for</span> token_ids, segments, valid_len in out]
</span></span><span style="display:flex;"><span>        valid_lens = [valid_len <span style="color:#fff;font-weight:bold">for</span> token_ids, segments, valid_len in out]
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> (torch.tensor(all_token_ids, dtype=torch.long),
</span></span><span style="display:flex;"><span>                torch.tensor(all_segments, dtype=torch.long),
</span></span><span style="display:flex;"><span>                torch.tensor(valid_lens))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 每个线程的处理细节</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> _mp_worker(<span style="color:#fff;font-weight:bold">self</span>, premise_hypothesis_tokens):
</span></span><span style="display:flex;"><span>        p_tokens, h_tokens = premise_hypothesis_tokens
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>._truncate_pair_of_tokens(p_tokens, h_tokens)
</span></span><span style="display:flex;"><span>        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)
</span></span><span style="display:flex;"><span>        token_ids = <span style="color:#fff;font-weight:bold">self</span>.vocab[tokens] + [<span style="color:#fff;font-weight:bold">self</span>.vocab[<span style="color:#0ff;font-weight:bold">&#39;&lt;pad&gt;&#39;</span>]] \
</span></span><span style="display:flex;"><span>                             * (<span style="color:#fff;font-weight:bold">self</span>.max_len - <span style="color:#fff;font-weight:bold">len</span>(tokens))
</span></span><span style="display:flex;"><span>        segments = segments + [<span style="color:#ff0;font-weight:bold">0</span>] * (<span style="color:#fff;font-weight:bold">self</span>.max_len - <span style="color:#fff;font-weight:bold">len</span>(segments))
</span></span><span style="display:flex;"><span>        valid_len = <span style="color:#fff;font-weight:bold">len</span>(tokens)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> token_ids, segments, valid_len
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 保证原始的p_tokens加上h_tokens的词元数小于最大长度</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> _truncate_pair_of_tokens(<span style="color:#fff;font-weight:bold">self</span>, p_tokens, h_tokens):
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 为BERT输入中的&#39;&lt;CLS&gt;&#39;、&#39;&lt;SEP&gt;&#39;和&#39;&lt;SEP&gt;&#39;词元保留位置</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">while</span> <span style="color:#fff;font-weight:bold">len</span>(p_tokens) + <span style="color:#fff;font-weight:bold">len</span>(h_tokens) &gt; <span style="color:#fff;font-weight:bold">self</span>.max_len - <span style="color:#ff0;font-weight:bold">3</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">len</span>(p_tokens) &gt; <span style="color:#fff;font-weight:bold">len</span>(h_tokens):
</span></span><span style="display:flex;"><span>                p_tokens.pop()
</span></span><span style="display:flex;"><span>            <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>                h_tokens.pop()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __getitem__(<span style="color:#fff;font-weight:bold">self</span>, idx):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> (<span style="color:#fff;font-weight:bold">self</span>.all_token_ids[idx], <span style="color:#fff;font-weight:bold">self</span>.all_segments[idx],
</span></span><span style="display:flex;"><span>                <span style="color:#fff;font-weight:bold">self</span>.valid_lens[idx]), <span style="color:#fff;font-weight:bold">self</span>.labels[idx]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __len__(<span style="color:#fff;font-weight:bold">self</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> <span style="color:#fff;font-weight:bold">len</span>(<span style="color:#fff;font-weight:bold">self</span>.all_token_ids)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>小批量加载数据</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512</span>
</span></span><span style="display:flex;"><span>batch_size, max_len, num_workers = <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">128</span>, d2l.get_dataloader_workers()
</span></span><span style="display:flex;"><span>data_dir = d2l.download_extract(<span style="color:#0ff;font-weight:bold">&#39;SNLI&#39;</span>)
</span></span><span style="display:flex;"><span>train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span style="color:#fff;font-weight:bold">True</span>), max_len, vocab)
</span></span><span style="display:flex;"><span>test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span style="color:#fff;font-weight:bold">False</span>), max_len, vocab)
</span></span><span style="display:flex;"><span>train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span style="color:#fff;font-weight:bold">True</span>,
</span></span><span style="display:flex;"><span>                                   num_workers=num_workers)
</span></span><span style="display:flex;"><span>test_iter = torch.utils.data.DataLoader(test_set, batch_size,
</span></span><span style="display:flex;"><span>                                  num_workers=num_workers)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># read 549367 examples</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># read 9824 examples</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="64-微调bert">6.4 微调BERT<a hidden class="anchor" aria-hidden="true" href="#64-微调bert">#</a></h2>
<ul>
<li>在原有的基础上，添加额外的MLP。该MLP由两个全连接层组成（如下的self.hidden与self.output）</li>
<li>基于bert输出中，对于输入序列的<code>&lt;cls&gt;</code>词元的嵌入表示进行微调、预测。</li>
</ul>
<p><strong>（1）定义前向传播，实例化模型</strong></p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> BERTClassifier(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, bert):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(BERTClassifier, <span style="color:#fff;font-weight:bold">self</span>).__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.encoder = bert.encoder
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.hidden = bert.hidden
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.output = nn.Linear(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, inputs):
</span></span><span style="display:flex;"><span>        tokens_X, segments_X, valid_lens_x = inputs
</span></span><span style="display:flex;"><span>        encoded_X = <span style="color:#fff;font-weight:bold">self</span>.encoder(tokens_X, segments_X, valid_lens_x)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> <span style="color:#fff;font-weight:bold">self</span>.output(<span style="color:#fff;font-weight:bold">self</span>.hidden(encoded_X[:, <span style="color:#ff0;font-weight:bold">0</span>, :]))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>net = BERTClassifier(bert)
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>MaskLM</code>类和<code>NextSentencePred</code>类这两个损失函数与微调下游应用无关，因此当BERT微调时，<code>MaskLM</code>和<code>NextSentencePred</code>中采用的多层感知机的参数不会更新（陈旧的，staled）。</p></blockquote>
<p><strong>（2）训练微调模型</strong></p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr, num_epochs = <span style="color:#ff0;font-weight:bold">1e-4</span>, <span style="color:#ff0;font-weight:bold">5</span>
</span></span><span style="display:flex;"><span>trainer = torch.optim.Adam(net.parameters(), lr=lr)
</span></span><span style="display:flex;"><span>loss = nn.CrossEntropyLoss(reduction=<span style="color:#0ff;font-weight:bold">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># loss 0.519, train acc 0.791, test acc 0.783</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 7622.9 examples/sec on [device(type=&#39;cuda&#39;, index=0), device(type=&#39;cuda&#39;, index=1)]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://zh-v2.d2l.ai/_images/output_natural-language-inference-bert_1857e6_102_1.svg" alt="../_images/output_natural-language-inference-bert_1857e6_102_1.svg"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lishensuo.github.io/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li>
      <li><a href="https://lishensuo.github.io/en/tags/d2l/">D2L</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lishensuo.github.io/en/posts/bioinfo/713d2l-%E7%AC%AC%E5%8D%81%E7%AB%A0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">
    <span class="title">« Prev Page</span>
    <br>
    <span>D2L--第十章注意力机制与Transformer</span>
  </a>
  <a class="next" href="https://lishensuo.github.io/en/posts/bioinfo/715d2l-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8F%8A%E5%8D%81%E4%BA%8C%E7%AB%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%A4%9Agpu%E5%B9%B6%E8%A1%8C/">
    <span class="title">Next Page »</span>
    <br>
    <span>D2L--第十一及十二章优化算法&amp;多GPU并行</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://lishensuo.github.io/en/">Li&#39;s Bioinfo-Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
		<br/>您是本站第 <span id="busuanzi_value_site_uv"></span> 位访问者，总浏览量为 <span id="busuanzi_value_site_pv"></span> 次
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script type="text/javascript"
async
src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\[\[','\]\]']],
processEscapes: true,
processEnvironments: true,
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
TeX: { equationNumbers: { autoNumber: "AMS" },
extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});

MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>

<style>
code.has-jax {
font: inherit;
font-size: 100%;
background: inherit;
border: inherit;
color: #515151;
}
</style></body>
</html>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">

<link rel="icon" href="/favicon.ico" type="image/x-icon"> 
<title>D2L--第四章多层感知机 | Li&#39;s Bioinfo-Blog</title>
<meta name="keywords" content="深度学习, D2L">
<meta name="description" content="1. 多层感知机
1.1 隐藏层

之前所学的线性模型意味着单调假设，并不适用于更复杂的建模问题，例如体温与疾病；图片某个像素点的强度与猫或狗的关系等；
多层感知机（MLP）：在输入层与输出层之间加入一个或多个隐藏层，以学习更加复杂的模型情况；

只有隐藏层与输出层涉及到神经元计算与参数更新，因此如下示例MLP的层数是2；
对于其中的隐藏层需要应用非线性的激活函数（σ），以突破对仍为线性本质的限制。




">
<meta name="author" content="Lishensuo">
<link rel="canonical" href="https://lishensuo.github.io/en/posts/bioinfo/707d2l-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.9e4de5e3ba61ea358168341aa7cdf70abfaafb7c697dfe8624af3ddff9a35c2f.css" integrity="sha256-nk3l47ph6jWBaDQap833Cr&#43;q&#43;3xpff6GJK893/mjXC8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js" integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://lishensuo.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://lishensuo.github.io/Q.gif">
<link rel="mask-icon" href="https://lishensuo.github.io/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lishensuo.github.io/en/posts/bioinfo/707d2l-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="D2L--第四章多层感知机" />
<meta property="og:description" content="1. 多层感知机
1.1 隐藏层

之前所学的线性模型意味着单调假设，并不适用于更复杂的建模问题，例如体温与疾病；图片某个像素点的强度与猫或狗的关系等；
多层感知机（MLP）：在输入层与输出层之间加入一个或多个隐藏层，以学习更加复杂的模型情况；

只有隐藏层与输出层涉及到神经元计算与参数更新，因此如下示例MLP的层数是2；
对于其中的隐藏层需要应用非线性的激活函数（σ），以突破对仍为线性本质的限制。




" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lishensuo.github.io/en/posts/bioinfo/707d2l-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-07-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="D2L--第四章多层感知机"/>
<meta name="twitter:description" content="1. 多层感知机
1.1 隐藏层

之前所学的线性模型意味着单调假设，并不适用于更复杂的建模问题，例如体温与疾病；图片某个像素点的强度与猫或狗的关系等；
多层感知机（MLP）：在输入层与输出层之间加入一个或多个隐藏层，以学习更加复杂的模型情况；

只有隐藏层与输出层涉及到神经元计算与参数更新，因此如下示例MLP的层数是2；
对于其中的隐藏层需要应用非线性的激活函数（σ），以突破对仍为线性本质的限制。




"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "分类",
      "item": "https://lishensuo.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📖 生信数据分析--分析流程，工具包等",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "D2L--第四章多层感知机",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/707d2l-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "D2L--第四章多层感知机",
  "name": "D2L--第四章多层感知机",
  "description": "1. 多层感知机 1.1 隐藏层 之前所学的线性模型意味着单调假设，并不适用于更复杂的建模问题，例如体温与疾病；图片某个像素点的强度与猫或狗的关系等； 多层感知机（MLP）：在输入层与输出层之间加入一个或多个隐藏层，以学习更加复杂的模型情况； 只有隐藏层与输出层涉及到神经元计算与参数更新，因此如下示例MLP的层数是2； 对于其中的隐藏层需要应用非线性的激活函数（σ），以突破对仍为线性本质的限制。 ",
  "keywords": [
    "深度学习", "D2L"
  ],
  "articleBody": "1. 多层感知机 1.1 隐藏层 之前所学的线性模型意味着单调假设，并不适用于更复杂的建模问题，例如体温与疾病；图片某个像素点的强度与猫或狗的关系等； 多层感知机（MLP）：在输入层与输出层之间加入一个或多个隐藏层，以学习更加复杂的模型情况； 只有隐藏层与输出层涉及到神经元计算与参数更新，因此如下示例MLP的层数是2； 对于其中的隐藏层需要应用非线性的激活函数（σ），以突破对仍为线性本质的限制。 对于n个样本，d个特征的输入X (n×d)。中间隐藏层的神经元个数为h，权重参数为W1(d×h)，偏置参数为b1(1×h)。输出层神经元个数为q，权重参数为W2(h×q)，偏置参数为b2(1×q)。 1.2 激活函数 1 2 3 %matplotlib inline import torch from d2l import torch as d2l （1）ReLu函数\n变换方式：隐藏层输出的结果若为负数，变为0；正数保持不变； 实现简单，表现良好。 1 2 3 4 5 6 7 8 9 x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y = torch.relu(x) # 绘制激活前（x）与后（y） d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5)) # 计算激活函数的导数 y.backward(torch.ones_like(x), retain_graph=True) d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5)) 不要太纠结x=0的情况—“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”\n（2）sigmoid函数\n变换方式：隐藏层输出的结果变换到0与1之间； 如今在隐藏层中已经不常用，多用于二分类问题中输出层的激活函数 1 2 3 4 5 6 7 8 # 绘制激活前（x）与后（y） y = torch.sigmoid(x) d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5)) # 计算激活函数的导数 x.grad.data.zero_() y.backward(torch.ones_like(x),retain_graph=True) d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5)) （3）tanh函数（双曲正切）\n变换方式：隐藏层输出的结果变换到-1与1之间； 1 2 3 4 5 6 7 8 # 绘制激活前（x）与后（y） y = torch.tanh(x) d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5)) # 计算激活函数的导数 x.grad.data.zero_() y.backward(torch.ones_like(x),retain_graph=True) d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5)) 2. 多层感知机的从零实现 1 2 3 4 5 6 import torch from torch import nn from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) 2.1 初始化模型参数 1 2 3 4 5 6 7 8 9 10 11 num_inputs, num_outputs, num_hiddens = 784, 10, 256 # 隐藏层 W1 = nn.Parameter(torch.randn( num_inputs, num_hiddens, requires_grad=True) * 0.01) b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True)) # 输出层 W2 = nn.Parameter(torch.randn( num_hiddens, num_outputs, requires_grad=True) * 0.01) b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True)) params = [W1, b1, W2, b2] 2.2 激活函数 自定义ReLu函数 1 2 3 def relu(X): a = torch.zeros_like(X) return torch.max(X, a) 2.3 模型 2层MLP结构 1 2 3 4 def net(X): X = X.reshape((-1, num_inputs)) H = relu(X@W1 + b1) # 这里“@”代表矩阵乘法 return (H@W2 + b2) 2.4 损失函数 交叉熵损失函数 1 loss = nn.CrossEntropyLoss(reduction='none') 2.5 训练 调用第三章的训练函数 1 2 3 num_epochs, lr = 10, 0.1 updater = torch.optim.SGD(params, lr=lr) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater) 3. 多层感知机的简洁实现 1 2 3 import torch from torch import nn from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 # 模型结构 net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), #激活函数 nn.Linear(256, 10)) # 参数初始化 def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); 1 2 3 4 5 6 7 8 batch_size, lr, num_epochs = 256, 0.1, 10 loss = nn.CrossEntropyLoss(reduction='none') trainer = torch.optim.SGD(net.parameters(), lr=lr) train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) 4. 模型欠拟合和过拟合 4.1 训练误差和泛化误差 训练误差：模型在训练数据集上计算得到的误差； 泛化误差：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望（理论）； 过拟合：模型在训练数据上拟合的比在潜在分布中更接近的现象。 影响模型泛化的因素： （1）参数数量：可调整参数的数量越多，越容易过拟合； （2）参数取值：参数的取值范围越大，越容易过拟合； （3）训练样本的数量：原则上，训练样本越少，越容易过拟合。【DL至少要上千样本】 4.2 模型选择 比较不同超参数设置下的同一类模型，会涉及到数据集的多种划分方式。 方式1：将数据分为3份 训练数据集用于训练模型 验证数据集用于比较、选择模型 测试数据集用于最终的泛化评估（联想高考） 在代码实现部分，教材所写的测试集其实还是验证集。\n方式2：K折交叉验证（样本量不多时） 将数据分为K个子集 每次选取K-1个子集训练，在剩余的一个自己验证； 取K次实验的均值。 在DL中，由于训练成本较高，不会太常用；在ML中常用。\n4.3 欠拟合还是过拟合 欠拟合 训练误差与验证误差都很大，差距较小； 模型可能过于简单，有理由相信可训练更复杂的模型减小训练误差 过拟合 训练误差明显小于验证误差； 值得注意，过拟合并不总是坏事。最终还是更关心验证误差。 4.4 多项式回归 教材中举了一个例子直观的解释了模型复杂度（参数数量）对于模型拟合的影响。 首先生成了200个样本的20个特征数据；特征分别来自于0~19次幂的结果，而真实的标签y仅来自如下图所示的前四个多项式。 第一次训练：取前4列特征，拟合正常； 第二次训练：取前2列特征，欠拟合（参数过少）； 第三次训练：取全部20列数据，过拟合（参数过多）。 详见教材代码：https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html 5. 权重衰减 5.1 范数与权重衰减 模型复杂度的影响因素之一是参数的大小取值范围，可通过L2范数衡量； 可将参数的范数作为惩罚项，和预测损失共同作为最小化的训练目标； 如下公式，具体可通过正则化常数λ控制惩罚项的影响程度。 较小的λ值对w约束较小； 较大的λ值对w约束较大； 通常，网络输出层的偏置项不会被正则化。\n5.2 高维线性回归 模拟数据，在5.3演示权重衰减的作用 1 2 3 4 5 6 7 8 9 # 训练样本数(20)较少，输入特征较多(200)，容易过拟合 n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5 true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05 train_data = d2l.synthetic_data(true_w, true_b, n_train) train_iter = d2l.load_array(train_data, batch_size) test_data = d2l.synthetic_data(true_w, true_b, n_test) test_iter = d2l.load_array(test_data, batch_size, is_train=False) 5.3 从零开始实现 初始化模型参数 1 2 3 4 def init_params(): w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True) b = torch.zeros(1, requires_grad=True) return [w, b] 定义L2范数 1 2 3 4 5 6 def l2_penalty(w): return torch.sum(w.pow(2)) / 2 ## L1范数 # def l1_penalty(w): # return torch.sum(torch.abs(w)) 定义训练代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def train(lambd): w, b = init_params() net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss #模型输出，损失函数 num_epochs, lr = 100, 0.003 animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test']) for epoch in range(num_epochs): for X, y in train_iter: # 增加了L2范数惩罚项， # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量 l = loss(net(X), y) + lambd * l2_penalty(w) l.sum().backward() d2l.sgd([w, b], lr, batch_size) if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print('w的L2范数是：', torch.norm(w).item()) 训练 1 2 3 4 5 6 7 ## (1) 忽略正则化直接训练：左图 train(lambd=0) # w的L2范数是： 12.963241577148438 ## (2) 使用权重衰减：右图 train(lambd=3) # w的L2范数是： 0.04280993342399597 5.4 简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def train_concise(wd): net = nn.Sequential(nn.Linear(num_inputs, 1)) for param in net.parameters(): param.data.normal_() loss = nn.MSELoss(reduction='none') num_epochs, lr = 100, 0.003 # 偏置参数没有衰减 trainer = torch.optim.SGD([ {\"params\":net[0].weight,'weight_decay': wd}, {\"params\":net[0].bias}], lr=lr) animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test']) for epoch in range(num_epochs): for X, y in train_iter: trainer.zero_grad() l = loss(net(X), y) l.mean().backward() trainer.step() if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print('w的L2范数：', net[0].weight.norm().item()) 1 2 3 train_concise(0) train_concise(3) 沐神推荐，一般可设置weight_decay为0.01, 0.001, 0.0001\n6. 暂退法 6.1 扰动的稳健性 防止模型过拟合的另一个角度是增加平滑性，不会对输入的微小变化敏感； 暂退法是在前向传播过程中，计算每一内部层的同时引入噪声，表现为随机丢弃(drop out)一些神经元; 在标准暂退法正则化中，每个中间隐藏层的激活值h以暂退概率p被随机变量*h’*替换。 上述公式表示按一种无偏向的方式引入噪声，每层的期望值等于没有噪音的值。\n6.2 实践中的暂退法 暂退法仅用于隐藏层的(激活后)输出结果中，使得不过度依赖隐藏层中的任意一个元素； 仅在训练过程中使用Dropout，在预测过程中不需要使用； Dropout多应用于MLP神经网络，而较少应用于CNN等 6.3 从零开始实现 自定义dropout函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import torch from torch import nn from d2l import torch as d2l def dropout_layer(X, dropout): assert 0 \u003c= dropout \u003c= 1 # 在本情况中，所有元素都被丢弃 if dropout == 1: return torch.zeros_like(X) # 在本情况中，所有元素都被保留 if dropout == 0: return X # torch.rand生成随机数 mask = (torch.rand(X.shape) \u003e dropout).float() #逻辑值 return mask * X / (1.0 - dropout) #丢弃逻辑值为FALSE的结果 # 演示 X= torch.arange(16, dtype = torch.float32).reshape((2, 8)) print(X) print(dropout_layer(X, 0.)) print(dropout_layer(X, 0.5)) print(dropout_layer(X, 1.)) 定义模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 参数：输入输出层，两层隐藏层 num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256 # Dropout dropout1, dropout2 = 0.3, 0.5 # 定义了一个神经网络类 Net，该类继承自 PyTorch 的 nn.Module class Net(nn.Module): def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = True): super(Net, self).__init__() #调用父类 nn.Module 的构造函数来初始化当前类 Net 的实例 self.num_inputs = num_inputs self.training = is_training self.lin1 = nn.Linear(num_inputs, num_hiddens1) self.lin2 = nn.Linear(num_hiddens1, num_hiddens2) self.lin3 = nn.Linear(num_hiddens2, num_outputs) self.relu = nn.ReLU() def forward(self, X): H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs)))) # 只有在训练模型时才使用dropout if self.training == True: # 在第一个全连接层之后添加一个dropout层 H1 = dropout_layer(H1, dropout1) H2 = self.relu(self.lin2(H1)) if self.training == True: # 在第二个全连接层之后添加一个dropout层 H2 = dropout_layer(H2, dropout2) out = self.lin3(H2) return out net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2) 训练测试 1 2 3 4 5 6 num_epochs, lr, batch_size = 10, 0.5, 256 loss = nn.CrossEntropyLoss(reduction='none') train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) trainer = torch.optim.SGD(net.parameters(), lr=lr) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) #左图 右图：Dropout均设为0的训练结果。\n6.4 简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), # 在第一个全连接层/激活函数之后添加一个dropout层 nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(), # 在第二个全连接层/激活函数之后添加一个dropout层 nn.Dropout(dropout2), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); trainer = torch.optim.SGD(net.parameters(), lr=lr) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) 7. 前向传播、后向传播和计算图 7.1 前向传播 前向传播：按顺序（从输入到输出）计算和存储神经网络中每层的结果； 如下图举例（简单起见，隐藏层不包括偏置项b） 输入层：x 隐藏层：参数W(1)，输出结果z；激活函数(Φ)输出结果h 输出层：参数W(2)，输出结果o 预测损失：L = l(o, y) 惩罚项：s ~ W(1), W(2) 目标函数（正则化损失）：J = L + s 7.2 反向传播 反向传播：从输出层到输入层计算和存储神经网络的参数梯度； 如上示例 首先计算J分别关于L与s的梯度； 然后可计算J关于o的梯度，s关于W(1)、W(2)的梯度； 再依次计算J关于W(2)的梯度，J关于h的梯度，J关于z的梯度，J关于W(1)的梯度 7.3 训练神经网络 前向传播与反向传播相互依赖； 例如 前向传播计算的正则化项s取决于模型w1与w2的当前值，由优化算法根据最近迭代的反向传播给出的； 反向传播参数的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。 反向传播需要重复利用前向传播中储存的中间值，副作用是需要暂用较多的内存。 8. 数值稳定性和模型初始化 8.1 梯度消失与梯度爆炸 向量对于向量的梯度为矩阵。当计算目标函数对于特定隐藏层参数的导数可能涉及多个矩阵的乘积。 梯度消失：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习 一个常见的例子是Sigmoid函数，参考上述2.2部分介绍 当Sigmoid函数的输入与输出很大或很小时，梯度近乎消失。 梯度爆炸：参数更新过大，破坏了模型的稳定收敛 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import torch M = torch.normal(0, 1, size=(4,4)) print('一个矩阵 \\n',M) for i in range(100): M = torch.mm(M,torch.normal(0, 1, size=(4, 4))) print('乘以100个矩阵后\\n', M) #一个矩阵 # tensor([[-0.7872, 2.7090, 0.5996, -1.3191], # [-1.8260, -0.7130, -0.5521, 0.1051], # [ 1.1213, 1.0472, -0.3991, -0.3802], # [ 0.5552, 0.4517, -0.3218, 0.5214]]) #乘以100个矩阵后 # tensor([[-2.1897e+26, 8.8308e+26, 1.9813e+26, 1.7019e+26], # [ 1.3110e+26, -5.2870e+26, -1.1862e+26, -1.0189e+26], # [-1.6008e+26, 6.4559e+26, 1.4485e+26, 1.2442e+26], # [ 3.0943e+25, -1.2479e+26, -2.7998e+25, -2.4050e+25]]) 8.2 参数初始化 Xavier初始化：对全连接层权重的初始化取决于输入与输出层的神经元数量 （1）若是正态分布，则从均值为0，方差如下的高斯分布中抽样权重；\n（2）也可以改为从下述范围的均匀分布中抽样权重\n",
  "wordCount" : "4159",
  "inLanguage": "en",
  "datePublished": "2024-07-28T00:00:00Z",
  "dateModified": "2024-07-28T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Lishensuo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lishensuo.github.io/en/posts/bioinfo/707d2l-%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Li's Bioinfo-Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lishensuo.github.io/img/Q.gif"
    }
  }
}
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lishensuo.github.io/en/" accesskey="h" title="Li&#39;s Bioinfo-Blog (Alt + H)">Li&#39;s Bioinfo-Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lishensuo.github.io/en/" title="主页">
                    <span>主页</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/posts" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/tags" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/about" title="关于">
                    <span>关于</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/search" title="搜索 (Alt &#43; /)" accesskey=/>
                    <span>搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lishensuo.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/">分类</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/bioinfo/">📖 生信数据分析--分析流程，工具包等</a></div>
    <h1 class="post-title">
      D2L--第四章多层感知机
    </h1>
    <div class="post-meta">













Create:&amp;nbsp;&lt;span title=&#39;2024-07-28 00:00:00 &#43;0000 UTC&#39;&gt;2024-07-28&lt;/span&gt;&amp;nbsp;|&amp;nbsp;Update:&amp;nbsp;2024-07-28&amp;nbsp;|&amp;nbsp;Words:&amp;nbsp;4159&amp;nbsp;|&amp;nbsp;9 min&amp;nbsp;|&amp;nbsp;Lishensuo

|  Viewers: <span id="busuanzi_value_page_pv"></span> 
	  
    </div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba" aria-label="1. 多层感知机">1. 多层感知机</a><ul>
                            
                    <li>
                        <a href="#11-%e9%9a%90%e8%97%8f%e5%b1%82" aria-label="1.1 隐藏层">1.1 隐藏层</a></li>
                    <li>
                        <a href="#12-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0" aria-label="1.2 激活函数">1.2 激活函数</a></li></ul>
                    </li>
                    <li>
                        <a href="#2-%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e4%bb%8e%e9%9b%b6%e5%ae%9e%e7%8e%b0" aria-label="2. 多层感知机的从零实现">2. 多层感知机的从零实现</a><ul>
                            
                    <li>
                        <a href="#21-%e5%88%9d%e5%a7%8b%e5%8c%96%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0" aria-label="2.1 初始化模型参数">2.1 初始化模型参数</a></li>
                    <li>
                        <a href="#22-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0" aria-label="2.2 激活函数">2.2 激活函数</a></li>
                    <li>
                        <a href="#23-%e6%a8%a1%e5%9e%8b" aria-label="2.3 模型">2.3 模型</a></li>
                    <li>
                        <a href="#24-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0" aria-label="2.4 损失函数">2.4 损失函数</a></li>
                    <li>
                        <a href="#25-%e8%ae%ad%e7%bb%83" aria-label="2.5 训练">2.5 训练</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e7%ae%80%e6%b4%81%e5%ae%9e%e7%8e%b0" aria-label="3. 多层感知机的简洁实现">3. 多层感知机的简洁实现</a></li>
                    <li>
                        <a href="#4-%e6%a8%a1%e5%9e%8b%e6%ac%a0%e6%8b%9f%e5%90%88%e5%92%8c%e8%bf%87%e6%8b%9f%e5%90%88" aria-label="4. 模型欠拟合和过拟合">4. 模型欠拟合和过拟合</a><ul>
                            
                    <li>
                        <a href="#41-%e8%ae%ad%e7%bb%83%e8%af%af%e5%b7%ae%e5%92%8c%e6%b3%9b%e5%8c%96%e8%af%af%e5%b7%ae" aria-label="4.1 训练误差和泛化误差">4.1 训练误差和泛化误差</a></li>
                    <li>
                        <a href="#42-%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9" aria-label="4.2 模型选择">4.2 模型选择</a></li>
                    <li>
                        <a href="#43-%e6%ac%a0%e6%8b%9f%e5%90%88%e8%bf%98%e6%98%af%e8%bf%87%e6%8b%9f%e5%90%88" aria-label="4.3 欠拟合还是过拟合">4.3 欠拟合还是过拟合</a></li>
                    <li>
                        <a href="#44-%e5%a4%9a%e9%a1%b9%e5%bc%8f%e5%9b%9e%e5%bd%92" aria-label="4.4 多项式回归">4.4 多项式回归</a></li></ul>
                    </li>
                    <li>
                        <a href="#5-%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f" aria-label="5. 权重衰减">5. 权重衰减</a><ul>
                            
                    <li>
                        <a href="#51-%e8%8c%83%e6%95%b0%e4%b8%8e%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f" aria-label="5.1 范数与权重衰减">5.1 范数与权重衰减</a></li>
                    <li>
                        <a href="#52-%e9%ab%98%e7%bb%b4%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92" aria-label="5.2 高维线性回归">5.2 高维线性回归</a></li>
                    <li>
                        <a href="#53-%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e5%ae%9e%e7%8e%b0" aria-label="5.3 从零开始实现">5.3 从零开始实现</a></li>
                    <li>
                        <a href="#54-%e7%ae%80%e6%b4%81%e5%ae%9e%e7%8e%b0" aria-label="5.4 简洁实现">5.4 简洁实现</a></li></ul>
                    </li>
                    <li>
                        <a href="#6-%e6%9a%82%e9%80%80%e6%b3%95" aria-label="6. 暂退法">6. 暂退法</a><ul>
                            
                    <li>
                        <a href="#61-%e6%89%b0%e5%8a%a8%e7%9a%84%e7%a8%b3%e5%81%a5%e6%80%a7" aria-label="6.1 扰动的稳健性">6.1 扰动的稳健性</a></li>
                    <li>
                        <a href="#62-%e5%ae%9e%e8%b7%b5%e4%b8%ad%e7%9a%84%e6%9a%82%e9%80%80%e6%b3%95" aria-label="6.2 实践中的暂退法">6.2 实践中的暂退法</a></li>
                    <li>
                        <a href="#63-%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e5%ae%9e%e7%8e%b0" aria-label="6.3 从零开始实现">6.3 从零开始实现</a></li>
                    <li>
                        <a href="#64-%e7%ae%80%e6%b4%81%e5%ae%9e%e7%8e%b0" aria-label="6.4 简洁实现">6.4 简洁实现</a></li></ul>
                    </li>
                    <li>
                        <a href="#7-%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%90%8e%e5%90%91%e4%bc%a0%e6%92%ad%e5%92%8c%e8%ae%a1%e7%ae%97%e5%9b%be" aria-label="7. 前向传播、后向传播和计算图">7. 前向传播、后向传播和计算图</a><ul>
                            
                    <li>
                        <a href="#71-%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad" aria-label="7.1 前向传播">7.1 前向传播</a></li>
                    <li>
                        <a href="#72-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad" aria-label="7.2 反向传播">7.2 反向传播</a></li>
                    <li>
                        <a href="#73-%e8%ae%ad%e7%bb%83%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" aria-label="7.3 训练神经网络">7.3 训练神经网络</a></li></ul>
                    </li>
                    <li>
                        <a href="#8-%e6%95%b0%e5%80%bc%e7%a8%b3%e5%ae%9a%e6%80%a7%e5%92%8c%e6%a8%a1%e5%9e%8b%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="8. 数值稳定性和模型初始化">8. 数值稳定性和模型初始化</a><ul>
                            
                    <li>
                        <a href="#81-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e4%b8%8e%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8" aria-label="8.1 梯度消失与梯度爆炸">8.1 梯度消失与梯度爆炸</a></li>
                    <li>
                        <a href="#82-%e5%8f%82%e6%95%b0%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="8.2 参数初始化">8.2 参数初始化</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>


  <div class="post-content"><h1 id="1-多层感知机">1. 多层感知机<a hidden class="anchor" aria-hidden="true" href="#1-多层感知机">#</a></h1>
<h2 id="11-隐藏层">1.1 隐藏层<a hidden class="anchor" aria-hidden="true" href="#11-隐藏层">#</a></h2>
<ul>
<li>之前所学的线性模型意味着单调假设，并不适用于更复杂的建模问题，例如体温与疾病；图片某个像素点的强度与猫或狗的关系等；</li>
<li>多层感知机（MLP）：在输入层与输出层之间加入一个或多个隐藏层，以学习更加复杂的模型情况；
<ul>
<li>只有隐藏层与输出层涉及到神经元计算与参数更新，因此如下示例MLP的层数是2；</li>
<li>对于其中的隐藏层需要应用非线性的激活函数（σ），以突破对仍为线性本质的限制。</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723082016977.png" alt="image-20240723082016977"  />
</p>
<ul>
<li>对于n个样本，d个特征的输入<strong>X</strong> (n×d)。中间隐藏层的神经元个数为h，权重参数为<strong>W1</strong>(d×h)，偏置参数为<strong>b1</strong>(1×h)。输出层神经元个数为q，权重参数为<strong>W2</strong>(h×q)，偏置参数为<strong>b2</strong>(1×q)。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723083413900.png" alt="image-20240723083413900"  />
</p>
<h2 id="12-激活函数">1.2 激活函数<a hidden class="anchor" aria-hidden="true" href="#12-激活函数">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>%matplotlib inline
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>（1）ReLu函数</strong></p>
<ul>
<li>变换方式：隐藏层输出的结果若为负数，变为0；正数保持不变；</li>
<li>实现简单，表现良好。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723131359090.png" alt="image-20240723131359090"  />
</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>x = torch.arange(-<span style="color:#ff0;font-weight:bold">8.0</span>, <span style="color:#ff0;font-weight:bold">8.0</span>, <span style="color:#ff0;font-weight:bold">0.1</span>, requires_grad=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>y = torch.relu(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 绘制激活前（x）与后（y）</span>
</span></span><span style="display:flex;"><span>d2l.plot(x.detach(), y.detach(), <span style="color:#0ff;font-weight:bold">&#39;x&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;relu(x)&#39;</span>, figsize=(<span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2.5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 计算激活函数的导数</span>
</span></span><span style="display:flex;"><span>y.backward(torch.ones_like(x), retain_graph=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>d2l.plot(x.detach(), x.grad, <span style="color:#0ff;font-weight:bold">&#39;x&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;grad of relu&#39;</span>, figsize=(<span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2.5</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723132432201.png" alt="image-20240723132432201"  />
</p>
<blockquote>
<p>不要太纠结x=0的情况—“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”</p></blockquote>
<p><strong>（2）sigmoid函数</strong></p>
<ul>
<li>变换方式：隐藏层输出的结果变换到0与1之间；</li>
<li>如今在隐藏层中已经不常用，多用于二分类问题中输出层的激活函数</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723132744827.png" alt="image-20240723132744827"  />
</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 绘制激活前（x）与后（y）</span>
</span></span><span style="display:flex;"><span>y = torch.sigmoid(x)
</span></span><span style="display:flex;"><span>d2l.plot(x.detach(), y.detach(), <span style="color:#0ff;font-weight:bold">&#39;x&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;sigmoid(x)&#39;</span>, figsize=(<span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2.5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 计算激活函数的导数</span>
</span></span><span style="display:flex;"><span>x.grad.data.zero_()
</span></span><span style="display:flex;"><span>y.backward(torch.ones_like(x),retain_graph=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>d2l.plot(x.detach(), x.grad, <span style="color:#0ff;font-weight:bold">&#39;x&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;grad of sigmoid&#39;</span>, figsize=(<span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2.5</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723133604798.png" alt="image-20240723133604798"  />
</p>
<p><strong>（3）tanh函数（双曲正切）</strong></p>
<ul>
<li>变换方式：隐藏层输出的结果变换到-1与1之间；</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240728225804350.png" alt="image-20240728225804350"  />
</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 绘制激活前（x）与后（y）</span>
</span></span><span style="display:flex;"><span>y = torch.tanh(x)
</span></span><span style="display:flex;"><span>d2l.plot(x.detach(), y.detach(), <span style="color:#0ff;font-weight:bold">&#39;x&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;tanh(x)&#39;</span>, figsize=(<span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2.5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 计算激活函数的导数</span>
</span></span><span style="display:flex;"><span>x.grad.data.zero_()
</span></span><span style="display:flex;"><span>y.backward(torch.ones_like(x),retain_graph=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>d2l.plot(x.detach(), x.grad, <span style="color:#0ff;font-weight:bold">&#39;x&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;grad of tanh&#39;</span>, figsize=(<span style="color:#ff0;font-weight:bold">5</span>, <span style="color:#ff0;font-weight:bold">2.5</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723134023390.png" alt="image-20240723134023390"  />
</p>
<h1 id="2-多层感知机的从零实现">2. 多层感知机的从零实现<a hidden class="anchor" aria-hidden="true" href="#2-多层感知机的从零实现">#</a></h1>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>batch_size = <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="21-初始化模型参数">2.1 初始化模型参数<a hidden class="anchor" aria-hidden="true" href="#21-初始化模型参数">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_inputs, num_outputs, num_hiddens = <span style="color:#ff0;font-weight:bold">784</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 隐藏层</span>
</span></span><span style="display:flex;"><span>W1 = nn.Parameter(torch.randn(
</span></span><span style="display:flex;"><span>    num_inputs, num_hiddens, requires_grad=<span style="color:#fff;font-weight:bold">True</span>) * <span style="color:#ff0;font-weight:bold">0.01</span>)
</span></span><span style="display:flex;"><span>b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span style="color:#fff;font-weight:bold">True</span>))
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 输出层</span>
</span></span><span style="display:flex;"><span>W2 = nn.Parameter(torch.randn(
</span></span><span style="display:flex;"><span>    num_hiddens, num_outputs, requires_grad=<span style="color:#fff;font-weight:bold">True</span>) * <span style="color:#ff0;font-weight:bold">0.01</span>)
</span></span><span style="display:flex;"><span>b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span style="color:#fff;font-weight:bold">True</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>params = [W1, b1, W2, b2]
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="22-激活函数">2.2 激活函数<a hidden class="anchor" aria-hidden="true" href="#22-激活函数">#</a></h2>
<ul>
<li>自定义ReLu函数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> relu(X):
</span></span><span style="display:flex;"><span>    a = torch.zeros_like(X)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> torch.max(X, a)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="23-模型">2.3 模型<a hidden class="anchor" aria-hidden="true" href="#23-模型">#</a></h2>
<ul>
<li>2层MLP结构</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> net(X):
</span></span><span style="display:flex;"><span>    X = X.reshape((-<span style="color:#ff0;font-weight:bold">1</span>, num_inputs))
</span></span><span style="display:flex;"><span>    H = relu(X@W1 + b1)  <span style="color:#007f7f"># 这里“@”代表矩阵乘法</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> (H@W2 + b2)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="24-损失函数">2.4 损失函数<a hidden class="anchor" aria-hidden="true" href="#24-损失函数">#</a></h2>
<ul>
<li>交叉熵损失函数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loss = nn.CrossEntropyLoss(reduction=<span style="color:#0ff;font-weight:bold">&#39;none&#39;</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="25-训练">2.5 训练<a hidden class="anchor" aria-hidden="true" href="#25-训练">#</a></h2>
<ul>
<li>调用第三章的训练函数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_epochs, lr = <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">0.1</span>
</span></span><span style="display:flex;"><span>updater = torch.optim.SGD(params, lr=lr)
</span></span><span style="display:flex;"><span>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="3-多层感知机的简洁实现">3. 多层感知机的简洁实现<a hidden class="anchor" aria-hidden="true" href="#3-多层感知机的简洁实现">#</a></h1>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 模型结构</span>
</span></span><span style="display:flex;"><span>net = nn.Sequential(nn.Flatten(),
</span></span><span style="display:flex;"><span>                    nn.Linear(<span style="color:#ff0;font-weight:bold">784</span>, <span style="color:#ff0;font-weight:bold">256</span>),
</span></span><span style="display:flex;"><span>                    nn.ReLU(), <span style="color:#007f7f">#激活函数</span>
</span></span><span style="display:flex;"><span>                    nn.Linear(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 参数初始化</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> init_weights(m):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">type</span>(m) == nn.Linear:
</span></span><span style="display:flex;"><span>        nn.init.normal_(m.weight, std=<span style="color:#ff0;font-weight:bold">0.01</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net.apply(init_weights);
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch_size, lr, num_epochs = <span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">0.1</span>, <span style="color:#ff0;font-weight:bold">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss = nn.CrossEntropyLoss(reduction=<span style="color:#0ff;font-weight:bold">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>trainer = torch.optim.SGD(net.parameters(), lr=lr)
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="4-模型欠拟合和过拟合">4. 模型欠拟合和过拟合<a hidden class="anchor" aria-hidden="true" href="#4-模型欠拟合和过拟合">#</a></h1>
<h2 id="41-训练误差和泛化误差">4.1 训练误差和泛化误差<a hidden class="anchor" aria-hidden="true" href="#41-训练误差和泛化误差">#</a></h2>
<ul>
<li>训练误差：模型在训练数据集上计算得到的误差；</li>
<li>泛化误差：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望（理论）；</li>
<li>过拟合：模型在训练数据上拟合的比在潜在分布中更接近的现象。</li>
<li>影响模型泛化的因素：
<ul>
<li>（1）参数数量：可调整参数的数量越多，越容易过拟合；</li>
<li>（2）参数取值：参数的取值范围越大，越容易过拟合；</li>
<li>（3）训练样本的数量：原则上，训练样本越少，越容易过拟合。【DL至少要上千样本】</li>
</ul>
</li>
</ul>
<h2 id="42-模型选择">4.2 模型选择<a hidden class="anchor" aria-hidden="true" href="#42-模型选择">#</a></h2>
<ul>
<li>比较不同超参数设置下的同一类模型，会涉及到数据集的多种划分方式。</li>
<li>方式1：将数据分为3份
<ul>
<li>训练数据集用于训练模型</li>
<li>验证数据集用于比较、选择模型</li>
<li>测试数据集用于最终的泛化评估（联想高考）</li>
</ul>
</li>
</ul>
<blockquote>
<p>在代码实现部分，教材所写的测试集其实还是验证集。</p></blockquote>
<ul>
<li>方式2：K折交叉验证（样本量不多时）
<ul>
<li>将数据分为K个子集</li>
<li>每次选取K-1个子集训练，在剩余的一个自己验证；</li>
<li>取K次实验的均值。</li>
</ul>
</li>
</ul>
<blockquote>
<p>在DL中，由于训练成本较高，不会太常用；在ML中常用。</p></blockquote>
<h2 id="43-欠拟合还是过拟合">4.3 欠拟合还是过拟合<a hidden class="anchor" aria-hidden="true" href="#43-欠拟合还是过拟合">#</a></h2>
<ul>
<li>欠拟合
<ul>
<li>训练误差与验证误差都很大，差距较小；</li>
<li>模型可能过于简单，有理由相信可训练更复杂的模型减小训练误差</li>
</ul>
</li>
<li>过拟合
<ul>
<li>训练误差明显小于验证误差；</li>
<li>值得注意，过拟合并不总是坏事。最终还是更关心验证误差。</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723210626475.png" alt="image-20240723210626475"  />
</p>
<h2 id="44-多项式回归">4.4 多项式回归<a hidden class="anchor" aria-hidden="true" href="#44-多项式回归">#</a></h2>
<ul>
<li>教材中举了一个例子直观的解释了模型复杂度（参数数量）对于模型拟合的影响。</li>
<li>首先生成了200个样本的20个特征数据；特征分别来自于0~19次幂的结果，而真实的标签y仅来自如下图所示的前四个多项式。
<ul>
<li>第一次训练：取前4列特征，拟合正常；</li>
<li>第二次训练：取前2列特征，欠拟合（参数过少）；</li>
<li>第三次训练：取全部20列数据，过拟合（参数过多）。</li>
</ul>
</li>
<li>详见教材代码：https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240723220009110.png" alt="image-20240723220009110"  />
</p>
<h1 id="5-权重衰减">5. 权重衰减<a hidden class="anchor" aria-hidden="true" href="#5-权重衰减">#</a></h1>
<h2 id="51-范数与权重衰减">5.1 范数与权重衰减<a hidden class="anchor" aria-hidden="true" href="#51-范数与权重衰减">#</a></h2>
<ul>
<li>模型复杂度的影响因素之一是参数的大小取值范围，可通过<strong>L2范数</strong>衡量；</li>
<li>可将参数的范数作为<strong>惩罚项</strong>，和预测损失共同作为最小化的训练目标；</li>
<li>如下公式，具体可通过正则化常数<strong>λ</strong>控制惩罚项的影响程度。
<ul>
<li>较小的<strong>λ</strong>值对<strong>w</strong>约束较小；</li>
<li>较大的<strong>λ</strong>值对<strong>w</strong>约束较大；</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240724203448047.png" alt="image-20240724203448047"  />
</p>
<blockquote>
<p>通常，网络输出层的偏置项不会被正则化。</p></blockquote>
<h2 id="52-高维线性回归">5.2 高维线性回归<a hidden class="anchor" aria-hidden="true" href="#52-高维线性回归">#</a></h2>
<ul>
<li>模拟数据，在5.3演示权重衰减的作用</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240724205316374.png" alt="image-20240724205316374"  />
</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 训练样本数(20)较少，输入特征较多(200)，容易过拟合</span>
</span></span><span style="display:flex;"><span>n_train, n_test, num_inputs, batch_size = <span style="color:#ff0;font-weight:bold">20</span>, <span style="color:#ff0;font-weight:bold">100</span>, <span style="color:#ff0;font-weight:bold">200</span>, <span style="color:#ff0;font-weight:bold">5</span>
</span></span><span style="display:flex;"><span>true_w, true_b = torch.ones((num_inputs, <span style="color:#ff0;font-weight:bold">1</span>)) * <span style="color:#ff0;font-weight:bold">0.01</span>, <span style="color:#ff0;font-weight:bold">0.05</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_data = d2l.synthetic_data(true_w, true_b, n_train)
</span></span><span style="display:flex;"><span>train_iter = d2l.load_array(train_data, batch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>test_data = d2l.synthetic_data(true_w, true_b, n_test)
</span></span><span style="display:flex;"><span>test_iter = d2l.load_array(test_data, batch_size, is_train=<span style="color:#fff;font-weight:bold">False</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="53-从零开始实现">5.3 从零开始实现<a hidden class="anchor" aria-hidden="true" href="#53-从零开始实现">#</a></h2>
<ul>
<li>初始化模型参数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> init_params():
</span></span><span style="display:flex;"><span>    w = torch.normal(<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, size=(num_inputs, <span style="color:#ff0;font-weight:bold">1</span>), requires_grad=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    b = torch.zeros(<span style="color:#ff0;font-weight:bold">1</span>, requires_grad=<span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> [w, b]
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>定义L2范数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> l2_penalty(w):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> torch.sum(w.pow(<span style="color:#ff0;font-weight:bold">2</span>)) / <span style="color:#ff0;font-weight:bold">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">## L1范数</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># def l1_penalty(w):</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#     return torch.sum(torch.abs(w))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>定义训练代码实现</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> train(lambd):
</span></span><span style="display:flex;"><span>    w, b = init_params()
</span></span><span style="display:flex;"><span>    net, loss = <span style="color:#fff;font-weight:bold">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss  <span style="color:#007f7f">#模型输出，损失函数</span>
</span></span><span style="display:flex;"><span>    num_epochs, lr = <span style="color:#ff0;font-weight:bold">100</span>, <span style="color:#ff0;font-weight:bold">0.003</span>
</span></span><span style="display:flex;"><span>    animator = d2l.Animator(xlabel=<span style="color:#0ff;font-weight:bold">&#39;epochs&#39;</span>, ylabel=<span style="color:#0ff;font-weight:bold">&#39;loss&#39;</span>, yscale=<span style="color:#0ff;font-weight:bold">&#39;log&#39;</span>,
</span></span><span style="display:flex;"><span>                            xlim=[<span style="color:#ff0;font-weight:bold">5</span>, num_epochs], legend=[<span style="color:#0ff;font-weight:bold">&#39;train&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;test&#39;</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> epoch in <span style="color:#fff;font-weight:bold">range</span>(num_epochs):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> X, y in train_iter:
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 增加了L2范数惩罚项，</span>
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span>
</span></span><span style="display:flex;"><span>            l = loss(net(X), y) + lambd * l2_penalty(w)
</span></span><span style="display:flex;"><span>            l.sum().backward()
</span></span><span style="display:flex;"><span>            d2l.sgd([w, b], lr, batch_size)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> (epoch + <span style="color:#ff0;font-weight:bold">1</span>) % <span style="color:#ff0;font-weight:bold">5</span> == <span style="color:#ff0;font-weight:bold">0</span>:
</span></span><span style="display:flex;"><span>            animator.add(epoch + <span style="color:#ff0;font-weight:bold">1</span>, (d2l.evaluate_loss(net, train_iter, loss),
</span></span><span style="display:flex;"><span>                                     d2l.evaluate_loss(net, test_iter, loss)))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;w的L2范数是：&#39;</span>, torch.norm(w).item())
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>训练</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f">## (1) 忽略正则化直接训练：左图</span>
</span></span><span style="display:flex;"><span>train(lambd=<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># w的L2范数是： 12.963241577148438</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">## (2) 使用权重衰减：右图</span>
</span></span><span style="display:flex;"><span>train(lambd=<span style="color:#ff0;font-weight:bold">3</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># w的L2范数是： 0.04280993342399597</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240724210337762.png" alt="image-20240724210337762"  />
</p>
<h2 id="54-简洁实现">5.4 简洁实现<a hidden class="anchor" aria-hidden="true" href="#54-简洁实现">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> train_concise(wd):
</span></span><span style="display:flex;"><span>    net = nn.Sequential(nn.Linear(num_inputs, <span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> param in net.parameters():
</span></span><span style="display:flex;"><span>        param.data.normal_()
</span></span><span style="display:flex;"><span>    loss = nn.MSELoss(reduction=<span style="color:#0ff;font-weight:bold">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>    num_epochs, lr = <span style="color:#ff0;font-weight:bold">100</span>, <span style="color:#ff0;font-weight:bold">0.003</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 偏置参数没有衰减</span>
</span></span><span style="display:flex;"><span>    trainer = torch.optim.SGD([
</span></span><span style="display:flex;"><span>        {<span style="color:#0ff;font-weight:bold">&#34;params&#34;</span>:net[<span style="color:#ff0;font-weight:bold">0</span>].weight,<span style="color:#0ff;font-weight:bold">&#39;weight_decay&#39;</span>: wd},
</span></span><span style="display:flex;"><span>        {<span style="color:#0ff;font-weight:bold">&#34;params&#34;</span>:net[<span style="color:#ff0;font-weight:bold">0</span>].bias}], lr=lr)
</span></span><span style="display:flex;"><span>    animator = d2l.Animator(xlabel=<span style="color:#0ff;font-weight:bold">&#39;epochs&#39;</span>, ylabel=<span style="color:#0ff;font-weight:bold">&#39;loss&#39;</span>, yscale=<span style="color:#0ff;font-weight:bold">&#39;log&#39;</span>,
</span></span><span style="display:flex;"><span>                            xlim=[<span style="color:#ff0;font-weight:bold">5</span>, num_epochs], legend=[<span style="color:#0ff;font-weight:bold">&#39;train&#39;</span>, <span style="color:#0ff;font-weight:bold">&#39;test&#39;</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> epoch in <span style="color:#fff;font-weight:bold">range</span>(num_epochs):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">for</span> X, y in train_iter:
</span></span><span style="display:flex;"><span>            trainer.zero_grad()
</span></span><span style="display:flex;"><span>            l = loss(net(X), y)
</span></span><span style="display:flex;"><span>            l.mean().backward()
</span></span><span style="display:flex;"><span>            trainer.step()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> (epoch + <span style="color:#ff0;font-weight:bold">1</span>) % <span style="color:#ff0;font-weight:bold">5</span> == <span style="color:#ff0;font-weight:bold">0</span>:
</span></span><span style="display:flex;"><span>            animator.add(epoch + <span style="color:#ff0;font-weight:bold">1</span>,
</span></span><span style="display:flex;"><span>                         (d2l.evaluate_loss(net, train_iter, loss),
</span></span><span style="display:flex;"><span>                          d2l.evaluate_loss(net, test_iter, loss)))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;w的L2范数：&#39;</span>, net[<span style="color:#ff0;font-weight:bold">0</span>].weight.norm().item())
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train_concise(<span style="color:#ff0;font-weight:bold">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_concise(<span style="color:#ff0;font-weight:bold">3</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>沐神推荐，一般可设置weight_decay为0.01, 0.001, 0.0001</p></blockquote>
<h1 id="6-暂退法">6. 暂退法<a hidden class="anchor" aria-hidden="true" href="#6-暂退法">#</a></h1>
<h2 id="61-扰动的稳健性">6.1 扰动的稳健性<a hidden class="anchor" aria-hidden="true" href="#61-扰动的稳健性">#</a></h2>
<ul>
<li>防止模型过拟合的另一个角度是增加平滑性，不会对输入的微小变化敏感；</li>
<li>暂退法是在前向传播过程中，计算每一内部层的同时引入噪声，表现为随机丢弃(drop out)一些神经元;</li>
<li>在标准暂退法正则化中，每个中间隐藏层的激活值<em>h</em>以暂退概率<em>p</em>被随机变量*h&rsquo;*替换。</li>
</ul>
<h4 id="image-20240724215837412"><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240724215837412.png" alt="image-20240724215837412"  />
</h4>
<blockquote>
<p>上述公式表示按一种无偏向的方式引入噪声，每层的期望值等于没有噪音的值。</p></blockquote>
<h2 id="62-实践中的暂退法">6.2 实践中的暂退法<a hidden class="anchor" aria-hidden="true" href="#62-实践中的暂退法">#</a></h2>
<ul>
<li>暂退法仅用于隐藏层的(激活后)输出结果中，使得不过度依赖隐藏层中的任意一个元素；</li>
<li>仅在训练过程中使用Dropout，在预测过程中不需要使用；</li>
<li>Dropout多应用于MLP神经网络，而较少应用于CNN等</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240724220754441.png" alt="image-20240724220754441"  />
</p>
<h2 id="63-从零开始实现">6.3 从零开始实现<a hidden class="anchor" aria-hidden="true" href="#63-从零开始实现">#</a></h2>
<ul>
<li>自定义dropout函数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> torch <span style="color:#fff;font-weight:bold">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> d2l <span style="color:#fff;font-weight:bold">import</span> torch <span style="color:#fff;font-weight:bold">as</span> d2l
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> dropout_layer(X, dropout):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> <span style="color:#ff0;font-weight:bold">0</span> &lt;= dropout &lt;= <span style="color:#ff0;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 在本情况中，所有元素都被丢弃</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> dropout == <span style="color:#ff0;font-weight:bold">1</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> torch.zeros_like(X)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># 在本情况中，所有元素都被保留</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> dropout == <span style="color:#ff0;font-weight:bold">0</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> X
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># torch.rand生成随机数</span>
</span></span><span style="display:flex;"><span>    mask = (torch.rand(X.shape) &gt; dropout).float() <span style="color:#007f7f">#逻辑值</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> mask * X / (<span style="color:#ff0;font-weight:bold">1.0</span> - dropout)  <span style="color:#007f7f">#丢弃逻辑值为FALSE的结果</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 演示</span>
</span></span><span style="display:flex;"><span>X= torch.arange(<span style="color:#ff0;font-weight:bold">16</span>, dtype = torch.float32).reshape((<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">8</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">print</span>(X)
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">print</span>(dropout_layer(X, <span style="color:#ff0;font-weight:bold">0.</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">print</span>(dropout_layer(X, <span style="color:#ff0;font-weight:bold">0.5</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">print</span>(dropout_layer(X, <span style="color:#ff0;font-weight:bold">1.</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>定义模型</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 参数：输入输出层，两层隐藏层</span>
</span></span><span style="display:flex;"><span>num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span style="color:#ff0;font-weight:bold">784</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Dropout</span>
</span></span><span style="display:flex;"><span>dropout1, dropout2 = <span style="color:#ff0;font-weight:bold">0.3</span>, <span style="color:#ff0;font-weight:bold">0.5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 定义了一个神经网络类 Net，该类继承自 PyTorch 的 nn.Module</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">class</span> Net(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> __init__(<span style="color:#fff;font-weight:bold">self</span>, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
</span></span><span style="display:flex;"><span>                 is_training = <span style="color:#fff;font-weight:bold">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">super</span>(Net, <span style="color:#fff;font-weight:bold">self</span>).__init__() <span style="color:#007f7f">#调用父类 nn.Module 的构造函数来初始化当前类 Net 的实例</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.num_inputs = num_inputs
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.training = is_training
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.lin1 = nn.Linear(num_inputs, num_hiddens1)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.lin3 = nn.Linear(num_hiddens2, num_outputs)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">self</span>.relu = nn.ReLU()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">def</span> forward(<span style="color:#fff;font-weight:bold">self</span>, X):
</span></span><span style="display:flex;"><span>        H1 = <span style="color:#fff;font-weight:bold">self</span>.relu(<span style="color:#fff;font-weight:bold">self</span>.lin1(X.reshape((-<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#fff;font-weight:bold">self</span>.num_inputs))))
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 只有在训练模型时才使用dropout</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">self</span>.training == <span style="color:#fff;font-weight:bold">True</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 在第一个全连接层之后添加一个dropout层</span>
</span></span><span style="display:flex;"><span>            H1 = dropout_layer(H1, dropout1)
</span></span><span style="display:flex;"><span>        H2 = <span style="color:#fff;font-weight:bold">self</span>.relu(<span style="color:#fff;font-weight:bold">self</span>.lin2(H1))
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">self</span>.training == <span style="color:#fff;font-weight:bold">True</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#007f7f"># 在第二个全连接层之后添加一个dropout层</span>
</span></span><span style="display:flex;"><span>            H2 = dropout_layer(H2, dropout2)
</span></span><span style="display:flex;"><span>        out = <span style="color:#fff;font-weight:bold">self</span>.lin3(H2)
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>训练测试</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_epochs, lr, batch_size = <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">0.5</span>, <span style="color:#ff0;font-weight:bold">256</span>
</span></span><span style="display:flex;"><span>loss = nn.CrossEntropyLoss(reduction=<span style="color:#0ff;font-weight:bold">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
</span></span><span style="display:flex;"><span>trainer = torch.optim.SGD(net.parameters(), lr=lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) <span style="color:#007f7f">#左图</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240724223822753.png" alt="image-20240724223822753"  />
</p>
<blockquote>
<p>右图：Dropout均设为0的训练结果。</p></blockquote>
<h2 id="64-简洁实现">6.4 简洁实现<a hidden class="anchor" aria-hidden="true" href="#64-简洁实现">#</a></h2>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net = nn.Sequential(nn.Flatten(),
</span></span><span style="display:flex;"><span>        nn.Linear(<span style="color:#ff0;font-weight:bold">784</span>, <span style="color:#ff0;font-weight:bold">256</span>),
</span></span><span style="display:flex;"><span>        nn.ReLU(),
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 在第一个全连接层/激活函数之后添加一个dropout层</span>
</span></span><span style="display:flex;"><span>        nn.Dropout(dropout1),
</span></span><span style="display:flex;"><span>        nn.Linear(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">256</span>),
</span></span><span style="display:flex;"><span>        nn.ReLU(),
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># 在第二个全连接层/激活函数之后添加一个dropout层</span>
</span></span><span style="display:flex;"><span>        nn.Dropout(dropout2),
</span></span><span style="display:flex;"><span>        nn.Linear(<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">10</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> init_weights(m):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">type</span>(m) == nn.Linear:
</span></span><span style="display:flex;"><span>        nn.init.normal_(m.weight, std=<span style="color:#ff0;font-weight:bold">0.01</span>)
</span></span><span style="display:flex;"><span>net.apply(init_weights);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer = torch.optim.SGD(net.parameters(), lr=lr)
</span></span><span style="display:flex;"><span>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="7-前向传播后向传播和计算图">7. 前向传播、后向传播和计算图<a hidden class="anchor" aria-hidden="true" href="#7-前向传播后向传播和计算图">#</a></h1>
<h2 id="71-前向传播">7.1 前向传播<a hidden class="anchor" aria-hidden="true" href="#71-前向传播">#</a></h2>
<ul>
<li><strong>前向传播</strong>：按顺序（从输入到输出）计算和存储神经网络中每层的结果；</li>
<li>如下图举例（简单起见，隐藏层不包括偏置项b）
<ul>
<li>输入层：<strong>x</strong></li>
<li>隐藏层：参数<strong>W(1)</strong>，输出结果<strong>z</strong>；激活函数(Φ)输出结果<strong>h</strong></li>
<li>输出层：参数<strong>W(2)</strong>，输出结果<strong>o</strong></li>
<li>预测损失：L = l(o, y)</li>
<li>惩罚项：s ~ W(1), W(2)</li>
<li>目标函数（正则化损失）：J = L + s</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240725074137329.png" alt="image-20240725074137329"  />
</p>
<h2 id="72-反向传播">7.2 反向传播<a hidden class="anchor" aria-hidden="true" href="#72-反向传播">#</a></h2>
<ul>
<li>反向传播：从输出层到输入层计算和存储神经网络的参数梯度；</li>
<li>如上示例
<ul>
<li>首先计算J分别关于L与s的梯度；</li>
<li>然后可计算J关于o的梯度，s关于W(1)、W(2)的梯度；</li>
<li>再依次计算J关于W(2)的梯度，J关于h的梯度，J关于z的梯度，J关于W(1)的梯度</li>
</ul>
</li>
</ul>
<h2 id="73-训练神经网络">7.3 训练神经网络<a hidden class="anchor" aria-hidden="true" href="#73-训练神经网络">#</a></h2>
<ul>
<li>前向传播与反向传播相互依赖；</li>
<li>例如
<ul>
<li>前向传播计算的正则化项s取决于模型w1与w2的当前值，由优化算法根据最近迭代的反向传播给出的；</li>
<li>反向传播参数的梯度计算，取决于由前向传播给出的隐藏变量h的当前值。</li>
</ul>
</li>
<li>反向传播需要重复利用前向传播中储存的中间值，副作用是需要暂用较多的内存。</li>
</ul>
<h1 id="8-数值稳定性和模型初始化">8. 数值稳定性和模型初始化<a hidden class="anchor" aria-hidden="true" href="#8-数值稳定性和模型初始化">#</a></h1>
<h2 id="81-梯度消失与梯度爆炸">8.1 梯度消失与梯度爆炸<a hidden class="anchor" aria-hidden="true" href="#81-梯度消失与梯度爆炸">#</a></h2>
<ul>
<li>向量对于向量的梯度为矩阵。当计算目标函数对于特定隐藏层参数的导数可能涉及多个矩阵的乘积。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240725134342762.png" alt="image-20240725134342762"  />
</p>
<ul>
<li><strong>梯度消失</strong>：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习
<ul>
<li>一个常见的例子是Sigmoid函数，参考上述2.2部分介绍</li>
<li>当Sigmoid函数的输入与输出很大或很小时，梯度近乎消失。</li>
</ul>
</li>
<li><strong>梯度爆炸</strong>：参数更新过大，破坏了模型的稳定收敛</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M = torch.normal(<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, size=(<span style="color:#ff0;font-weight:bold">4</span>,<span style="color:#ff0;font-weight:bold">4</span>))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;一个矩阵 </span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#39;</span>,M)
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> i in <span style="color:#fff;font-weight:bold">range</span>(<span style="color:#ff0;font-weight:bold">100</span>):
</span></span><span style="display:flex;"><span>    M = torch.mm(M,torch.normal(<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, size=(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">4</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">&#39;乘以100个矩阵后</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#39;</span>, M)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#一个矩阵</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[-0.7872,  2.7090,  0.5996, -1.3191],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#        [-1.8260, -0.7130, -0.5521,  0.1051],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#        [ 1.1213,  1.0472, -0.3991, -0.3802],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#        [ 0.5552,  0.4517, -0.3218,  0.5214]])</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#乘以100个矩阵后</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[-2.1897e+26,  8.8308e+26,  1.9813e+26,  1.7019e+26],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#        [ 1.3110e+26, -5.2870e+26, -1.1862e+26, -1.0189e+26],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#        [-1.6008e+26,  6.4559e+26,  1.4485e+26,  1.2442e+26],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#        [ 3.0943e+25, -1.2479e+26, -2.7998e+25, -2.4050e+25]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="82-参数初始化">8.2 参数初始化<a hidden class="anchor" aria-hidden="true" href="#82-参数初始化">#</a></h2>
<ul>
<li><strong>Xavier初始化</strong>：对全连接层权重的初始化取决于输入与输出层的神经元数量</li>
</ul>
<p>（1）若是正态分布，则从均值为0，方差如下的高斯分布中抽样权重；</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240725135448048.png" alt="image-20240725135448048"  />
</p>
<p>（2）也可以改为从下述范围的均匀分布中抽样权重</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20240725135618444.png" alt="image-20240725135618444"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lishensuo.github.io/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li>
      <li><a href="https://lishensuo.github.io/en/tags/d2l/">D2L</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lishensuo.github.io/en/posts/bioinfo/706d2l-%E7%AC%AC%E4%B8%89%E7%AB%A0%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
    <span class="title">« Prev Page</span>
    <br>
    <span>D2L--第三章线性神经网络</span>
  </a>
  <a class="next" href="https://lishensuo.github.io/en/posts/bioinfo/708d2l-%E7%AC%AC%E4%BA%94%E7%AB%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/">
    <span class="title">Next Page »</span>
    <br>
    <span>D2L--第五章深度学习计算</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://lishensuo.github.io/en/">Li&#39;s Bioinfo-Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
		<br/>您是本站第 <span id="busuanzi_value_site_uv"></span> 位访问者，总浏览量为 <span id="busuanzi_value_site_pv"></span> 次
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script type="text/javascript"
async
src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\[\[','\]\]']],
processEscapes: true,
processEnvironments: true,
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
TeX: { equationNumbers: { autoNumber: "AMS" },
extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});

MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>

<style>
code.has-jax {
font: inherit;
font-size: 100%;
background: inherit;
border: inherit;
color: #515151;
}
</style></body>
</html>

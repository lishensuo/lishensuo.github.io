<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">

<link rel="icon" href="/favicon.ico" type="image/x-icon"> 
<title>Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ– | Li&#39;s Bioinfo-Blog</title>
<meta name="keywords" content="æ·±åº¦å­¦ä¹ ">
<meta name="description" content="æ³¨æ„åŠ›è®¡ç®—

æ³¨æ„åŠ›è®¡ç®—çš„ä¸‰è¦ç´ åˆ†åˆ«æ˜¯ï¼šQueryï¼Œ Keyï¼ŒValueã€‚è€Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä¸‰è€…åˆ™æ˜¯ç­‰ä»·çš„ã€‚
ç»“åˆå¦‚ä¸‹å›¾ç¤ºä¾‹ï¼šä¸€ä¸ªåºåˆ—æœ‰2ä¸ªè¯å…ƒï¼Œæ¯ä¸ªè¯å…ƒæœ‰3ä¸ªç‰¹å¾ ,å³è¾“å…¥ä¸º(2, 3)

æ¯ä¸ªQueryè¯å…ƒä¼šè®¡ç®—ä¸å…¶å®ƒè¯å…ƒKeyçš„â€œç›¸ä¼¼åº¦â€ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œå†ç»è¿‡softmaxï¼ˆæ¯è¡Œçš„å’Œç­‰äº1ï¼‰è½¬æ¢ï¼Œå¾—åˆ° 2 Ã— 2 æƒé‡çŸ©é˜µ
ç„¶åå°†å…¶ä¸ValueçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—(2, 2) Ã— (2, 3)ï¼Œå¾—åˆ°æ–°çš„(2, 3)è¾“å‡ºç»“æœ

å½¢è±¡ç†è§£ï¼šå¯¹äºè¯å…ƒAçš„è¾“å‡ºç‰¹å¾1ï¼Œç­‰äºè¾“å…¥è¯å…ƒA, Bçš„ç‰¹å¾çš„åŠ æƒå’Œã€‚




å¤šå¤´æ³¨æ„åŠ›ï¼šæœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºå°†ç‰¹å¾ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ç§°ä¸ºä¸€ä¸ªâ€œå¤´â€ã€‚æ¯ä¸ªå¤´ç‹¬ç«‹è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶åœ¨ä¸€èµ·ï¼›ä»¥æœŸå­¦ä¹ ä¸åŒçš„å…³ç³»å’Œæ¨¡å¼ã€‚


">
<meta name="author" content="Lishensuo">
<link rel="canonical" href="https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.9e4de5e3ba61ea358168341aa7cdf70abfaafb7c697dfe8624af3ddff9a35c2f.css" integrity="sha256-nk3l47ph6jWBaDQap833Cr&#43;q&#43;3xpff6GJK893/mjXC8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js" integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://lishensuo.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://lishensuo.github.io/Q.gif">
<link rel="mask-icon" href="https://lishensuo.github.io/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ–" />
<meta property="og:description" content="æ³¨æ„åŠ›è®¡ç®—

æ³¨æ„åŠ›è®¡ç®—çš„ä¸‰è¦ç´ åˆ†åˆ«æ˜¯ï¼šQueryï¼Œ Keyï¼ŒValueã€‚è€Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä¸‰è€…åˆ™æ˜¯ç­‰ä»·çš„ã€‚
ç»“åˆå¦‚ä¸‹å›¾ç¤ºä¾‹ï¼šä¸€ä¸ªåºåˆ—æœ‰2ä¸ªè¯å…ƒï¼Œæ¯ä¸ªè¯å…ƒæœ‰3ä¸ªç‰¹å¾ ,å³è¾“å…¥ä¸º(2, 3)

æ¯ä¸ªQueryè¯å…ƒä¼šè®¡ç®—ä¸å…¶å®ƒè¯å…ƒKeyçš„â€œç›¸ä¼¼åº¦â€ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œå†ç»è¿‡softmaxï¼ˆæ¯è¡Œçš„å’Œç­‰äº1ï¼‰è½¬æ¢ï¼Œå¾—åˆ° 2 Ã— 2 æƒé‡çŸ©é˜µ
ç„¶åå°†å…¶ä¸ValueçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—(2, 2) Ã— (2, 3)ï¼Œå¾—åˆ°æ–°çš„(2, 3)è¾“å‡ºç»“æœ

å½¢è±¡ç†è§£ï¼šå¯¹äºè¯å…ƒAçš„è¾“å‡ºç‰¹å¾1ï¼Œç­‰äºè¾“å…¥è¯å…ƒA, Bçš„ç‰¹å¾çš„åŠ æƒå’Œã€‚




å¤šå¤´æ³¨æ„åŠ›ï¼šæœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºå°†ç‰¹å¾ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ç§°ä¸ºä¸€ä¸ªâ€œå¤´â€ã€‚æ¯ä¸ªå¤´ç‹¬ç«‹è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶åœ¨ä¸€èµ·ï¼›ä»¥æœŸå­¦ä¹ ä¸åŒçš„å…³ç³»å’Œæ¨¡å¼ã€‚


" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-10-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ–"/>
<meta name="twitter:description" content="æ³¨æ„åŠ›è®¡ç®—

æ³¨æ„åŠ›è®¡ç®—çš„ä¸‰è¦ç´ åˆ†åˆ«æ˜¯ï¼šQueryï¼Œ Keyï¼ŒValueã€‚è€Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä¸‰è€…åˆ™æ˜¯ç­‰ä»·çš„ã€‚
ç»“åˆå¦‚ä¸‹å›¾ç¤ºä¾‹ï¼šä¸€ä¸ªåºåˆ—æœ‰2ä¸ªè¯å…ƒï¼Œæ¯ä¸ªè¯å…ƒæœ‰3ä¸ªç‰¹å¾ ,å³è¾“å…¥ä¸º(2, 3)

æ¯ä¸ªQueryè¯å…ƒä¼šè®¡ç®—ä¸å…¶å®ƒè¯å…ƒKeyçš„â€œç›¸ä¼¼åº¦â€ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œå†ç»è¿‡softmaxï¼ˆæ¯è¡Œçš„å’Œç­‰äº1ï¼‰è½¬æ¢ï¼Œå¾—åˆ° 2 Ã— 2 æƒé‡çŸ©é˜µ
ç„¶åå°†å…¶ä¸ValueçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—(2, 2) Ã— (2, 3)ï¼Œå¾—åˆ°æ–°çš„(2, 3)è¾“å‡ºç»“æœ

å½¢è±¡ç†è§£ï¼šå¯¹äºè¯å…ƒAçš„è¾“å‡ºç‰¹å¾1ï¼Œç­‰äºè¾“å…¥è¯å…ƒA, Bçš„ç‰¹å¾çš„åŠ æƒå’Œã€‚




å¤šå¤´æ³¨æ„åŠ›ï¼šæœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºå°†ç‰¹å¾ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ç§°ä¸ºä¸€ä¸ªâ€œå¤´â€ã€‚æ¯ä¸ªå¤´ç‹¬ç«‹è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶åœ¨ä¸€èµ·ï¼›ä»¥æœŸå­¦ä¹ ä¸åŒçš„å…³ç³»å’Œæ¨¡å¼ã€‚


"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "åˆ†ç±»",
      "item": "https://lishensuo.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "ğŸ“– ç”Ÿä¿¡æ•°æ®åˆ†æ--åˆ†ææµç¨‹ï¼Œå·¥å…·åŒ…ç­‰",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ–",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ–",
  "name": "Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ–",
  "description": "æ³¨æ„åŠ›è®¡ç®—\næ³¨æ„åŠ›è®¡ç®—çš„ä¸‰è¦ç´ åˆ†åˆ«æ˜¯ï¼šQueryï¼Œ Keyï¼ŒValueã€‚è€Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä¸‰è€…åˆ™æ˜¯ç­‰ä»·çš„ã€‚ ç»“åˆå¦‚ä¸‹å›¾ç¤ºä¾‹ï¼šä¸€ä¸ªåºåˆ—æœ‰2ä¸ªè¯å…ƒï¼Œæ¯ä¸ªè¯å…ƒæœ‰3ä¸ªç‰¹å¾ ,å³è¾“å…¥ä¸º(2, 3) æ¯ä¸ªQueryè¯å…ƒä¼šè®¡ç®—ä¸å…¶å®ƒè¯å…ƒKeyçš„â€œç›¸ä¼¼åº¦â€ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œå†ç»è¿‡softmaxï¼ˆæ¯è¡Œçš„å’Œç­‰äº1ï¼‰è½¬æ¢ï¼Œå¾—åˆ° 2 Ã— 2 æƒé‡çŸ©é˜µ ç„¶åå°†å…¶ä¸ValueçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—(2, 2) Ã— (2, 3)ï¼Œå¾—åˆ°æ–°çš„(2, 3)è¾“å‡ºç»“æœ å½¢è±¡ç†è§£ï¼šå¯¹äºè¯å…ƒAçš„è¾“å‡ºç‰¹å¾1ï¼Œç­‰äºè¾“å…¥è¯å…ƒA, Bçš„ç‰¹å¾çš„åŠ æƒå’Œã€‚ å¤šå¤´æ³¨æ„åŠ›ï¼šæœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºå°†ç‰¹å¾ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ç§°ä¸ºä¸€ä¸ªâ€œå¤´â€ã€‚æ¯ä¸ªå¤´ç‹¬ç«‹è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶åœ¨ä¸€èµ·ï¼›ä»¥æœŸå­¦ä¹ ä¸åŒçš„å…³ç³»å’Œæ¨¡å¼ã€‚ ",
  "keywords": [
    "æ·±åº¦å­¦ä¹ "
  ],
  "articleBody": "æ³¨æ„åŠ›è®¡ç®—\næ³¨æ„åŠ›è®¡ç®—çš„ä¸‰è¦ç´ åˆ†åˆ«æ˜¯ï¼šQueryï¼Œ Keyï¼ŒValueã€‚è€Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä¸‰è€…åˆ™æ˜¯ç­‰ä»·çš„ã€‚ ç»“åˆå¦‚ä¸‹å›¾ç¤ºä¾‹ï¼šä¸€ä¸ªåºåˆ—æœ‰2ä¸ªè¯å…ƒï¼Œæ¯ä¸ªè¯å…ƒæœ‰3ä¸ªç‰¹å¾ ,å³è¾“å…¥ä¸º(2, 3) æ¯ä¸ªQueryè¯å…ƒä¼šè®¡ç®—ä¸å…¶å®ƒè¯å…ƒKeyçš„â€œç›¸ä¼¼åº¦â€ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œå†ç»è¿‡softmaxï¼ˆæ¯è¡Œçš„å’Œç­‰äº1ï¼‰è½¬æ¢ï¼Œå¾—åˆ° 2 Ã— 2 æƒé‡çŸ©é˜µ ç„¶åå°†å…¶ä¸ValueçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—(2, 2) Ã— (2, 3)ï¼Œå¾—åˆ°æ–°çš„(2, 3)è¾“å‡ºç»“æœ å½¢è±¡ç†è§£ï¼šå¯¹äºè¯å…ƒAçš„è¾“å‡ºç‰¹å¾1ï¼Œç­‰äºè¾“å…¥è¯å…ƒA, Bçš„ç‰¹å¾çš„åŠ æƒå’Œã€‚ å¤šå¤´æ³¨æ„åŠ›ï¼šæœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºå°†ç‰¹å¾ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ç§°ä¸ºä¸€ä¸ªâ€œå¤´â€ã€‚æ¯ä¸ªå¤´ç‹¬ç«‹è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶åœ¨ä¸€èµ·ï¼›ä»¥æœŸå­¦ä¹ ä¸åŒçš„å…³ç³»å’Œæ¨¡å¼ã€‚ æ³¨æ„åŠ›è®¡ç®—æœ¬èº«ä¸æ¶‰åŠå¯å­¦ä¹ å‚æ•°ã€‚ä¸€èˆ¬åœ¨inputå‰ï¼Œoutåï¼Œå„è®¾ç½®ä¸€å±‚MLPçº¿æ€§å˜æ¢ã€‚ å‚è€ƒï¼šhttps://blog.csdn.net/God_WeiYang/article/details/131820781\n0. æ¨¡æ‹Ÿæ•°æ® é€šå¸¸æƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›è®¡ç®—çš„è¾“å…¥æ•°æ®æ‹¥æœ‰å››ä¸ªç»´åº¦ï¼š(batch_size, num_heads, seq_length, head_dim)\nword embed = num_heads Ã— head_dim\n1 2 3 4 5 6 7 8 9 10 11 12 # æ³¨æ„ä¸ä¸Šé¢é¡ºåºä¸å¤ªç›¸åŒ def sample_input(bach_size, seq_length, n_head, n_dim): q = torch.randn((bach_size, seq_length, n_head, n_dim)).to(\"cuda:0\", torch.float16) k = torch.rand_like(q) v = torch.rand_like(q) return q, k, v q, k, v = sample_input(32, 100, 8, 64) q.shape, k.shape, v.shape # (torch.Size([32, 100, 8, 64]), # torch.Size([32, 100, 8, 64]), # torch.Size([32, 100, 8, 64])) æœ€ç»ˆè¯å…ƒçš„ç‰¹å¾é•¿åº¦ä¸ºï¼šnum_heads Ã— head_dim\n1. æ‰‹åŠ¨Attentionè®¡ç®— causalå‚æ•°è¡¨ç¤ºæ³¨æ„åŠ›æ©ç æ“ä½œï¼Œå¸¸ç”¨äºGPTç”Ÿæˆæ¨¡å‹ä¸­ã€‚è¡¨ç¤ºè®¡ç®—åºåˆ—ä¸­ç¬¬nä¸ªè¯å…ƒæ—¶ï¼Œåªå…³æ³¨ç¬¬1åˆ°n-1ä¸ªï¼ˆé™¤äº†æœ¬èº«ï¼‰ï¼› å…·ä½“é€šè¿‡torch.finfo(q.dtype).minè®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼Œåˆ™å…¶softmaxè½¬æ¢åçš„æƒé‡å€¼ä¸º0ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def custom_attention(q, k, v, causal=False): score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1)) if causal: mask = torch.triu(torch.ones(score.shape[-2], score.shape[-1]), diagonal=1) mask = mask.masked_fill(mask==1, torch.finfo(q.dtype).min) mask = mask.to(q.device, q.dtype) score = score + mask attn = F.softmax(score, dim=-1) o = torch.matmul(attn, v) return o q1 = q.transpose(1, 2) # torch.Size([32, 8, 100, 64]) k1 = k.transpose(1, 2) v1 = v.transpose(1, 2) o1 = custom_attention(q1, k1, v1) o1.transpose(2, 3).shape # torch.Size([32, 100, 8, 64]) 2. Flash Attention 1/2 Flash Attention ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ– Transformer æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒé€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜è®¡ç®—é€Ÿåº¦æ¥å¤„ç†é•¿åºåˆ—è¾“å…¥ã€‚ https://github.com/Dao-AILab/flash-attention 2.1 å®‰è£… ä¹‹å‰å­¦ä¹ çš„scGPTä¸»è¦ä½¿ç”¨äº†FlashAttention-1ï¼Œå®‰è£…æ–¹æ³•è§ä¹‹å‰çš„æ•´ç† 1 2 3 4 5 6 7 8 9 10 11 12 13 # scgptä¼ªä»£ç ç¤ºä¾‹ from flash_attn.flash_attention import FlashAttention # __init__ self.self_attn = FlashAttention(attention_dropout=attention_dropout) # forward pcpt_context, pcpt_attn_weights = self.self_attn( pcpt_qkv, key_padding_mask=pcpt_key_padding_mask, need_weights=need_weights, causal=self.causal, # If true, autoregressive modeling ) æœ¬æ¬¡ä¸»è¦å­¦ä¹ FlashAttention-2ç‰ˆæœ¬ ï¼ˆwith Better Parallelism and Work Partitioningï¼‰ã€‚å®‰è£…è¿‡ç¨‹è¸©äº†å¾ˆå¤šå‘ï¼Œç›®å‰æ‰¾åˆ°ä¸€ç§å¯ä¿¡çš„æ–¹å¼ã€‚ 1 2 3 4 5 6 7 8 9 10 conda create -n flash python=3.10 mamba -y # CUDA 11.8 mamba install cudatoolkit==11.8 -c nvidia # torch 2.3.0 mamba install pytorch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 pytorch-cuda=11.8 -c pytorch -c nvidia # nvcc conda install nvidia/label/cuda-11.8.0::cuda-nvcc nvcc -V # ä¸‹è½½æºæ–‡ä»¶ï¼Œæœ¬åœ°å®‰è£…ï¼šhttps://github.com/Dao-AILab/flash-attention/releases pip install 'flash_attn-2.6.3+cu118torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl' éæœ¬åœ°å®‰è£…æ–¹å¼ä¸ºï¼š\n1 2 3 4 5 pip install ninja #åŠ é€Ÿbuildå®‰è£…è¿›ç¨‹ï¼Œå®æµ‹å‘ç°åªå¯¹ä¸‹é¢çš„cloneå®‰è£…æœ‰ä½œç”¨ pip install flash-attn --no-build-isolation # or clone github repo python setup.py install ç”±äºæ˜¯åœ¨condaç¯å¢ƒä¸‹å®‰è£…çš„cudaï¼Œæ‰€ä»¥åœ¨ä¸Šè¿°è¿‡ç¨‹ä¸­ä¼šå‡ºç°ç±»ä¼¼cuda_runtime_api.h: No such file or directoryçš„æŠ¥é”™ã€‚æŸ¥äº†å¾ˆå¤šæ•™ç¨‹ï¼Œæ¯”è¾ƒé è°±çš„æ–¹æ³•æ˜¯å®‰è£…cudatoolkit-devã€‚è¿™åŒæ—¶å¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼Œç›®å‰å…¶æœ€é«˜ç‰ˆæœ¬ä¸º11.7ï¼Œéœ€è¦å®‰è£…ä¸ä¹‹å¯¹åº”çš„cudaç¯å¢ƒï¼Œä»¥åŠtorchç­‰\n1 2 3 4 5 mamba install -c conda-forge cudatoolkit-dev mamba install cudatoolkit==11.7 -c nvidia mamba install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia python setup.py install 2.2 ä½¿ç”¨ è¾“å…¥ç»´åº¦è¦æ±‚ä¸€èˆ¬æ˜¯ï¼š(batch_size, seqlen, nheads, headdim) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from flash_attn import flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func # flash_attn_funcå¸¸è§„è®¡ç®— o2 = flash_attn_func(q, k, v) o2.shape # torch.Size([32, 100, 8, 64]) # flash_attn_qkvpacked_funcæ‰“åŒ…è®¡ç®— qkv_pack = torch.concat([q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)], dim=2) # (batch_size, seqlen, 3, nheads, headdim) qkv_pack.shape # torch.Size([32, 100, 3, 8, 64]) flash_attn_qkvpacked_func(qkv_pack).shape # torch.Size([32, 100, 8, 64]) 3. F.scaled_dot_product_attention torch.nn.functional.scaled_dot_product_attentionæ˜¯torch2.0ç‰ˆæœ¬æ›´æ–°åï¼Œæ–°å¢çš„æ³¨æ„åŠ›åŠ é€Ÿè®¡ç®—æ–¹æ³• https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html https://pytorch.apachecn.org/2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/ å…¶é‡‡ç”¨äº†åŒ…æ‹¬Flash-attention2åœ¨å†…çš„ä¸‰ç§åŠ é€Ÿç®—æ³• FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning Memory-Efficient Attention A PyTorch implementation defined in C++ matching the above formulation All implementations are enabled by default. Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs 1 2 3 4 5 6 7 8 9 10 import torch.nn.functional as F # è¾“å…¥è¦æ±‚(batch_size, nheads, seqlen, headdim) q3 = q.transpose(1, 2) # torch.Size([32, 8, 100, 64]) k3 = k.transpose(1, 2) v3 = v.transpose(1, 2) o3 = F.scaled_dot_product_attention(q3, k3, v3) o3.transpose(1, 2).shape # torch.Size([32, 100, 8, 64]) 4. Benchmark 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import math import time from einops import rearrange import torch import torch.nn.functional as F from flash_attn import flash_attn_func def custom_attention(q, k, v, causal=False): score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1)) if causal: mask = torch.triu(torch.ones(score.shape[-2], score.shape[-1]), diagonal=1) mask = mask.masked_fill(mask==1, torch.finfo(q.dtype).min) mask = mask.to(q.device, q.dtype) score = score + mask attn = F.softmax(score, dim=-1) o = torch.matmul(attn, v) return o def pytorch_func(q, k, v, causal=False): o = F.scaled_dot_product_attention(q, k, v, is_causal=causal) # o = F.scaled_dot_product_attention(q, k, v, is_causal=causal)[0] return o def flash_attention(q, k, v, causal=False): o = flash_attn_func(q, k, v, causal=causal) return o å®šä¹‰å‡½æ•°ï¼Œæµ‹è¯•æ³¨æ„åŠ›è®¡ç®—çš„æ—¶é—´ä»¥åŠæ˜¾å­˜æ¶ˆè€— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def test(func_name, q, k, v, *args, **kwargs): if func_name in [\"custom_attention\", \"pytorch_func\"]: q = rearrange(q, \"a b c d -\u003e a c b d\") k = rearrange(k, \"a b c d -\u003e a c b d\") v = rearrange(v, \"a b c d -\u003e a c b d\") torch.cuda.reset_peak_memory_stats() # é‡ç½® CUDA å†…å­˜ç»Ÿè®¡ä¿¡æ¯ torch.cuda.synchronize() # ç¡®ä¿æ‰€æœ‰ CUDA æ“ä½œå®Œæˆåå†ç»§ç»­ # globals():å­—å…¸ï¼ŒåŒ…å«äº†å½“å‰ä½œç”¨åŸŸå†…çš„æ‰€æœ‰å…¨å±€å˜é‡å’Œå‡½æ•° for _ in range(5): o = globals()[func_name](q, k, v, *args, **kwargs) torch.cuda.synchronize() st = time.time() o = globals()[func_name](q, k, v, *args, **kwargs) torch.cuda.synchronize() tt = time.time() - st max_memory = torch.cuda.max_memory_allocated() // 2**20 #å•ä½MB torch.cuda.empty_cache() # æ¸…é™¤æœªä½¿ç”¨çš„å†…å­˜ï¼ˆé‡Šæ”¾é‚£äº›å·²è¢«åˆ é™¤ä½†æœªé‡Šæ”¾çš„å†…å­˜ï¼‰ if func_name in [\"custom_attention\", \"pytorch_func\"]: o = rearrange(o, \"a c b d -\u003e a b c d\") return o, tt, max_memory æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ï¼Œä¸‰ç§è®¡ç®—æ—¶é—´ä»¥åŠæ˜¾å­˜æ¶ˆè€—æƒ…å†µ ï¼ˆ1ï¼‰åºåˆ—çš„é•¿åº¦è¶Šé•¿æ—¶ï¼Œpytorch funcä¸flash attentionè®¡ç®—ä¼˜åŠ¿è¶Šæ˜æ˜¾ ï¼ˆ2ï¼‰pytorch funcä¸flash attentionçš„å·®è·ä¸å¤ªæ˜æ˜¾ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 for seqlen in [256, 512, 1024]: print(f\"## Sequence length: {seqlen}\") q, k, v = sample_input(32, seqlen, 8, 64) ## (1) o, t, m = test(\"custom_attention\", q, k, v, causal=False) print(f\"custom pytorch time: {t:.6f}, peak memory: {m} MB\") ## (2) pf_o, pf_t, pf_m = test(\"pytorch_func\", q, k, v, causal=False) print(f\"pytorch func time: {pf_t:.6f}, speedup: {t/pf_t:.2f}; peak memory: {pf_m} MB, save: {int((m-pf_m)/m*100)}%\") assert torch.allclose(o, pf_o, rtol=1e-2, atol=1e-2) ## (3) fa_o, fa_t, fa_m = test(\"flash_attention\", q, k, v, causal=False) print(f\"flash attention time: {fa_t:.6f}, speedup: {t/fa_t:.2f}; peak memory: {fa_m} MB, save: {int((m-fa_m)/m*100)}%\") assert torch.allclose(o, fa_o, rtol=1e-2, atol=1e-2) # ## Sequence length: 256 # custom pytorch time: 0.000259, peak memory: 216 MB # pytorch func time: 0.000058, speedup: 4.44; peak memory: 120 MB, save: 44% # flash attention time: 0.000073, speedup: 3.54; peak memory: 96 MB, save: 55% # ## Sequence length: 512 # custom pytorch time: 0.001135, peak memory: 384 MB # pytorch func time: 0.000142, speedup: 7.98; peak memory: 120 MB, save: 68% # flash attention time: 0.000154, speedup: 7.38; peak memory: 128 MB, save: 66% # ## Sequence length: 1024 # custom pytorch time: 0.004096, peak memory: 1272 MB # pytorch func time: 0.000478, speedup: 8.58; peak memory: 233 MB, save: 81% # flash attention time: 0.000493, speedup: 8.30; peak memory: 249 MB, save: 80% æµ‹è¯•ä¸åŒç‰¹å¾é•¿åº¦ï¼Œä¸‰ç§è®¡ç®—æ—¶é—´ä»¥åŠæ˜¾å­˜æ¶ˆè€—æƒ…å†µ è¯å…ƒçš„ç‰¹å¾ç»´åº¦è¶Šå¤§æ—¶ï¼Œpytorch funcä¸flash attentionè®¡ç®—ä¼˜åŠ¿ä¼šé€æ¸ä¸‹é™ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 for seqlen in [256, 512, 1024]: print(f\"## Sequence length: {seqlen}\") q, k, v = sample_input(32, seqlen, 8, 64) ## (1) o, t, m = test(\"custom_attention\", q, k, v, causal=False) print(f\"custom pytorch time: {t:.6f}, peak memory: {m} MB\") ## (2) pf_o, pf_t, pf_m = test(\"pytorch_func\", q, k, v, causal=False) print(f\"pytorch func time: {pf_t:.6f}, speedup: {t/pf_t:.2f}; peak memory: {pf_m} MB, save: {int((m-pf_m)/m*100)}%\") assert torch.allclose(o, pf_o, rtol=1e-2, atol=1e-2) ## (3) fa_o, fa_t, fa_m = test(\"flash_attention\", q, k, v, causal=False) print(f\"flash attention time: {fa_t:.6f}, speedup: {t/fa_t:.2f}; peak memory: {fa_m} MB, save: {int((m-fa_m)/m*100)}%\") assert torch.allclose(o, fa_o, rtol=1e-2, atol=1e-2) # ## Sequence length: 32 # custom pytorch time: 0.003847, peak memory: 1128 MB # pytorch func time: 0.000276, speedup: 13.92; peak memory: 105 MB, save: 90% # flash attention time: 0.000286, speedup: 13.45; peak memory: 121 MB, save: 89% # ## Sequence length: 64 # custom pytorch time: 0.004104, peak memory: 1272 MB # pytorch func time: 0.000485, speedup: 8.45; peak memory: 233 MB, save: 81% # flash attention time: 0.000500, speedup: 8.21; peak memory: 249 MB, save: 80% # ## Sequence length: 128 # custom pytorch time: 0.004594, peak memory: 1512 MB # pytorch func time: 0.000952, speedup: 4.82; peak memory: 457 MB, save: 69% # flash attention time: 0.000956, speedup: 4.81; peak memory: 489 MB, save: 67% 5. nn.MultiheadAttentionåŠmaskæ“ä½œ torch.nn.MultiheadAttention\nhttps://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\ntorch å®ç°çš„æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›å±‚ç±»ï¼ŒåŒ…å«å®Œæ•´è¾“å…¥ä¸è¾“å‡ºçš„æƒé‡å‚æ•° 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn query = key = value = torch.randn(4, 10, 512) # å®ä¾‹åŒ– mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True, dropout=0.0) # forwardå‰å‘è®¡ç®— out_put, attn_weight = mha(query, key, value, need_weights = True) out_put.shape # torch.Size([4, 10, 512]) attn_weight.shape # torch.Size([4, 10, 10]) # æŸ¥çœ‹æ³¨æ„åŠ›å±‚å‚æ•° for name, param in mha.named_parameters(): print(f\"Name: {name}\") print(f\"Shape: {param.shape}\\n\") # Name: in_proj_weight # Shape: torch.Size([1536, 512]) # Name: in_proj_bias # Shape: torch.Size([1536]) # Name: out_proj.weight # Shape: torch.Size([512, 512]) # Name: out_proj.bias # Shape: torch.Size([512]) é»˜è®¤need_weights = Trueï¼Œå³è¿”å›å¤šå¤´æ³¨æ„åŠ›çŸ©é˜µè®¡ç®—ç»“æœã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œuse the optimized scaled_dot_product_attention and achieve the best performance for MHA.\nå‰å‘è®¡ç®—çš„maskç›¸å…³å‚æ•°\nkey_padding_maskï¼šç”¨äºæ ‡è®°ä¸€ä¸ªåºåˆ—ä¸­çš„padå¡«å……å­—ç¬¦ï¼Œä½¿å¾—queryä¸ä¼šå…³æ³¨ä¸å…¶çš„æ³¨æ„åŠ›ã€‚\nå…¶shapeé€šå¸¸æ˜¯ (Batch_size, Seq_len)ã€‚ Trueè¡¨ç¤ºæ˜¯padå¡«å……å­—ç¬¦ 1 2 3 4 5 6 7 8 9 10 key_padding_mask = torch.tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]).bool() #(4, 10) out_put, _ = mha(query, key, value, need_weights = False, key_padding_mask=key_padding_mask) out_put[0,:3,:3] # tensor([[ 0.3662, -0.3266, -0.2634], # [ 0.0777, -0.1854, -0.0766], # [ 0.2575, -0.0160, -0.1000]], grad_fn=) attn_maskï¼šæ³¨æ„åŠ›æ©ç çŸ©é˜µï¼Œç›´æ¥å¯¹ç‰¹å®šçš„query-keyç»„åˆè¿›è¡Œæ©ç æ“ä½œ\nå…¶shapeé€šå¸¸æ˜¯(Batch_size*Num_head, Seq_len, Seq_len) Trueè¡¨ç¤ºç‰¹å®šæ³¨æ„åŠ›è¢«æ©ç  ä¸key_padding_maskæœ€ç»ˆmaskçš„æ•ˆæœæ˜¯ç›¸åŒçš„ã€‚å¦‚ä¸‹æ¼”ç¤ºå¦‚ä½•å°†key_padding_maskè½¬æ¢ä¸ºattn_maskï¼Œå¾—åˆ°ä¸€è‡´çš„è¾“å‡ºç»“æœã€‚ https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion 1 2 3 4 5 6 7 8 9 10 attn_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) attn_mask = attn_mask.expand(4, 8, 10, 10) attn_mask = attn_mask.reshape(4 * 8, 10, 10) # torch.Size([32, 10, 10]) out_put, _ = mha(query, key, value, need_weights = False, attn_mask=attn_mask) out_put[0,:3,:3] # tensor([[ 0.3662, -0.3266, -0.2634], # [ 0.0777, -0.1854, -0.0766], # [ 0.2575, -0.0160, -0.1000]], grad_fn=) å…³äºä¸Šè¿°key_padding_maskçš„è½¬æ¢æœ‰å¦‚ä¸‹å€¼å¾—æ³¨æ„çš„ç»†èŠ‚ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mask = torch.randn(3, 4) # (Batch_size, Seq_len), num_head = 2 ## ç›´æ¥repeatä¸æ˜¯æ‰€æœŸæœ›çš„ç»“æœ mask.unsqueeze(1).repeat(2,1,1) # repeat all batch as whole and cannot match the multi-head # tensor([[[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]], # [[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]]]) ## å¦‚ä¸‹ä¸¤ç§æ–¹å¼å‡æ˜¯ç¬¦åˆé¢„æœŸçš„è½¬æ¢ mask.unsqueeze(1).unsqueeze(2).expand(3, 2, 1, 4).reshape(6, 1, 4) # repeat each seq two multi-head (expected) # tensor([[[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]]]) mask.unsqueeze(1).repeat_interleave(repeats=2, dim=0) # repeat each seq two multi-head (expected) 6. Flash-Attn V2 maskæ“ä½œ åœ¨ flash_attn v1ä¸­çš„ï¼Œ FlashAttentionæ³¨æ„åŠ›è®¡ç®—æ˜¯æ”¯æŒkey_padding_maskå‚æ•°çš„ï¼›åŒæ—¶ä¹Ÿæä¾›äº†å°è£…å¥½çš„ FlashMHAæ³¨æ„åŠ›å±‚ï¼ˆåŒ…æ‹¬æƒé‡å¯å­¦ä¹ å‚æ•°ï¼‰ã€‚ ï¼Falseä»£è¡¨å¡«å……å­—ç¬¦ï¼Œè¿™ä¸ä¸Šé¢çš„MultiheadAttentionç›¸å åœ¨ flash_attn v2ä¸­ï¼Œflash_attn_funcæœ¬èº«æ˜¯ä»…ç”¨äºè®¡ç®—æ³¨æ„åŠ›è¿‡ç¨‹ï¼Œæ²¡æœ‰maskæ“ä½œã€‚ 1 2 3 from flash_attn import flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func help(flash_attn_func) åœ¨torch v2å¼•å…¥çš„F.scaled_dot_product_attentionå¼•å…¥äº†Flash-Attn v2ï¼Œå…·ä½“å‚çœ‹ä¸Šé¢ç¬¬ä¸‰ç‚¹çš„ä»‹ç»ã€‚å€¼å¾—æ³¨æ„æ˜¯ï¼Œå®ƒæ˜¯æ”¯æŒattn_maskå‚æ•°çš„ã€‚ attn_mask (optional Tensor) â€“ Attention mask; shape must be broadcastable to the shape of attention weights, which is (N,â€¦,L,S)(N,â€¦,L,S). Two types of masks are supported. A boolean mask where a value of True indicates that the element should take part in attention. A float mask of the same type as query, key, value that is added to the attention score.\nå¦‚ä¸Šå‚æ•°è¯´æ˜æœ‰ä¸¤ä¸ªæ³¨æ„ç‚¹\nï¼ˆ1ï¼‰å…³äºshapeï¼Œä¸å¼ºåˆ¶è¦æ±‚æ˜¯(Batch_size*Num_head, Seq_len, Seq_len)ã€‚broadcastableä¹Ÿå¯ä»¥ï¼›\nï¼ˆ2ï¼‰ä¸Flash-Attnä¸€æ ·ï¼ŒFalseè¡¨ç¤ºå¡«å……/è¢«æ©ç \n1 2 3 4 5 6 7 8 9 10 # æ–¹å¼1 attn_mask = key_padding_mask.unsqueeze(1) attn_mask.shape # torch.Size([4, 1, 10]) output = F.scaled_dot_product_attention( query, key, value, attn_mask=attn_mask ) output[0,:3,:3] # tensor([[ 0.7855, -0.2025, -0.5377], # [ 0.1681, -2.8067, 0.0794], # [-0.1817, 0.0185, 0.1878]]) 1 2 3 4 5 6 7 8 9 10 # æ–¹å¼2 attn_mask = key_padding_mask.unsqueeze(1).repeat(1,10,1) attn_mask.shape # torch.Size([4, 10, 10]) output = F.scaled_dot_product_attention( query, key, value, attn_mask=attn_mask ) output[0,:3,:3] # tensor([[ 0.7855, -0.2025, -0.5377], # [ 0.1681, -2.8067, 0.0794], # [-0.1817, 0.0185, 0.1878]]) æ³¨æ„ç‚¹ï¼š\nï¼ˆ1ï¼‰ä¸Šè¿°ä»…ä¸ºå•å¤´æ³¨æ„åŠ›è®¡ç®—ï¼›\nï¼ˆ2ï¼‰F.scaled_dot_product_attentionä»…æä¾›æ³¨æ„åŠ›è®¡ç®—ï¼Œä¸èƒ½ä½œä¸ºå®Œæ•´çš„æ³¨æ„åŠ›å±‚ï¼ˆç¼ºå°‘æƒé‡å‚æ•°ï¼‰\næ ¸å¿ƒï¼šattn_maskä¸key_padding_maskå‚æ•°å¹¶æ²¡æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚\nè¡¥å……~\nSP1. Performer https://arxiv.org/pdf/2009.14794v4 2020 Google Research https://github.com/lucidrains/performer-pytorch Pytorchç‰ˆæœ¬å®ç° Performeræ³¨æ„åŠ›ï¼šå°†å¤æ‚åº¦é™ä½åˆ°çº¿æ€§ï¼Œä½¿å¾—å®ƒåœ¨å¤„ç†é•¿åºåˆ—æ—¶æ›´åŠ é«˜æ•ˆï¼›å¹¶ä»ç†è®ºè§’åº¦è¯æ˜æ˜¯å¯è¡Œçš„ã€‚\nå¯¹äºé•¿åº¦ä¸ºLçš„è¾“å…¥åºåˆ—ï¼ŒåµŒå…¥å‘é‡é•¿åº¦æ˜¯d\nè®¡ç®—å¸¸è§„Transfomerè‡ªæ³¨æ„åŠ›æ—¶ (ä¸‹å›¾å·¦æ )ï¼Œå…¶å¤æ‚åº¦ä¸ºO(L2) â†’ Quadratic;\n(1) æ³¨æ„åŠ›çŸ©é˜µï¼šQ * (K)T = (L, d) * (d, L) = (L, L) â†’ O(L2 * d) â†’ O(L2) (2) Softmaxè®¡ç®—å½’ä¸€åŒ–æƒé‡ï¼š(L, L) = (L, L) â†’ O(L2) (3) åŠ æƒå’Œè¡¨ç¤ºï¼šQ * (K)T * V = (L, L) * (L, d) = (L, d) â†’ O(L2 * d) â†’ O(L2) Performeræ³¨æ„åŠ›è®¡ç®—çš„æ³¨æ„åŠ›è®¡ç®— (ä¸‹å›¾å³æ ) å¤æ‚åº¦ä¸ºO(L) â†’ Linear\n(1) é¦–å…ˆå¯¹Qä¸Kè¿›è¡Œéšæœºç‰¹å¾æ˜ å°„ï¼ˆRandom Feature Mappingï¼‰ Qï¼š(L, d) â†’ Qâ€™ (L, r), Kï¼š(L, d) â†’ Kâ€™ (L, r) (2) ç„¶åè®¡ç®— (Kâ€™)T * V = (r, L) * (L, d) = (r, d) â†’ O(Lrd) â†’ O(L) (3) æœ€åè®¡ç®— Qâ€™ * (Kâ€™)T * V = (L, r) * (r, d) = (L, d) â†’ O(Lrd) â†’ O(L) è®¡ç®—å¤æ‚åº¦æ—¶ï¼Œå¯å¿½ç•¥å¸¸æ•°é¡¹ã€ä½é˜¶é¡¹ï¼Œä»¥åŠç³»æ•°ã€‚\n",
  "wordCount" : "4575",
  "inLanguage": "en",
  "datePublished": "2024-10-28T00:00:00Z",
  "dateModified": "2024-10-28T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Lishensuo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Li's Bioinfo-Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lishensuo.github.io/img/Q.gif"
    }
  }
}
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lishensuo.github.io/en/" accesskey="h" title="Li&#39;s Bioinfo-Blog (Alt + H)">Li&#39;s Bioinfo-Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lishensuo.github.io/en/" title="ä¸»é¡µ">
                    <span>ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/posts" title="åˆ†ç±»">
                    <span>åˆ†ç±»</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/tags" title="æ ‡ç­¾">
                    <span>æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/archives/" title="å½’æ¡£">
                    <span>å½’æ¡£</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/about" title="å…³äº">
                    <span>å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/search" title="æœç´¢ (Alt &#43; /)" accesskey=/>
                    <span>æœç´¢</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lishensuo.github.io/en/">Home</a>&nbsp;Â»&nbsp;<a href="https://lishensuo.github.io/en/posts/">åˆ†ç±»</a>&nbsp;Â»&nbsp;<a href="https://lishensuo.github.io/en/posts/bioinfo/">ğŸ“– ç”Ÿä¿¡æ•°æ®åˆ†æ--åˆ†ææµç¨‹ï¼Œå·¥å…·åŒ…ç­‰</a></div>
    <h1 class="post-title">
      Flash Attentionæ³¨æ„åŠ›ä¼˜åŒ–
    </h1>
    <div class="post-meta">













Create:&amp;nbsp;&lt;span title=&#39;2024-10-28 00:00:00 &#43;0000 UTC&#39;&gt;2024-10-28&lt;/span&gt;&amp;nbsp;|&amp;nbsp;Update:&amp;nbsp;2024-10-28&amp;nbsp;|&amp;nbsp;Words:&amp;nbsp;4575&amp;nbsp;|&amp;nbsp;10 min&amp;nbsp;|&amp;nbsp;Lishensuo

|  Viewers: <span id="busuanzi_value_page_pv"></span> 
	  
    </div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#0-%e6%a8%a1%e6%8b%9f%e6%95%b0%e6%8d%ae" aria-label="0. æ¨¡æ‹Ÿæ•°æ®">0. æ¨¡æ‹Ÿæ•°æ®</a></li>
                    <li>
                        <a href="#1-%e6%89%8b%e5%8a%a8attention%e8%ae%a1%e7%ae%97" aria-label="1. æ‰‹åŠ¨Attentionè®¡ç®—">1. æ‰‹åŠ¨Attentionè®¡ç®—</a></li>
                    <li>
                        <a href="#2-flash-attention-12" aria-label="2. Flash Attention 1/2">2. Flash Attention 1/2</a><ul>
                            
                    <li>
                        <a href="#21-%e5%ae%89%e8%a3%85" aria-label="2.1 å®‰è£…">2.1 å®‰è£…</a></li>
                    <li>
                        <a href="#22-%e4%bd%bf%e7%94%a8" aria-label="2.2 ä½¿ç”¨">2.2 ä½¿ç”¨</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-fscaled_dot_product_attention" aria-label="3. F.scaled_dot_product_attention">3. F.scaled_dot_product_attention</a></li>
                    <li>
                        <a href="#4-benchmark" aria-label="4. Benchmark">4. Benchmark</a></li>
                    <li>
                        <a href="#5-nnmultiheadattention%e5%8f%8amask%e6%93%8d%e4%bd%9c" aria-label="5. nn.MultiheadAttentionåŠmaskæ“ä½œ">5. nn.MultiheadAttentionåŠmaskæ“ä½œ</a></li>
                    <li>
                        <a href="#6-flash-attn-v2-mask%e6%93%8d%e4%bd%9c" aria-label="6. Flash-Attn V2 maskæ“ä½œ">6. Flash-Attn V2 maskæ“ä½œ</a></li>
                    <li>
                        <a href="#sp1-performer" aria-label="SP1. Performer">SP1. Performer</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>


  <div class="post-content"><p><strong>æ³¨æ„åŠ›è®¡ç®—</strong></p>
<ul>
<li><strong>æ³¨æ„åŠ›</strong>è®¡ç®—çš„ä¸‰è¦ç´ åˆ†åˆ«æ˜¯ï¼šQueryï¼Œ Keyï¼ŒValueã€‚è€Œåœ¨<strong>è‡ªæ³¨æ„åŠ›</strong>è®¡ç®—ä¸­ï¼Œä¸‰è€…åˆ™æ˜¯ç­‰ä»·çš„ã€‚</li>
<li>ç»“åˆå¦‚ä¸‹å›¾ç¤ºä¾‹ï¼šä¸€ä¸ªåºåˆ—æœ‰2ä¸ªè¯å…ƒï¼Œæ¯ä¸ªè¯å…ƒæœ‰3ä¸ªç‰¹å¾ ,å³è¾“å…¥ä¸º(2, 3)
<ul>
<li>æ¯ä¸ªQueryè¯å…ƒä¼šè®¡ç®—ä¸å…¶å®ƒè¯å…ƒKeyçš„â€œç›¸ä¼¼åº¦â€ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ï¼Œå†ç»è¿‡softmaxï¼ˆæ¯è¡Œçš„å’Œç­‰äº1ï¼‰è½¬æ¢ï¼Œå¾—åˆ° 2 Ã— 2 æƒé‡çŸ©é˜µ</li>
<li>ç„¶åå°†å…¶ä¸ValueçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—(2, 2) Ã— (2, 3)ï¼Œå¾—åˆ°æ–°çš„(2, 3)è¾“å‡ºç»“æœ
<ul>
<li>å½¢è±¡ç†è§£ï¼šå¯¹äºè¯å…ƒAçš„è¾“å‡ºç‰¹å¾1ï¼Œç­‰äºè¾“å…¥è¯å…ƒA, Bçš„ç‰¹å¾çš„åŠ æƒå’Œã€‚</li>
</ul>
</li>
</ul>
</li>
<li><strong>å¤šå¤´æ³¨æ„åŠ›</strong>ï¼šæœ¬è´¨ä¸Šå¯ä»¥ç†è§£ä¸ºå°†ç‰¹å¾ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ç§°ä¸ºä¸€ä¸ªâ€œå¤´â€ã€‚æ¯ä¸ªå¤´ç‹¬ç«‹è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºåˆå¹¶åœ¨ä¸€èµ·ï¼›ä»¥æœŸå­¦ä¹ ä¸åŒçš„å…³ç³»å’Œæ¨¡å¼ã€‚</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20241027123614983.png" alt="image-20241027123614983"  />
</p>
<ul>
<li>æ³¨æ„åŠ›è®¡ç®—æœ¬èº«ä¸æ¶‰åŠå¯å­¦ä¹ å‚æ•°ã€‚ä¸€èˆ¬åœ¨inputå‰ï¼Œoutåï¼Œå„è®¾ç½®ä¸€å±‚MLPçº¿æ€§å˜æ¢ã€‚</li>
</ul>
<blockquote>
<p>å‚è€ƒï¼šhttps://blog.csdn.net/God_WeiYang/article/details/131820781</p></blockquote>
<h1 id="0-æ¨¡æ‹Ÿæ•°æ®">0. æ¨¡æ‹Ÿæ•°æ®<a hidden class="anchor" aria-hidden="true" href="#0-æ¨¡æ‹Ÿæ•°æ®">#</a></h1>
<ul>
<li>
<p>é€šå¸¸æƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›è®¡ç®—çš„è¾“å…¥æ•°æ®æ‹¥æœ‰å››ä¸ªç»´åº¦ï¼š(batch_size, num_heads, seq_length, head_dim)</p>
<blockquote>
<p>word embed = num_heads Ã— head_dim</p></blockquote>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># æ³¨æ„ä¸ä¸Šé¢é¡ºåºä¸å¤ªç›¸åŒ</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> sample_input(bach_size, seq_length, n_head, n_dim):
</span></span><span style="display:flex;"><span>    q = torch.randn((bach_size, seq_length, n_head, n_dim)).to(<span style="color:#0ff;font-weight:bold">&#34;cuda:0&#34;</span>, torch.float16)
</span></span><span style="display:flex;"><span>    k = torch.rand_like(q)
</span></span><span style="display:flex;"><span>    v = torch.rand_like(q)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> q, k, v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>q, k, v = sample_input(<span style="color:#ff0;font-weight:bold">32</span>, <span style="color:#ff0;font-weight:bold">100</span>, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">64</span>)
</span></span><span style="display:flex;"><span>q.shape, k.shape, v.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># (torch.Size([32, 100, 8, 64]),</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#  torch.Size([32, 100, 8, 64]),</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#  torch.Size([32, 100, 8, 64]))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>æœ€ç»ˆè¯å…ƒçš„ç‰¹å¾é•¿åº¦ä¸ºï¼šnum_heads  Ã— head_dim</p></blockquote>
<h1 id="1-æ‰‹åŠ¨attentionè®¡ç®—">1. æ‰‹åŠ¨Attentionè®¡ç®—<a hidden class="anchor" aria-hidden="true" href="#1-æ‰‹åŠ¨attentionè®¡ç®—">#</a></h1>
<ul>
<li>causalå‚æ•°è¡¨ç¤ºæ³¨æ„åŠ›æ©ç æ“ä½œï¼Œå¸¸ç”¨äºGPTç”Ÿæˆæ¨¡å‹ä¸­ã€‚è¡¨ç¤ºè®¡ç®—åºåˆ—ä¸­ç¬¬nä¸ªè¯å…ƒæ—¶ï¼Œåªå…³æ³¨ç¬¬1åˆ°n-1ä¸ªï¼ˆé™¤äº†æœ¬èº«ï¼‰ï¼›</li>
<li>å…·ä½“é€šè¿‡<code>torch.finfo(q.dtype).min</code>è®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼Œåˆ™å…¶softmaxè½¬æ¢åçš„æƒé‡å€¼ä¸º0ã€‚</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> custom_attention(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    score = torch.matmul(q, k.transpose(-<span style="color:#ff0;font-weight:bold">2</span>, -<span style="color:#ff0;font-weight:bold">1</span>)) / math.sqrt(q.size(-<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> causal:
</span></span><span style="display:flex;"><span>        mask = torch.triu(torch.ones(score.shape[-<span style="color:#ff0;font-weight:bold">2</span>], score.shape[-<span style="color:#ff0;font-weight:bold">1</span>]), diagonal=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        mask = mask.masked_fill(mask==<span style="color:#ff0;font-weight:bold">1</span>, torch.finfo(q.dtype).min)
</span></span><span style="display:flex;"><span>        mask = mask.to(q.device, q.dtype)
</span></span><span style="display:flex;"><span>        score = score + mask
</span></span><span style="display:flex;"><span>    attn = F.softmax(score, dim=-<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>    o = torch.matmul(attn, v)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>q1 = q.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>) <span style="color:#007f7f"># torch.Size([32, 8, 100, 64])</span>
</span></span><span style="display:flex;"><span>k1 = k.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>v1 = v.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>o1 = custom_attention(q1, k1, v1)
</span></span><span style="display:flex;"><span>o1.transpose(<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">3</span>).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="2-flash-attention-12">2. Flash Attention 1/2<a hidden class="anchor" aria-hidden="true" href="#2-flash-attention-12">#</a></h1>
<ul>
<li>Flash Attention ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ– Transformer æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒé€šè¿‡å‡<strong>å°‘å†…å­˜</strong>ä½¿ç”¨å’Œ<strong>æé«˜è®¡ç®—é€Ÿåº¦</strong>æ¥å¤„ç†é•¿åºåˆ—è¾“å…¥ã€‚</li>
<li><a href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a></li>
</ul>
<h2 id="21-å®‰è£…">2.1 å®‰è£…<a hidden class="anchor" aria-hidden="true" href="#21-å®‰è£…">#</a></h2>
<ul>
<li>ä¹‹å‰å­¦ä¹ çš„scGPTä¸»è¦ä½¿ç”¨äº†FlashAttention-1ï¼Œå®‰è£…æ–¹æ³•è§ä¹‹å‰çš„æ•´ç†</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># scgptä¼ªä»£ç ç¤ºä¾‹</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn.flash_attention <span style="color:#fff;font-weight:bold">import</span> FlashAttention
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># __init__</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">self</span>.self_attn = FlashAttention(attention_dropout=attention_dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># forward</span>
</span></span><span style="display:flex;"><span>pcpt_context, pcpt_attn_weights = <span style="color:#fff;font-weight:bold">self</span>.self_attn(
</span></span><span style="display:flex;"><span>    pcpt_qkv,
</span></span><span style="display:flex;"><span>    key_padding_mask=pcpt_key_padding_mask,
</span></span><span style="display:flex;"><span>    need_weights=need_weights,
</span></span><span style="display:flex;"><span>    causal=<span style="color:#fff;font-weight:bold">self</span>.causal, <span style="color:#007f7f"># If true, autoregressive modeling</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>æœ¬æ¬¡ä¸»è¦å­¦ä¹ FlashAttention-2ç‰ˆæœ¬ ï¼ˆwith Better Parallelism and Work Partitioningï¼‰ã€‚å®‰è£…è¿‡ç¨‹è¸©äº†å¾ˆå¤šå‘ï¼Œç›®å‰æ‰¾åˆ°ä¸€ç§å¯ä¿¡çš„æ–¹å¼ã€‚</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conda create -n flash python=<span style="color:#ff0;font-weight:bold">3.10</span> mamba -y
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># CUDA 11.8</span>
</span></span><span style="display:flex;"><span>mamba install cudatoolkit==<span style="color:#ff0;font-weight:bold">11.8</span> -c nvidia
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch 2.3.0</span>
</span></span><span style="display:flex;"><span>mamba install pytorch==<span style="color:#ff0;font-weight:bold">2.3.0</span> torchvision==<span style="color:#ff0;font-weight:bold">0.18.0</span> torchaudio==<span style="color:#ff0;font-weight:bold">2.3.0</span> pytorch-cuda=<span style="color:#ff0;font-weight:bold">11.8</span> -c pytorch -c nvidia
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># nvcc</span>
</span></span><span style="display:flex;"><span>conda install nvidia/label/cuda-<span style="color:#ff0;font-weight:bold">11.8.0</span>::cuda-nvcc
</span></span><span style="display:flex;"><span>nvcc -V
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ä¸‹è½½æºæ–‡ä»¶ï¼Œæœ¬åœ°å®‰è£…ï¼šhttps://github.com/Dao-AILab/flash-attention/releases</span>
</span></span><span style="display:flex;"><span>pip install <span style="color:#0ff;font-weight:bold">&#39;flash_attn-2.6.3+cu118torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>éæœ¬åœ°å®‰è£…æ–¹å¼ä¸ºï¼š</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pip install ninja <span style="color:#007f7f">#åŠ é€Ÿbuildå®‰è£…è¿›ç¨‹ï¼Œå®æµ‹å‘ç°åªå¯¹ä¸‹é¢çš„cloneå®‰è£…æœ‰ä½œç”¨</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pip install flash-attn --no-build-isolation
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># or clone github repo</span>
</span></span><span style="display:flex;"><span>python setup.py install
</span></span></code></pre></td></tr></table>
</div>
</div><p>ç”±äºæ˜¯åœ¨condaç¯å¢ƒä¸‹å®‰è£…çš„cudaï¼Œæ‰€ä»¥åœ¨ä¸Šè¿°è¿‡ç¨‹ä¸­ä¼šå‡ºç°ç±»ä¼¼<code>cuda_runtime_api.h: No such file or directory</code>çš„æŠ¥é”™ã€‚æŸ¥äº†å¾ˆå¤šæ•™ç¨‹ï¼Œæ¯”è¾ƒé è°±çš„æ–¹æ³•æ˜¯å®‰è£…<code>cudatoolkit-dev</code>ã€‚è¿™åŒæ—¶å¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼Œç›®å‰å…¶æœ€é«˜ç‰ˆæœ¬ä¸º11.7ï¼Œéœ€è¦å®‰è£…ä¸ä¹‹å¯¹åº”çš„cudaç¯å¢ƒï¼Œä»¥åŠtorchç­‰</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mamba install -c conda-forge cudatoolkit-dev
</span></span><span style="display:flex;"><span>mamba install cudatoolkit==<span style="color:#ff0;font-weight:bold">11.7</span> -c nvidia
</span></span><span style="display:flex;"><span>mamba install pytorch==<span style="color:#ff0;font-weight:bold">2.0.1</span> torchvision==<span style="color:#ff0;font-weight:bold">0.15.2</span> torchaudio==<span style="color:#ff0;font-weight:bold">2.0.2</span> pytorch-cuda=<span style="color:#ff0;font-weight:bold">11.7</span> -c pytorch -c nvidia
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>python setup.py install
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="22-ä½¿ç”¨">2.2 ä½¿ç”¨<a hidden class="anchor" aria-hidden="true" href="#22-ä½¿ç”¨">#</a></h2>
<ul>
<li>è¾“å…¥ç»´åº¦è¦æ±‚ä¸€èˆ¬æ˜¯ï¼š(batch_size, seqlen, nheads, headdim)</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn <span style="color:#fff;font-weight:bold">import</span> flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash_attn_funcå¸¸è§„è®¡ç®—</span>
</span></span><span style="display:flex;"><span>o2 = flash_attn_func(q, k, v)
</span></span><span style="display:flex;"><span>o2.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash_attn_qkvpacked_funcæ‰“åŒ…è®¡ç®—</span>
</span></span><span style="display:flex;"><span>qkv_pack = torch.concat([q.unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>), k.unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>), v.unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>)], dim=<span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># (batch_size, seqlen, 3, nheads, headdim)</span>
</span></span><span style="display:flex;"><span>qkv_pack.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 3, 8, 64])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>flash_attn_qkvpacked_func(qkv_pack).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="3-fscaled_dot_product_attention">3. F.scaled_dot_product_attention<a hidden class="anchor" aria-hidden="true" href="#3-fscaled_dot_product_attention">#</a></h1>
<ul>
<li><code>torch.nn.functional.scaled_dot_product_attention</code>æ˜¯torch2.0ç‰ˆæœ¬æ›´æ–°åï¼Œæ–°å¢çš„æ³¨æ„åŠ›åŠ é€Ÿè®¡ç®—æ–¹æ³•
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a></li>
<li><a href="https://pytorch.apachecn.org/2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/">https://pytorch.apachecn.org/2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/</a></li>
</ul>
</li>
<li>å…¶é‡‡ç”¨äº†åŒ…æ‹¬Flash-attention2åœ¨å†…çš„ä¸‰ç§åŠ é€Ÿç®—æ³•
<ul>
<li><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
<li><a href="https://github.com/facebookresearch/xformers">Memory-Efficient Attention</a></li>
<li>A PyTorch implementation defined in C++ matching the above formulation</li>
</ul>
</li>
<li>All implementations are enabled by default. Scaled dot product attention attempts to <strong>automatically select the most optimal</strong> implementation based on the inputs</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch.nn.functional <span style="color:#fff;font-weight:bold">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># è¾“å…¥è¦æ±‚(batch_size, nheads, seqlen, headdim)</span>
</span></span><span style="display:flex;"><span>q3 = q.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>) <span style="color:#007f7f"># torch.Size([32, 8, 100, 64])</span>
</span></span><span style="display:flex;"><span>k3 = k.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>v3 = v.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>o3 = F.scaled_dot_product_attention(q3, k3, v3)
</span></span><span style="display:flex;"><span>o3.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="4-benchmark">4. Benchmark<a hidden class="anchor" aria-hidden="true" href="#4-benchmark">#</a></h1>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> time
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> einops <span style="color:#fff;font-weight:bold">import</span> rearrange
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch.nn.functional <span style="color:#fff;font-weight:bold">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn <span style="color:#fff;font-weight:bold">import</span> flash_attn_func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> custom_attention(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    score = torch.matmul(q, k.transpose(-<span style="color:#ff0;font-weight:bold">2</span>, -<span style="color:#ff0;font-weight:bold">1</span>)) / math.sqrt(q.size(-<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> causal:
</span></span><span style="display:flex;"><span>        mask = torch.triu(torch.ones(score.shape[-<span style="color:#ff0;font-weight:bold">2</span>], score.shape[-<span style="color:#ff0;font-weight:bold">1</span>]), diagonal=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        mask = mask.masked_fill(mask==<span style="color:#ff0;font-weight:bold">1</span>, torch.finfo(q.dtype).min)
</span></span><span style="display:flex;"><span>        mask = mask.to(q.device, q.dtype)
</span></span><span style="display:flex;"><span>        score = score + mask
</span></span><span style="display:flex;"><span>    attn = F.softmax(score, dim=-<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>    o = torch.matmul(attn, v)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> pytorch_func(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    o = F.scaled_dot_product_attention(q, k, v, is_causal=causal)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># o = F.scaled_dot_product_attention(q, k, v, is_causal=causal)[0]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> flash_attention(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    o = flash_attn_func(q, k, v, causal=causal)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>å®šä¹‰å‡½æ•°ï¼Œæµ‹è¯•æ³¨æ„åŠ›è®¡ç®—çš„æ—¶é—´ä»¥åŠæ˜¾å­˜æ¶ˆè€—</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> test(func_name, q, k, v, *args, **kwargs):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> func_name in [<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, <span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>]:
</span></span><span style="display:flex;"><span>        q = rearrange(q, <span style="color:#0ff;font-weight:bold">&#34;a b c d -&gt; a c b d&#34;</span>)
</span></span><span style="display:flex;"><span>        k = rearrange(k, <span style="color:#0ff;font-weight:bold">&#34;a b c d -&gt; a c b d&#34;</span>)
</span></span><span style="display:flex;"><span>        v = rearrange(v, <span style="color:#0ff;font-weight:bold">&#34;a b c d -&gt; a c b d&#34;</span>)
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    torch.cuda.reset_peak_memory_stats() <span style="color:#007f7f"># é‡ç½® CUDA å†…å­˜ç»Ÿè®¡ä¿¡æ¯</span>
</span></span><span style="display:flex;"><span>    torch.cuda.synchronize()             <span style="color:#007f7f"># ç¡®ä¿æ‰€æœ‰ CUDA æ“ä½œå®Œæˆåå†ç»§ç»­</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># globals():å­—å…¸ï¼ŒåŒ…å«äº†å½“å‰ä½œç”¨åŸŸå†…çš„æ‰€æœ‰å…¨å±€å˜é‡å’Œå‡½æ•°</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> _ in <span style="color:#fff;font-weight:bold">range</span>(<span style="color:#ff0;font-weight:bold">5</span>):
</span></span><span style="display:flex;"><span>        o = <span style="color:#fff;font-weight:bold">globals</span>()[func_name](q, k, v, *args, **kwargs)
</span></span><span style="display:flex;"><span>    torch.cuda.synchronize()
</span></span><span style="display:flex;"><span>    st = time.time()
</span></span><span style="display:flex;"><span>    o = <span style="color:#fff;font-weight:bold">globals</span>()[func_name](q, k, v, *args, **kwargs)
</span></span><span style="display:flex;"><span>    torch.cuda.synchronize()
</span></span><span style="display:flex;"><span>    tt = time.time() - st
</span></span><span style="display:flex;"><span>    max_memory = torch.cuda.max_memory_allocated() // <span style="color:#ff0;font-weight:bold">2</span>**<span style="color:#ff0;font-weight:bold">20</span>  <span style="color:#007f7f">#å•ä½MB</span>
</span></span><span style="display:flex;"><span>    torch.cuda.empty_cache() <span style="color:#007f7f"># æ¸…é™¤æœªä½¿ç”¨çš„å†…å­˜ï¼ˆé‡Šæ”¾é‚£äº›å·²è¢«åˆ é™¤ä½†æœªé‡Šæ”¾çš„å†…å­˜ï¼‰</span>
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> func_name in [<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, <span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>]:
</span></span><span style="display:flex;"><span>        o = rearrange(o, <span style="color:#0ff;font-weight:bold">&#34;a c b d -&gt; a b c d&#34;</span>)
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o, tt, max_memory
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>æµ‹è¯•<strong>ä¸åŒåºåˆ—é•¿åº¦</strong>ï¼Œä¸‰ç§è®¡ç®—æ—¶é—´ä»¥åŠæ˜¾å­˜æ¶ˆè€—æƒ…å†µ
<ul>
<li>ï¼ˆ1ï¼‰åºåˆ—çš„é•¿åº¦è¶Šé•¿æ—¶ï¼Œpytorch funcä¸flash attentionè®¡ç®—ä¼˜åŠ¿è¶Šæ˜æ˜¾</li>
<li>ï¼ˆ2ï¼‰pytorch funcä¸flash attentionçš„å·®è·ä¸å¤ªæ˜æ˜¾ã€‚</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> seqlen in [<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">1024</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;## Sequence length: </span><span style="color:#0ff;font-weight:bold">{</span>seqlen<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span>    q, k, v = sample_input(<span style="color:#ff0;font-weight:bold">32</span>, seqlen, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">64</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (1)</span>
</span></span><span style="display:flex;"><span>    o, t, m = test(<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;custom pytorch time: </span><span style="color:#0ff;font-weight:bold">{</span>t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (2)</span>
</span></span><span style="display:flex;"><span>    pf_o, pf_t, pf_m = test(<span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;pytorch func time: </span><span style="color:#0ff;font-weight:bold">{</span>pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>pf_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-pf_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, pf_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (3)</span>
</span></span><span style="display:flex;"><span>    fa_o, fa_t, fa_m = test(<span style="color:#0ff;font-weight:bold">&#34;flash_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;flash attention time: </span><span style="color:#0ff;font-weight:bold">{</span>fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>fa_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-fa_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, fa_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 256</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.000259, peak memory: 216 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000058, speedup: 4.44; peak memory: 120 MB, save: 44%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000073, speedup: 3.54; peak memory: 96 MB, save: 55%</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 512</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.001135, peak memory: 384 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000142, speedup: 7.98; peak memory: 120 MB, save: 68%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000154, speedup: 7.38; peak memory: 128 MB, save: 66%</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 1024</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.004096, peak memory: 1272 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000478, speedup: 8.58; peak memory: 233 MB, save: 81%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000493, speedup: 8.30; peak memory: 249 MB, save: 80%</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>æµ‹è¯•<strong>ä¸åŒç‰¹å¾é•¿åº¦</strong>ï¼Œä¸‰ç§è®¡ç®—æ—¶é—´ä»¥åŠæ˜¾å­˜æ¶ˆè€—æƒ…å†µ
<ul>
<li>è¯å…ƒçš„ç‰¹å¾ç»´åº¦è¶Šå¤§æ—¶ï¼Œpytorch funcä¸flash attentionè®¡ç®—ä¼˜åŠ¿ä¼šé€æ¸ä¸‹é™ã€‚</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> seqlen in [<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">1024</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;## Sequence length: </span><span style="color:#0ff;font-weight:bold">{</span>seqlen<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span>    q, k, v = sample_input(<span style="color:#ff0;font-weight:bold">32</span>, seqlen, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">64</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (1)</span>
</span></span><span style="display:flex;"><span>    o, t, m = test(<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;custom pytorch time: </span><span style="color:#0ff;font-weight:bold">{</span>t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (2)</span>
</span></span><span style="display:flex;"><span>    pf_o, pf_t, pf_m = test(<span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;pytorch func time: </span><span style="color:#0ff;font-weight:bold">{</span>pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>pf_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-pf_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, pf_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (3)</span>
</span></span><span style="display:flex;"><span>    fa_o, fa_t, fa_m = test(<span style="color:#0ff;font-weight:bold">&#34;flash_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;flash attention time: </span><span style="color:#0ff;font-weight:bold">{</span>fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>fa_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-fa_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, fa_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 32</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.003847, peak memory: 1128 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000276, speedup: 13.92; peak memory: 105 MB, save: 90%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000286, speedup: 13.45; peak memory: 121 MB, save: 89%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 64</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.004104, peak memory: 1272 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000485, speedup: 8.45; peak memory: 233 MB, save: 81%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000500, speedup: 8.21; peak memory: 249 MB, save: 80%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 128</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.004594, peak memory: 1512 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000952, speedup: 4.82; peak memory: 457 MB, save: 69%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000956, speedup: 4.81; peak memory: 489 MB, save: 67%</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="5-nnmultiheadattentionåŠmaskæ“ä½œ">5. nn.MultiheadAttentionåŠmaskæ“ä½œ<a hidden class="anchor" aria-hidden="true" href="#5-nnmultiheadattentionåŠmaskæ“ä½œ">#</a></h1>
<p><code>torch.nn.MultiheadAttention</code></p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></p>
<ul>
<li>torch å®ç°çš„æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›å±‚ç±»ï¼ŒåŒ…å«å®Œæ•´è¾“å…¥ä¸è¾“å‡ºçš„æƒé‡å‚æ•°</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch.nn <span style="color:#fff;font-weight:bold">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query = key = value = torch.randn(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">512</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># å®ä¾‹åŒ–</span>
</span></span><span style="display:flex;"><span>mha = nn.MultiheadAttention(embed_dim=<span style="color:#ff0;font-weight:bold">512</span>, num_heads=<span style="color:#ff0;font-weight:bold">8</span>, 
</span></span><span style="display:flex;"><span>                            batch_first=<span style="color:#fff;font-weight:bold">True</span>, dropout=<span style="color:#ff0;font-weight:bold">0.0</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># forwardå‰å‘è®¡ç®—</span>
</span></span><span style="display:flex;"><span>out_put, attn_weight = mha(query, key, value, need_weights = <span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>out_put.shape  <span style="color:#007f7f"># torch.Size([4, 10, 512])</span>
</span></span><span style="display:flex;"><span>attn_weight.shape  <span style="color:#007f7f"># torch.Size([4, 10, 10])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># æŸ¥çœ‹æ³¨æ„åŠ›å±‚å‚æ•°</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> name, param in mha.named_parameters():
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;Name: </span><span style="color:#0ff;font-weight:bold">{</span>name<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;Shape: </span><span style="color:#0ff;font-weight:bold">{</span>param.shape<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: in_proj_weight</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([1536, 512])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: in_proj_bias</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([1536])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: out_proj.weight</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([512, 512])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: out_proj.bias</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([512])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>é»˜è®¤need_weights = Trueï¼Œå³è¿”å›å¤šå¤´æ³¨æ„åŠ›çŸ©é˜µè®¡ç®—ç»“æœã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œuse the optimized <code>scaled_dot_product_attention</code> and achieve the best performance for MHA.</p></blockquote>
<p><strong>å‰å‘è®¡ç®—çš„maskç›¸å…³å‚æ•°</strong></p>
<p><strong>key_padding_mask</strong>ï¼šç”¨äºæ ‡è®°ä¸€ä¸ªåºåˆ—ä¸­çš„padå¡«å……å­—ç¬¦ï¼Œä½¿å¾—queryä¸ä¼šå…³æ³¨ä¸å…¶çš„æ³¨æ„åŠ›ã€‚</p>
<ul>
<li>å…¶shapeé€šå¸¸æ˜¯ (Batch_size, Seq_len)ã€‚</li>
<li>Trueè¡¨ç¤ºæ˜¯padå¡«å……å­—ç¬¦</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>key_padding_mask = torch.tensor([[<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>],
</span></span><span style="display:flex;"><span>                                 [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>],
</span></span><span style="display:flex;"><span>                                 [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>],
</span></span><span style="display:flex;"><span>                                 [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>]]).bool() <span style="color:#007f7f">#(4, 10)</span>
</span></span><span style="display:flex;"><span>out_put, _ = mha(query, key, value, need_weights = <span style="color:#fff;font-weight:bold">False</span>,
</span></span><span style="display:flex;"><span>                 key_padding_mask=key_padding_mask)
</span></span><span style="display:flex;"><span>out_put[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.3662, -0.3266, -0.2634],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.0777, -0.1854, -0.0766],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.2575, -0.0160, -0.1000]], grad_fn=&lt;SliceBackward0&gt;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>attn_mask</strong>ï¼šæ³¨æ„åŠ›æ©ç çŸ©é˜µï¼Œç›´æ¥å¯¹ç‰¹å®šçš„query-keyç»„åˆè¿›è¡Œæ©ç æ“ä½œ</p>
<ul>
<li>å…¶shapeé€šå¸¸æ˜¯(Batch_size*Num_head, Seq_len, Seq_len)</li>
<li>Trueè¡¨ç¤ºç‰¹å®šæ³¨æ„åŠ›è¢«æ©ç </li>
<li>ä¸key_padding_maskæœ€ç»ˆmaskçš„æ•ˆæœæ˜¯ç›¸åŒçš„ã€‚å¦‚ä¸‹æ¼”ç¤ºå¦‚ä½•å°†key_padding_maskè½¬æ¢ä¸ºattn_maskï¼Œå¾—åˆ°ä¸€è‡´çš„è¾“å‡ºç»“æœã€‚</li>
<li><a href="https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion">https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion</a></li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attn_mask = key_padding_mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>attn_mask = attn_mask.expand(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">10</span>)
</span></span><span style="display:flex;"><span>attn_mask = attn_mask.reshape(<span style="color:#ff0;font-weight:bold">4</span> * <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">10</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 10, 10])</span>
</span></span><span style="display:flex;"><span>out_put, _ = mha(query, key, value, need_weights = <span style="color:#fff;font-weight:bold">False</span>,
</span></span><span style="display:flex;"><span>              attn_mask=attn_mask)
</span></span><span style="display:flex;"><span>out_put[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.3662, -0.3266, -0.2634],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.0777, -0.1854, -0.0766],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.2575, -0.0160, -0.1000]], grad_fn=&lt;SliceBackward0&gt;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>å…³äºä¸Šè¿°key_padding_maskçš„è½¬æ¢æœ‰å¦‚ä¸‹å€¼å¾—æ³¨æ„çš„ç»†èŠ‚ã€‚</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mask = torch.randn(<span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">4</span>)   <span style="color:#007f7f"># (Batch_size, Seq_len), num_head = 2</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">## ç›´æ¥repeatä¸æ˜¯æ‰€æœŸæœ›çš„ç»“æœ</span>
</span></span><span style="display:flex;"><span>mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).repeat(<span style="color:#ff0;font-weight:bold">2</span>,<span style="color:#ff0;font-weight:bold">1</span>,<span style="color:#ff0;font-weight:bold">1</span>)  <span style="color:#007f7f"># repeat all batch as whole and cannot match the multi-head</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">## å¦‚ä¸‹ä¸¤ç§æ–¹å¼å‡æ˜¯ç¬¦åˆé¢„æœŸçš„è½¬æ¢</span>
</span></span><span style="display:flex;"><span>mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>).expand(<span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">4</span>).reshape(<span style="color:#ff0;font-weight:bold">6</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">4</span>) <span style="color:#007f7f"># repeat each seq two multi-head (expected)</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).repeat_interleave(repeats=<span style="color:#ff0;font-weight:bold">2</span>, dim=<span style="color:#ff0;font-weight:bold">0</span>)  <span style="color:#007f7f"># repeat each seq two multi-head (expected)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h1 id="6-flash-attn-v2-maskæ“ä½œ">6. Flash-Attn V2 maskæ“ä½œ<a hidden class="anchor" aria-hidden="true" href="#6-flash-attn-v2-maskæ“ä½œ">#</a></h1>
<ul>
<li>åœ¨ flash_attn v1ä¸­çš„ï¼Œ <code>FlashAttention</code>æ³¨æ„åŠ›è®¡ç®—æ˜¯æ”¯æŒ<code>key_padding_mask</code>å‚æ•°çš„ï¼›åŒæ—¶ä¹Ÿæä¾›äº†å°è£…å¥½çš„ <code>FlashMHA</code>æ³¨æ„åŠ›å±‚ï¼ˆåŒ…æ‹¬æƒé‡å¯å­¦ä¹ å‚æ•°ï¼‰ã€‚
<ul>
<li>ï¼<code>False</code>ä»£è¡¨å¡«å……å­—ç¬¦ï¼Œè¿™ä¸ä¸Šé¢çš„MultiheadAttentionç›¸å</li>
</ul>
</li>
<li>åœ¨ flash_attn v2ä¸­ï¼Œflash_attn_funcæœ¬èº«æ˜¯ä»…ç”¨äºè®¡ç®—æ³¨æ„åŠ›è¿‡ç¨‹ï¼Œæ²¡æœ‰maskæ“ä½œã€‚</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn <span style="color:#fff;font-weight:bold">import</span> flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>help(flash_attn_func)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>åœ¨torch v2å¼•å…¥çš„<code>F.scaled_dot_product_attention</code>å¼•å…¥äº†Flash-Attn v2ï¼Œå…·ä½“å‚çœ‹ä¸Šé¢ç¬¬ä¸‰ç‚¹çš„ä»‹ç»ã€‚å€¼å¾—æ³¨æ„æ˜¯ï¼Œå®ƒæ˜¯æ”¯æŒ<code>attn_mask</code>å‚æ•°çš„ã€‚</li>
</ul>
<blockquote>
<p><strong>attn_mask</strong> (<em>optional Tensor</em>) â€“ Attention mask; shape must be <strong>broadcastable</strong> to the shape of attention weights, which is (N,&hellip;,L,S)(<em>N</em>,&hellip;,<em>L</em>,<em>S</em>). Two types of masks are supported. A boolean mask where a value of <strong>True</strong> indicates that the element <em>should</em> take part in attention. A float mask of the same type as query, key, value that is added to the attention score.</p></blockquote>
<p>å¦‚ä¸Šå‚æ•°è¯´æ˜æœ‰ä¸¤ä¸ªæ³¨æ„ç‚¹</p>
<p>ï¼ˆ1ï¼‰å…³äºshapeï¼Œä¸å¼ºåˆ¶è¦æ±‚æ˜¯(Batch_size*Num_head, Seq_len, Seq_len)ã€‚broadcastableä¹Ÿå¯ä»¥ï¼›</p>
<p>ï¼ˆ2ï¼‰ä¸Flash-Attnä¸€æ ·ï¼ŒFalseè¡¨ç¤ºå¡«å……/è¢«æ©ç </p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># æ–¹å¼1</span>
</span></span><span style="display:flex;"><span>attn_mask = key_padding_mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>attn_mask.shape <span style="color:#007f7f"># torch.Size([4, 1, 10])</span>
</span></span><span style="display:flex;"><span>output = F.scaled_dot_product_attention(
</span></span><span style="display:flex;"><span>        query, key, value, attn_mask=attn_mask
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>output[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.7855, -0.2025, -0.5377],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.1681, -2.8067,  0.0794],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [-0.1817,  0.0185,  0.1878]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># æ–¹å¼2</span>
</span></span><span style="display:flex;"><span>attn_mask = key_padding_mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).repeat(<span style="color:#ff0;font-weight:bold">1</span>,<span style="color:#ff0;font-weight:bold">10</span>,<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>attn_mask.shape <span style="color:#007f7f"># torch.Size([4, 10, 10])</span>
</span></span><span style="display:flex;"><span>output = F.scaled_dot_product_attention(
</span></span><span style="display:flex;"><span>        query, key, value, attn_mask=attn_mask
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>output[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.7855, -0.2025, -0.5377],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.1681, -2.8067,  0.0794],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [-0.1817,  0.0185,  0.1878]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>æ³¨æ„ç‚¹ï¼š</p>
<p>ï¼ˆ1ï¼‰ä¸Šè¿°ä»…ä¸ºå•å¤´æ³¨æ„åŠ›è®¡ç®—ï¼›</p>
<p>ï¼ˆ2ï¼‰F.scaled_dot_product_attentionä»…æä¾›æ³¨æ„åŠ›è®¡ç®—ï¼Œä¸èƒ½ä½œä¸ºå®Œæ•´çš„æ³¨æ„åŠ›å±‚ï¼ˆç¼ºå°‘æƒé‡å‚æ•°ï¼‰</p>
<p>æ ¸å¿ƒï¼šattn_maskä¸key_padding_maskå‚æ•°å¹¶æ²¡æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚</p></blockquote>
<p><strong>è¡¥å……~</strong></p>
<hr>
<h1 id="sp1-performer">SP1. Performer<a hidden class="anchor" aria-hidden="true" href="#sp1-performer">#</a></h1>
<ul>
<li><a href="https://arxiv.org/pdf/2009.14794v4">https://arxiv.org/pdf/2009.14794v4</a> 2020 Google Research</li>
<li><a href="https://github.com/lucidrains/performer-pytorch">https://github.com/lucidrains/performer-pytorch</a> Pytorchç‰ˆæœ¬å®ç°</li>
</ul>
<p>Performeræ³¨æ„åŠ›ï¼šå°†å¤æ‚åº¦é™ä½åˆ°çº¿æ€§ï¼Œä½¿å¾—å®ƒåœ¨å¤„ç†é•¿åºåˆ—æ—¶æ›´åŠ é«˜æ•ˆï¼›å¹¶ä»ç†è®ºè§’åº¦è¯æ˜æ˜¯å¯è¡Œçš„ã€‚</p>
<p>å¯¹äºé•¿åº¦ä¸º<strong>L</strong>çš„è¾“å…¥åºåˆ—ï¼ŒåµŒå…¥å‘é‡é•¿åº¦æ˜¯<strong>d</strong></p>
<p>è®¡ç®—å¸¸è§„Transfomerè‡ªæ³¨æ„åŠ›æ—¶ (ä¸‹å›¾å·¦æ )ï¼Œå…¶å¤æ‚åº¦ä¸ºO(<strong>L2</strong>) â†’ Quadratic;</p>
<ul>
<li>(1) æ³¨æ„åŠ›çŸ©é˜µï¼šQ * (K)T = (L, d) * (d, L) = (L, L) â†’ O(L2 * d) â†’ O(L2)</li>
<li>(2) Softmaxè®¡ç®—å½’ä¸€åŒ–æƒé‡ï¼š(L, L) = (L, L) â†’ O(L2)</li>
<li>(3) åŠ æƒå’Œè¡¨ç¤ºï¼šQ * (K)T * V = (L, L) * (L, d) = (L, d) â†’ O(L2 * d) â†’ O(L2)</li>
</ul>
<p>Performeræ³¨æ„åŠ›è®¡ç®—çš„æ³¨æ„åŠ›è®¡ç®— (ä¸‹å›¾å³æ ) å¤æ‚åº¦ä¸ºO(<strong>L</strong>) â†’ Linear</p>
<ul>
<li>(1) é¦–å…ˆå¯¹Qä¸Kè¿›è¡Œéšæœºç‰¹å¾æ˜ å°„ï¼ˆRandom Feature Mappingï¼‰
<ul>
<li>Qï¼š(L, d) â†’ Q&rsquo; (L, r),   Kï¼š(L, d) â†’ K&rsquo; (L, r)</li>
</ul>
</li>
<li>(2) ç„¶åè®¡ç®— (K&rsquo;)T * V = (r, L) * (L, d) = (r, d) â†’ O(Lrd) â†’ O(L)</li>
<li>(3) æœ€åè®¡ç®— Q&rsquo; * (K&rsquo;)T * V = (L, r) * (r, d) = (L, d) â†’ O(Lrd) â†’ O(L)</li>
</ul>
<blockquote>
<p>è®¡ç®—å¤æ‚åº¦æ—¶ï¼Œå¯å¿½ç•¥å¸¸æ•°é¡¹ã€ä½é˜¶é¡¹ï¼Œä»¥åŠç³»æ•°ã€‚</p></blockquote>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20241108151812052.png" alt="image-20241108151812052"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lishensuo.github.io/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">æ·±åº¦å­¦ä¹ </a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lishensuo.github.io/en/posts/bioinfo/724%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%84config%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE/">
    <span class="title">Â« Prev Page</span>
    <br>
    <span>æ·±åº¦å­¦ä¹ å¸¸è§„Configè®­ç»ƒé…ç½®</span>
  </a>
  <a class="next" href="https://lishensuo.github.io/en/posts/bioinfo/726%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97%E6%8C%87%E6%A0%87log%E8%AE%B0%E5%BD%95/">
    <span class="title">Next Page Â»</span>
    <br>
    <span>æœºå™¨å­¦ä¹ æ—¥å¿—æŒ‡æ ‡logè®°å½•</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://lishensuo.github.io/en/">Li&#39;s Bioinfo-Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
		<br/>æ‚¨æ˜¯æœ¬ç«™ç¬¬ <span id="busuanzi_value_site_uv"></span> ä½è®¿é—®è€…ï¼Œæ€»æµè§ˆé‡ä¸º <span id="busuanzi_value_site_pv"></span> æ¬¡
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script type="text/javascript"
async
src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\[\[','\]\]']],
processEscapes: true,
processEnvironments: true,
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
TeX: { equationNumbers: { autoNumber: "AMS" },
extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});

MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>

<style>
code.has-jax {
font: inherit;
font-size: 100%;
background: inherit;
border: inherit;
color: #515151;
}
</style></body>
</html>

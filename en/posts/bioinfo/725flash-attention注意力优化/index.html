<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">

<link rel="icon" href="/favicon.ico" type="image/x-icon"> 
<title>Flash Attention注意力优化 | Li&#39;s Bioinfo-Blog</title>
<meta name="keywords" content="深度学习">
<meta name="description" content="注意力计算

注意力计算的三要素分别是：Query， Key，Value。而在自注意力计算中，三者则是等价的。
结合如下图示例：一个序列有2个词元，每个词元有3个特征 ,即输入为(2, 3)

每个Query词元会计算与其它词元Key的“相似度”（包括自己），再经过softmax（每行的和等于1）转换，得到 2 × 2 权重矩阵
然后将其与Value矩阵进行乘法运算(2, 2) × (2, 3)，得到新的(2, 3)输出结果

形象理解：对于词元A的输出特征1，等于输入词元A, B的特征的加权和。




多头注意力：本质上可以理解为将特征维度分成多个部分，每个部分称为一个“头”。每个头独立进行注意力计算，然后将所有头的输出合并在一起；以期学习不同的关系和模式。


">
<meta name="author" content="Lishensuo">
<link rel="canonical" href="https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.9e4de5e3ba61ea358168341aa7cdf70abfaafb7c697dfe8624af3ddff9a35c2f.css" integrity="sha256-nk3l47ph6jWBaDQap833Cr&#43;q&#43;3xpff6GJK893/mjXC8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js" integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://lishensuo.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://lishensuo.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://lishensuo.github.io/Q.gif">
<link rel="mask-icon" href="https://lishensuo.github.io/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Flash Attention注意力优化" />
<meta property="og:description" content="注意力计算

注意力计算的三要素分别是：Query， Key，Value。而在自注意力计算中，三者则是等价的。
结合如下图示例：一个序列有2个词元，每个词元有3个特征 ,即输入为(2, 3)

每个Query词元会计算与其它词元Key的“相似度”（包括自己），再经过softmax（每行的和等于1）转换，得到 2 × 2 权重矩阵
然后将其与Value矩阵进行乘法运算(2, 2) × (2, 3)，得到新的(2, 3)输出结果

形象理解：对于词元A的输出特征1，等于输入词元A, B的特征的加权和。




多头注意力：本质上可以理解为将特征维度分成多个部分，每个部分称为一个“头”。每个头独立进行注意力计算，然后将所有头的输出合并在一起；以期学习不同的关系和模式。


" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-10-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Flash Attention注意力优化"/>
<meta name="twitter:description" content="注意力计算

注意力计算的三要素分别是：Query， Key，Value。而在自注意力计算中，三者则是等价的。
结合如下图示例：一个序列有2个词元，每个词元有3个特征 ,即输入为(2, 3)

每个Query词元会计算与其它词元Key的“相似度”（包括自己），再经过softmax（每行的和等于1）转换，得到 2 × 2 权重矩阵
然后将其与Value矩阵进行乘法运算(2, 2) × (2, 3)，得到新的(2, 3)输出结果

形象理解：对于词元A的输出特征1，等于输入词元A, B的特征的加权和。




多头注意力：本质上可以理解为将特征维度分成多个部分，每个部分称为一个“头”。每个头独立进行注意力计算，然后将所有头的输出合并在一起；以期学习不同的关系和模式。


"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "分类",
      "item": "https://lishensuo.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📖 生信数据分析--分析流程，工具包等",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Flash Attention注意力优化",
      "item": "https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Flash Attention注意力优化",
  "name": "Flash Attention注意力优化",
  "description": "注意力计算\n注意力计算的三要素分别是：Query， Key，Value。而在自注意力计算中，三者则是等价的。 结合如下图示例：一个序列有2个词元，每个词元有3个特征 ,即输入为(2, 3) 每个Query词元会计算与其它词元Key的“相似度”（包括自己），再经过softmax（每行的和等于1）转换，得到 2 × 2 权重矩阵 然后将其与Value矩阵进行乘法运算(2, 2) × (2, 3)，得到新的(2, 3)输出结果 形象理解：对于词元A的输出特征1，等于输入词元A, B的特征的加权和。 多头注意力：本质上可以理解为将特征维度分成多个部分，每个部分称为一个“头”。每个头独立进行注意力计算，然后将所有头的输出合并在一起；以期学习不同的关系和模式。 ",
  "keywords": [
    "深度学习"
  ],
  "articleBody": "注意力计算\n注意力计算的三要素分别是：Query， Key，Value。而在自注意力计算中，三者则是等价的。 结合如下图示例：一个序列有2个词元，每个词元有3个特征 ,即输入为(2, 3) 每个Query词元会计算与其它词元Key的“相似度”（包括自己），再经过softmax（每行的和等于1）转换，得到 2 × 2 权重矩阵 然后将其与Value矩阵进行乘法运算(2, 2) × (2, 3)，得到新的(2, 3)输出结果 形象理解：对于词元A的输出特征1，等于输入词元A, B的特征的加权和。 多头注意力：本质上可以理解为将特征维度分成多个部分，每个部分称为一个“头”。每个头独立进行注意力计算，然后将所有头的输出合并在一起；以期学习不同的关系和模式。 注意力计算本身不涉及可学习参数。一般在input前，out后，各设置一层MLP线性变换。 参考：https://blog.csdn.net/God_WeiYang/article/details/131820781\n0. 模拟数据 通常情况下，注意力计算的输入数据拥有四个维度：(batch_size, num_heads, seq_length, head_dim)\nword embed = num_heads × head_dim\n1 2 3 4 5 6 7 8 9 10 11 12 # 注意与上面顺序不太相同 def sample_input(bach_size, seq_length, n_head, n_dim): q = torch.randn((bach_size, seq_length, n_head, n_dim)).to(\"cuda:0\", torch.float16) k = torch.rand_like(q) v = torch.rand_like(q) return q, k, v q, k, v = sample_input(32, 100, 8, 64) q.shape, k.shape, v.shape # (torch.Size([32, 100, 8, 64]), # torch.Size([32, 100, 8, 64]), # torch.Size([32, 100, 8, 64])) 最终词元的特征长度为：num_heads × head_dim\n1. 手动Attention计算 causal参数表示注意力掩码操作，常用于GPT生成模型中。表示计算序列中第n个词元时，只关注第1到n-1个（除了本身）； 具体通过torch.finfo(q.dtype).min设置为负无穷，则其softmax转换后的权重值为0。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def custom_attention(q, k, v, causal=False): score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1)) if causal: mask = torch.triu(torch.ones(score.shape[-2], score.shape[-1]), diagonal=1) mask = mask.masked_fill(mask==1, torch.finfo(q.dtype).min) mask = mask.to(q.device, q.dtype) score = score + mask attn = F.softmax(score, dim=-1) o = torch.matmul(attn, v) return o q1 = q.transpose(1, 2) # torch.Size([32, 8, 100, 64]) k1 = k.transpose(1, 2) v1 = v.transpose(1, 2) o1 = custom_attention(q1, k1, v1) o1.transpose(2, 3).shape # torch.Size([32, 100, 8, 64]) 2. Flash Attention 1/2 Flash Attention 一种高效的注意力计算方法，旨在优化 Transformer 模型中的注意力机制。它通过减少内存使用和提高计算速度来处理长序列输入。 https://github.com/Dao-AILab/flash-attention 2.1 安装 之前学习的scGPT主要使用了FlashAttention-1，安装方法见之前的整理 1 2 3 4 5 6 7 8 9 10 11 12 13 # scgpt伪代码示例 from flash_attn.flash_attention import FlashAttention # __init__ self.self_attn = FlashAttention(attention_dropout=attention_dropout) # forward pcpt_context, pcpt_attn_weights = self.self_attn( pcpt_qkv, key_padding_mask=pcpt_key_padding_mask, need_weights=need_weights, causal=self.causal, # If true, autoregressive modeling ) 本次主要学习FlashAttention-2版本 （with Better Parallelism and Work Partitioning）。安装过程踩了很多坑，目前找到一种可信的方式。 1 2 3 4 5 6 7 8 9 10 conda create -n flash python=3.10 mamba -y # CUDA 11.8 mamba install cudatoolkit==11.8 -c nvidia # torch 2.3.0 mamba install pytorch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 pytorch-cuda=11.8 -c pytorch -c nvidia # nvcc conda install nvidia/label/cuda-11.8.0::cuda-nvcc nvcc -V # 下载源文件，本地安装：https://github.com/Dao-AILab/flash-attention/releases pip install 'flash_attn-2.6.3+cu118torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl' 非本地安装方式为：\n1 2 3 4 5 pip install ninja #加速build安装进程，实测发现只对下面的clone安装有作用 pip install flash-attn --no-build-isolation # or clone github repo python setup.py install 由于是在conda环境下安装的cuda，所以在上述过程中会出现类似cuda_runtime_api.h: No such file or directory的报错。查了很多教程，比较靠谱的方法是安装cudatoolkit-dev。这同时带来一个问题，目前其最高版本为11.7，需要安装与之对应的cuda环境，以及torch等\n1 2 3 4 5 mamba install -c conda-forge cudatoolkit-dev mamba install cudatoolkit==11.7 -c nvidia mamba install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia python setup.py install 2.2 使用 输入维度要求一般是：(batch_size, seqlen, nheads, headdim) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from flash_attn import flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func # flash_attn_func常规计算 o2 = flash_attn_func(q, k, v) o2.shape # torch.Size([32, 100, 8, 64]) # flash_attn_qkvpacked_func打包计算 qkv_pack = torch.concat([q.unsqueeze(2), k.unsqueeze(2), v.unsqueeze(2)], dim=2) # (batch_size, seqlen, 3, nheads, headdim) qkv_pack.shape # torch.Size([32, 100, 3, 8, 64]) flash_attn_qkvpacked_func(qkv_pack).shape # torch.Size([32, 100, 8, 64]) 3. F.scaled_dot_product_attention torch.nn.functional.scaled_dot_product_attention是torch2.0版本更新后，新增的注意力加速计算方法 https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html https://pytorch.apachecn.org/2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/ 其采用了包括Flash-attention2在内的三种加速算法 FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning Memory-Efficient Attention A PyTorch implementation defined in C++ matching the above formulation All implementations are enabled by default. Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs 1 2 3 4 5 6 7 8 9 10 import torch.nn.functional as F # 输入要求(batch_size, nheads, seqlen, headdim) q3 = q.transpose(1, 2) # torch.Size([32, 8, 100, 64]) k3 = k.transpose(1, 2) v3 = v.transpose(1, 2) o3 = F.scaled_dot_product_attention(q3, k3, v3) o3.transpose(1, 2).shape # torch.Size([32, 100, 8, 64]) 4. Benchmark 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import math import time from einops import rearrange import torch import torch.nn.functional as F from flash_attn import flash_attn_func def custom_attention(q, k, v, causal=False): score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1)) if causal: mask = torch.triu(torch.ones(score.shape[-2], score.shape[-1]), diagonal=1) mask = mask.masked_fill(mask==1, torch.finfo(q.dtype).min) mask = mask.to(q.device, q.dtype) score = score + mask attn = F.softmax(score, dim=-1) o = torch.matmul(attn, v) return o def pytorch_func(q, k, v, causal=False): o = F.scaled_dot_product_attention(q, k, v, is_causal=causal) # o = F.scaled_dot_product_attention(q, k, v, is_causal=causal)[0] return o def flash_attention(q, k, v, causal=False): o = flash_attn_func(q, k, v, causal=causal) return o 定义函数，测试注意力计算的时间以及显存消耗 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def test(func_name, q, k, v, *args, **kwargs): if func_name in [\"custom_attention\", \"pytorch_func\"]: q = rearrange(q, \"a b c d -\u003e a c b d\") k = rearrange(k, \"a b c d -\u003e a c b d\") v = rearrange(v, \"a b c d -\u003e a c b d\") torch.cuda.reset_peak_memory_stats() # 重置 CUDA 内存统计信息 torch.cuda.synchronize() # 确保所有 CUDA 操作完成后再继续 # globals():字典，包含了当前作用域内的所有全局变量和函数 for _ in range(5): o = globals()[func_name](q, k, v, *args, **kwargs) torch.cuda.synchronize() st = time.time() o = globals()[func_name](q, k, v, *args, **kwargs) torch.cuda.synchronize() tt = time.time() - st max_memory = torch.cuda.max_memory_allocated() // 2**20 #单位MB torch.cuda.empty_cache() # 清除未使用的内存（释放那些已被删除但未释放的内存） if func_name in [\"custom_attention\", \"pytorch_func\"]: o = rearrange(o, \"a c b d -\u003e a b c d\") return o, tt, max_memory 测试不同序列长度，三种计算时间以及显存消耗情况 （1）序列的长度越长时，pytorch func与flash attention计算优势越明显 （2）pytorch func与flash attention的差距不太明显。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 for seqlen in [256, 512, 1024]: print(f\"## Sequence length: {seqlen}\") q, k, v = sample_input(32, seqlen, 8, 64) ## (1) o, t, m = test(\"custom_attention\", q, k, v, causal=False) print(f\"custom pytorch time: {t:.6f}, peak memory: {m} MB\") ## (2) pf_o, pf_t, pf_m = test(\"pytorch_func\", q, k, v, causal=False) print(f\"pytorch func time: {pf_t:.6f}, speedup: {t/pf_t:.2f}; peak memory: {pf_m} MB, save: {int((m-pf_m)/m*100)}%\") assert torch.allclose(o, pf_o, rtol=1e-2, atol=1e-2) ## (3) fa_o, fa_t, fa_m = test(\"flash_attention\", q, k, v, causal=False) print(f\"flash attention time: {fa_t:.6f}, speedup: {t/fa_t:.2f}; peak memory: {fa_m} MB, save: {int((m-fa_m)/m*100)}%\") assert torch.allclose(o, fa_o, rtol=1e-2, atol=1e-2) # ## Sequence length: 256 # custom pytorch time: 0.000259, peak memory: 216 MB # pytorch func time: 0.000058, speedup: 4.44; peak memory: 120 MB, save: 44% # flash attention time: 0.000073, speedup: 3.54; peak memory: 96 MB, save: 55% # ## Sequence length: 512 # custom pytorch time: 0.001135, peak memory: 384 MB # pytorch func time: 0.000142, speedup: 7.98; peak memory: 120 MB, save: 68% # flash attention time: 0.000154, speedup: 7.38; peak memory: 128 MB, save: 66% # ## Sequence length: 1024 # custom pytorch time: 0.004096, peak memory: 1272 MB # pytorch func time: 0.000478, speedup: 8.58; peak memory: 233 MB, save: 81% # flash attention time: 0.000493, speedup: 8.30; peak memory: 249 MB, save: 80% 测试不同特征长度，三种计算时间以及显存消耗情况 词元的特征维度越大时，pytorch func与flash attention计算优势会逐渐下降。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 for seqlen in [256, 512, 1024]: print(f\"## Sequence length: {seqlen}\") q, k, v = sample_input(32, seqlen, 8, 64) ## (1) o, t, m = test(\"custom_attention\", q, k, v, causal=False) print(f\"custom pytorch time: {t:.6f}, peak memory: {m} MB\") ## (2) pf_o, pf_t, pf_m = test(\"pytorch_func\", q, k, v, causal=False) print(f\"pytorch func time: {pf_t:.6f}, speedup: {t/pf_t:.2f}; peak memory: {pf_m} MB, save: {int((m-pf_m)/m*100)}%\") assert torch.allclose(o, pf_o, rtol=1e-2, atol=1e-2) ## (3) fa_o, fa_t, fa_m = test(\"flash_attention\", q, k, v, causal=False) print(f\"flash attention time: {fa_t:.6f}, speedup: {t/fa_t:.2f}; peak memory: {fa_m} MB, save: {int((m-fa_m)/m*100)}%\") assert torch.allclose(o, fa_o, rtol=1e-2, atol=1e-2) # ## Sequence length: 32 # custom pytorch time: 0.003847, peak memory: 1128 MB # pytorch func time: 0.000276, speedup: 13.92; peak memory: 105 MB, save: 90% # flash attention time: 0.000286, speedup: 13.45; peak memory: 121 MB, save: 89% # ## Sequence length: 64 # custom pytorch time: 0.004104, peak memory: 1272 MB # pytorch func time: 0.000485, speedup: 8.45; peak memory: 233 MB, save: 81% # flash attention time: 0.000500, speedup: 8.21; peak memory: 249 MB, save: 80% # ## Sequence length: 128 # custom pytorch time: 0.004594, peak memory: 1512 MB # pytorch func time: 0.000952, speedup: 4.82; peak memory: 457 MB, save: 69% # flash attention time: 0.000956, speedup: 4.81; peak memory: 489 MB, save: 67% 5. nn.MultiheadAttention及mask操作 torch.nn.MultiheadAttention\nhttps://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\ntorch 实现的标准多头注意力层类，包含完整输入与输出的权重参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn query = key = value = torch.randn(4, 10, 512) # 实例化 mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True, dropout=0.0) # forward前向计算 out_put, attn_weight = mha(query, key, value, need_weights = True) out_put.shape # torch.Size([4, 10, 512]) attn_weight.shape # torch.Size([4, 10, 10]) # 查看注意力层参数 for name, param in mha.named_parameters(): print(f\"Name: {name}\") print(f\"Shape: {param.shape}\\n\") # Name: in_proj_weight # Shape: torch.Size([1536, 512]) # Name: in_proj_bias # Shape: torch.Size([1536]) # Name: out_proj.weight # Shape: torch.Size([512, 512]) # Name: out_proj.bias # Shape: torch.Size([512]) 默认need_weights = True，即返回多头注意力矩阵计算结果。如果设置为False，use the optimized scaled_dot_product_attention and achieve the best performance for MHA.\n前向计算的mask相关参数\nkey_padding_mask：用于标记一个序列中的pad填充字符，使得query不会关注与其的注意力。\n其shape通常是 (Batch_size, Seq_len)。 True表示是pad填充字符 1 2 3 4 5 6 7 8 9 10 key_padding_mask = torch.tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]).bool() #(4, 10) out_put, _ = mha(query, key, value, need_weights = False, key_padding_mask=key_padding_mask) out_put[0,:3,:3] # tensor([[ 0.3662, -0.3266, -0.2634], # [ 0.0777, -0.1854, -0.0766], # [ 0.2575, -0.0160, -0.1000]], grad_fn=) attn_mask：注意力掩码矩阵，直接对特定的query-key组合进行掩码操作\n其shape通常是(Batch_size*Num_head, Seq_len, Seq_len) True表示特定注意力被掩码 与key_padding_mask最终mask的效果是相同的。如下演示如何将key_padding_mask转换为attn_mask，得到一致的输出结果。 https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion 1 2 3 4 5 6 7 8 9 10 attn_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) attn_mask = attn_mask.expand(4, 8, 10, 10) attn_mask = attn_mask.reshape(4 * 8, 10, 10) # torch.Size([32, 10, 10]) out_put, _ = mha(query, key, value, need_weights = False, attn_mask=attn_mask) out_put[0,:3,:3] # tensor([[ 0.3662, -0.3266, -0.2634], # [ 0.0777, -0.1854, -0.0766], # [ 0.2575, -0.0160, -0.1000]], grad_fn=) 关于上述key_padding_mask的转换有如下值得注意的细节。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mask = torch.randn(3, 4) # (Batch_size, Seq_len), num_head = 2 ## 直接repeat不是所期望的结果 mask.unsqueeze(1).repeat(2,1,1) # repeat all batch as whole and cannot match the multi-head # tensor([[[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]], # [[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]]]) ## 如下两种方式均是符合预期的转换 mask.unsqueeze(1).unsqueeze(2).expand(3, 2, 1, 4).reshape(6, 1, 4) # repeat each seq two multi-head (expected) # tensor([[[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[ 0.5375, 1.3711, -0.3028, 1.0184]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[-0.1414, 1.3067, -0.0363, 0.8482]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]], # [[ 0.3573, -0.9005, -0.3998, -0.8608]]]) mask.unsqueeze(1).repeat_interleave(repeats=2, dim=0) # repeat each seq two multi-head (expected) 6. Flash-Attn V2 mask操作 在 flash_attn v1中的， FlashAttention注意力计算是支持key_padding_mask参数的；同时也提供了封装好的 FlashMHA注意力层（包括权重可学习参数）。 ！False代表填充字符，这与上面的MultiheadAttention相反 在 flash_attn v2中，flash_attn_func本身是仅用于计算注意力过程，没有mask操作。 1 2 3 from flash_attn import flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func help(flash_attn_func) 在torch v2引入的F.scaled_dot_product_attention引入了Flash-Attn v2，具体参看上面第三点的介绍。值得注意是，它是支持attn_mask参数的。 attn_mask (optional Tensor) – Attention mask; shape must be broadcastable to the shape of attention weights, which is (N,…,L,S)(N,…,L,S). Two types of masks are supported. A boolean mask where a value of True indicates that the element should take part in attention. A float mask of the same type as query, key, value that is added to the attention score.\n如上参数说明有两个注意点\n（1）关于shape，不强制要求是(Batch_size*Num_head, Seq_len, Seq_len)。broadcastable也可以；\n（2）与Flash-Attn一样，False表示填充/被掩码\n1 2 3 4 5 6 7 8 9 10 # 方式1 attn_mask = key_padding_mask.unsqueeze(1) attn_mask.shape # torch.Size([4, 1, 10]) output = F.scaled_dot_product_attention( query, key, value, attn_mask=attn_mask ) output[0,:3,:3] # tensor([[ 0.7855, -0.2025, -0.5377], # [ 0.1681, -2.8067, 0.0794], # [-0.1817, 0.0185, 0.1878]]) 1 2 3 4 5 6 7 8 9 10 # 方式2 attn_mask = key_padding_mask.unsqueeze(1).repeat(1,10,1) attn_mask.shape # torch.Size([4, 10, 10]) output = F.scaled_dot_product_attention( query, key, value, attn_mask=attn_mask ) output[0,:3,:3] # tensor([[ 0.7855, -0.2025, -0.5377], # [ 0.1681, -2.8067, 0.0794], # [-0.1817, 0.0185, 0.1878]]) 注意点：\n（1）上述仅为单头注意力计算；\n（2）F.scaled_dot_product_attention仅提供注意力计算，不能作为完整的注意力层（缺少权重参数）\n核心：attn_mask与key_padding_mask参数并没有本质的区别。\n补充~\nSP1. Performer https://arxiv.org/pdf/2009.14794v4 2020 Google Research https://github.com/lucidrains/performer-pytorch Pytorch版本实现 Performer注意力：将复杂度降低到线性，使得它在处理长序列时更加高效；并从理论角度证明是可行的。\n对于长度为L的输入序列，嵌入向量长度是d\n计算常规Transfomer自注意力时 (下图左栏)，其复杂度为O(L2) → Quadratic;\n(1) 注意力矩阵：Q * (K)T = (L, d) * (d, L) = (L, L) → O(L2 * d) → O(L2) (2) Softmax计算归一化权重：(L, L) = (L, L) → O(L2) (3) 加权和表示：Q * (K)T * V = (L, L) * (L, d) = (L, d) → O(L2 * d) → O(L2) Performer注意力计算的注意力计算 (下图右栏) 复杂度为O(L) → Linear\n(1) 首先对Q与K进行随机特征映射（Random Feature Mapping） Q：(L, d) → Q’ (L, r), K：(L, d) → K’ (L, r) (2) 然后计算 (K’)T * V = (r, L) * (L, d) = (r, d) → O(Lrd) → O(L) (3) 最后计算 Q’ * (K’)T * V = (L, r) * (r, d) = (L, d) → O(Lrd) → O(L) 计算复杂度时，可忽略常数项、低阶项，以及系数。\n",
  "wordCount" : "4575",
  "inLanguage": "en",
  "datePublished": "2024-10-28T00:00:00Z",
  "dateModified": "2024-10-28T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Lishensuo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lishensuo.github.io/en/posts/bioinfo/725flash-attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Li's Bioinfo-Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lishensuo.github.io/img/Q.gif"
    }
  }
}
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lishensuo.github.io/en/" accesskey="h" title="Li&#39;s Bioinfo-Blog (Alt + H)">Li&#39;s Bioinfo-Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lishensuo.github.io/en/" title="主页">
                    <span>主页</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/posts" title="分类">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/tags" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/about" title="关于">
                    <span>关于</span>
                </a>
            </li>
            <li>
                <a href="https://lishensuo.github.io/en/search" title="搜索 (Alt &#43; /)" accesskey=/>
                    <span>搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lishensuo.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/">分类</a>&nbsp;»&nbsp;<a href="https://lishensuo.github.io/en/posts/bioinfo/">📖 生信数据分析--分析流程，工具包等</a></div>
    <h1 class="post-title">
      Flash Attention注意力优化
    </h1>
    <div class="post-meta">













Create:&amp;nbsp;&lt;span title=&#39;2024-10-28 00:00:00 &#43;0000 UTC&#39;&gt;2024-10-28&lt;/span&gt;&amp;nbsp;|&amp;nbsp;Update:&amp;nbsp;2024-10-28&amp;nbsp;|&amp;nbsp;Words:&amp;nbsp;4575&amp;nbsp;|&amp;nbsp;10 min&amp;nbsp;|&amp;nbsp;Lishensuo

|  Viewers: <span id="busuanzi_value_page_pv"></span> 
	  
    </div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#0-%e6%a8%a1%e6%8b%9f%e6%95%b0%e6%8d%ae" aria-label="0. 模拟数据">0. 模拟数据</a></li>
                    <li>
                        <a href="#1-%e6%89%8b%e5%8a%a8attention%e8%ae%a1%e7%ae%97" aria-label="1. 手动Attention计算">1. 手动Attention计算</a></li>
                    <li>
                        <a href="#2-flash-attention-12" aria-label="2. Flash Attention 1/2">2. Flash Attention 1/2</a><ul>
                            
                    <li>
                        <a href="#21-%e5%ae%89%e8%a3%85" aria-label="2.1 安装">2.1 安装</a></li>
                    <li>
                        <a href="#22-%e4%bd%bf%e7%94%a8" aria-label="2.2 使用">2.2 使用</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-fscaled_dot_product_attention" aria-label="3. F.scaled_dot_product_attention">3. F.scaled_dot_product_attention</a></li>
                    <li>
                        <a href="#4-benchmark" aria-label="4. Benchmark">4. Benchmark</a></li>
                    <li>
                        <a href="#5-nnmultiheadattention%e5%8f%8amask%e6%93%8d%e4%bd%9c" aria-label="5. nn.MultiheadAttention及mask操作">5. nn.MultiheadAttention及mask操作</a></li>
                    <li>
                        <a href="#6-flash-attn-v2-mask%e6%93%8d%e4%bd%9c" aria-label="6. Flash-Attn V2 mask操作">6. Flash-Attn V2 mask操作</a></li>
                    <li>
                        <a href="#sp1-performer" aria-label="SP1. Performer">SP1. Performer</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>


  <div class="post-content"><p><strong>注意力计算</strong></p>
<ul>
<li><strong>注意力</strong>计算的三要素分别是：Query， Key，Value。而在<strong>自注意力</strong>计算中，三者则是等价的。</li>
<li>结合如下图示例：一个序列有2个词元，每个词元有3个特征 ,即输入为(2, 3)
<ul>
<li>每个Query词元会计算与其它词元Key的“相似度”（包括自己），再经过softmax（每行的和等于1）转换，得到 2 × 2 权重矩阵</li>
<li>然后将其与Value矩阵进行乘法运算(2, 2) × (2, 3)，得到新的(2, 3)输出结果
<ul>
<li>形象理解：对于词元A的输出特征1，等于输入词元A, B的特征的加权和。</li>
</ul>
</li>
</ul>
</li>
<li><strong>多头注意力</strong>：本质上可以理解为将特征维度分成多个部分，每个部分称为一个“头”。每个头独立进行注意力计算，然后将所有头的输出合并在一起；以期学习不同的关系和模式。</li>
</ul>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20241027123614983.png" alt="image-20241027123614983"  />
</p>
<ul>
<li>注意力计算本身不涉及可学习参数。一般在input前，out后，各设置一层MLP线性变换。</li>
</ul>
<blockquote>
<p>参考：https://blog.csdn.net/God_WeiYang/article/details/131820781</p></blockquote>
<h1 id="0-模拟数据">0. 模拟数据<a hidden class="anchor" aria-hidden="true" href="#0-模拟数据">#</a></h1>
<ul>
<li>
<p>通常情况下，注意力计算的输入数据拥有四个维度：(batch_size, num_heads, seq_length, head_dim)</p>
<blockquote>
<p>word embed = num_heads × head_dim</p></blockquote>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 注意与上面顺序不太相同</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> sample_input(bach_size, seq_length, n_head, n_dim):
</span></span><span style="display:flex;"><span>    q = torch.randn((bach_size, seq_length, n_head, n_dim)).to(<span style="color:#0ff;font-weight:bold">&#34;cuda:0&#34;</span>, torch.float16)
</span></span><span style="display:flex;"><span>    k = torch.rand_like(q)
</span></span><span style="display:flex;"><span>    v = torch.rand_like(q)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> q, k, v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>q, k, v = sample_input(<span style="color:#ff0;font-weight:bold">32</span>, <span style="color:#ff0;font-weight:bold">100</span>, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">64</span>)
</span></span><span style="display:flex;"><span>q.shape, k.shape, v.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># (torch.Size([32, 100, 8, 64]),</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#  torch.Size([32, 100, 8, 64]),</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#  torch.Size([32, 100, 8, 64]))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>最终词元的特征长度为：num_heads  × head_dim</p></blockquote>
<h1 id="1-手动attention计算">1. 手动Attention计算<a hidden class="anchor" aria-hidden="true" href="#1-手动attention计算">#</a></h1>
<ul>
<li>causal参数表示注意力掩码操作，常用于GPT生成模型中。表示计算序列中第n个词元时，只关注第1到n-1个（除了本身）；</li>
<li>具体通过<code>torch.finfo(q.dtype).min</code>设置为负无穷，则其softmax转换后的权重值为0。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> custom_attention(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    score = torch.matmul(q, k.transpose(-<span style="color:#ff0;font-weight:bold">2</span>, -<span style="color:#ff0;font-weight:bold">1</span>)) / math.sqrt(q.size(-<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> causal:
</span></span><span style="display:flex;"><span>        mask = torch.triu(torch.ones(score.shape[-<span style="color:#ff0;font-weight:bold">2</span>], score.shape[-<span style="color:#ff0;font-weight:bold">1</span>]), diagonal=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        mask = mask.masked_fill(mask==<span style="color:#ff0;font-weight:bold">1</span>, torch.finfo(q.dtype).min)
</span></span><span style="display:flex;"><span>        mask = mask.to(q.device, q.dtype)
</span></span><span style="display:flex;"><span>        score = score + mask
</span></span><span style="display:flex;"><span>    attn = F.softmax(score, dim=-<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>    o = torch.matmul(attn, v)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>q1 = q.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>) <span style="color:#007f7f"># torch.Size([32, 8, 100, 64])</span>
</span></span><span style="display:flex;"><span>k1 = k.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>v1 = v.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>o1 = custom_attention(q1, k1, v1)
</span></span><span style="display:flex;"><span>o1.transpose(<span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">3</span>).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="2-flash-attention-12">2. Flash Attention 1/2<a hidden class="anchor" aria-hidden="true" href="#2-flash-attention-12">#</a></h1>
<ul>
<li>Flash Attention 一种高效的注意力计算方法，旨在优化 Transformer 模型中的注意力机制。它通过减<strong>少内存</strong>使用和<strong>提高计算速度</strong>来处理长序列输入。</li>
<li><a href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a></li>
</ul>
<h2 id="21-安装">2.1 安装<a hidden class="anchor" aria-hidden="true" href="#21-安装">#</a></h2>
<ul>
<li>之前学习的scGPT主要使用了FlashAttention-1，安装方法见之前的整理</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># scgpt伪代码示例</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn.flash_attention <span style="color:#fff;font-weight:bold">import</span> FlashAttention
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># __init__</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">self</span>.self_attn = FlashAttention(attention_dropout=attention_dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># forward</span>
</span></span><span style="display:flex;"><span>pcpt_context, pcpt_attn_weights = <span style="color:#fff;font-weight:bold">self</span>.self_attn(
</span></span><span style="display:flex;"><span>    pcpt_qkv,
</span></span><span style="display:flex;"><span>    key_padding_mask=pcpt_key_padding_mask,
</span></span><span style="display:flex;"><span>    need_weights=need_weights,
</span></span><span style="display:flex;"><span>    causal=<span style="color:#fff;font-weight:bold">self</span>.causal, <span style="color:#007f7f"># If true, autoregressive modeling</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>本次主要学习FlashAttention-2版本 （with Better Parallelism and Work Partitioning）。安装过程踩了很多坑，目前找到一种可信的方式。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conda create -n flash python=<span style="color:#ff0;font-weight:bold">3.10</span> mamba -y
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># CUDA 11.8</span>
</span></span><span style="display:flex;"><span>mamba install cudatoolkit==<span style="color:#ff0;font-weight:bold">11.8</span> -c nvidia
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch 2.3.0</span>
</span></span><span style="display:flex;"><span>mamba install pytorch==<span style="color:#ff0;font-weight:bold">2.3.0</span> torchvision==<span style="color:#ff0;font-weight:bold">0.18.0</span> torchaudio==<span style="color:#ff0;font-weight:bold">2.3.0</span> pytorch-cuda=<span style="color:#ff0;font-weight:bold">11.8</span> -c pytorch -c nvidia
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># nvcc</span>
</span></span><span style="display:flex;"><span>conda install nvidia/label/cuda-<span style="color:#ff0;font-weight:bold">11.8.0</span>::cuda-nvcc
</span></span><span style="display:flex;"><span>nvcc -V
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 下载源文件，本地安装：https://github.com/Dao-AILab/flash-attention/releases</span>
</span></span><span style="display:flex;"><span>pip install <span style="color:#0ff;font-weight:bold">&#39;flash_attn-2.6.3+cu118torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>非本地安装方式为：</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pip install ninja <span style="color:#007f7f">#加速build安装进程，实测发现只对下面的clone安装有作用</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pip install flash-attn --no-build-isolation
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># or clone github repo</span>
</span></span><span style="display:flex;"><span>python setup.py install
</span></span></code></pre></td></tr></table>
</div>
</div><p>由于是在conda环境下安装的cuda，所以在上述过程中会出现类似<code>cuda_runtime_api.h: No such file or directory</code>的报错。查了很多教程，比较靠谱的方法是安装<code>cudatoolkit-dev</code>。这同时带来一个问题，目前其最高版本为11.7，需要安装与之对应的cuda环境，以及torch等</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mamba install -c conda-forge cudatoolkit-dev
</span></span><span style="display:flex;"><span>mamba install cudatoolkit==<span style="color:#ff0;font-weight:bold">11.7</span> -c nvidia
</span></span><span style="display:flex;"><span>mamba install pytorch==<span style="color:#ff0;font-weight:bold">2.0.1</span> torchvision==<span style="color:#ff0;font-weight:bold">0.15.2</span> torchaudio==<span style="color:#ff0;font-weight:bold">2.0.2</span> pytorch-cuda=<span style="color:#ff0;font-weight:bold">11.7</span> -c pytorch -c nvidia
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>python setup.py install
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="22-使用">2.2 使用<a hidden class="anchor" aria-hidden="true" href="#22-使用">#</a></h2>
<ul>
<li>输入维度要求一般是：(batch_size, seqlen, nheads, headdim)</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn <span style="color:#fff;font-weight:bold">import</span> flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash_attn_func常规计算</span>
</span></span><span style="display:flex;"><span>o2 = flash_attn_func(q, k, v)
</span></span><span style="display:flex;"><span>o2.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash_attn_qkvpacked_func打包计算</span>
</span></span><span style="display:flex;"><span>qkv_pack = torch.concat([q.unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>), k.unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>), v.unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>)], dim=<span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># (batch_size, seqlen, 3, nheads, headdim)</span>
</span></span><span style="display:flex;"><span>qkv_pack.shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 3, 8, 64])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>flash_attn_qkvpacked_func(qkv_pack).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="3-fscaled_dot_product_attention">3. F.scaled_dot_product_attention<a hidden class="anchor" aria-hidden="true" href="#3-fscaled_dot_product_attention">#</a></h1>
<ul>
<li><code>torch.nn.functional.scaled_dot_product_attention</code>是torch2.0版本更新后，新增的注意力加速计算方法
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a></li>
<li><a href="https://pytorch.apachecn.org/2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/">https://pytorch.apachecn.org/2.0/tutorials/intermediate/scaled_dot_product_attention_tutorial/</a></li>
</ul>
</li>
<li>其采用了包括Flash-attention2在内的三种加速算法
<ul>
<li><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
<li><a href="https://github.com/facebookresearch/xformers">Memory-Efficient Attention</a></li>
<li>A PyTorch implementation defined in C++ matching the above formulation</li>
</ul>
</li>
<li>All implementations are enabled by default. Scaled dot product attention attempts to <strong>automatically select the most optimal</strong> implementation based on the inputs</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch.nn.functional <span style="color:#fff;font-weight:bold">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 输入要求(batch_size, nheads, seqlen, headdim)</span>
</span></span><span style="display:flex;"><span>q3 = q.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>) <span style="color:#007f7f"># torch.Size([32, 8, 100, 64])</span>
</span></span><span style="display:flex;"><span>k3 = k.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>v3 = v.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>o3 = F.scaled_dot_product_attention(q3, k3, v3)
</span></span><span style="display:flex;"><span>o3.transpose(<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">2</span>).shape
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 100, 8, 64])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="4-benchmark">4. Benchmark<a hidden class="anchor" aria-hidden="true" href="#4-benchmark">#</a></h1>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> time
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> einops <span style="color:#fff;font-weight:bold">import</span> rearrange
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch.nn.functional <span style="color:#fff;font-weight:bold">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn <span style="color:#fff;font-weight:bold">import</span> flash_attn_func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> custom_attention(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    score = torch.matmul(q, k.transpose(-<span style="color:#ff0;font-weight:bold">2</span>, -<span style="color:#ff0;font-weight:bold">1</span>)) / math.sqrt(q.size(-<span style="color:#ff0;font-weight:bold">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> causal:
</span></span><span style="display:flex;"><span>        mask = torch.triu(torch.ones(score.shape[-<span style="color:#ff0;font-weight:bold">2</span>], score.shape[-<span style="color:#ff0;font-weight:bold">1</span>]), diagonal=<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>        mask = mask.masked_fill(mask==<span style="color:#ff0;font-weight:bold">1</span>, torch.finfo(q.dtype).min)
</span></span><span style="display:flex;"><span>        mask = mask.to(q.device, q.dtype)
</span></span><span style="display:flex;"><span>        score = score + mask
</span></span><span style="display:flex;"><span>    attn = F.softmax(score, dim=-<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>    o = torch.matmul(attn, v)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> pytorch_func(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    o = F.scaled_dot_product_attention(q, k, v, is_causal=causal)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># o = F.scaled_dot_product_attention(q, k, v, is_causal=causal)[0]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> flash_attention(q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>):
</span></span><span style="display:flex;"><span>    o = flash_attn_func(q, k, v, causal=causal)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>定义函数，测试注意力计算的时间以及显存消耗</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">def</span> test(func_name, q, k, v, *args, **kwargs):
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> func_name in [<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, <span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>]:
</span></span><span style="display:flex;"><span>        q = rearrange(q, <span style="color:#0ff;font-weight:bold">&#34;a b c d -&gt; a c b d&#34;</span>)
</span></span><span style="display:flex;"><span>        k = rearrange(k, <span style="color:#0ff;font-weight:bold">&#34;a b c d -&gt; a c b d&#34;</span>)
</span></span><span style="display:flex;"><span>        v = rearrange(v, <span style="color:#0ff;font-weight:bold">&#34;a b c d -&gt; a c b d&#34;</span>)
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    torch.cuda.reset_peak_memory_stats() <span style="color:#007f7f"># 重置 CUDA 内存统计信息</span>
</span></span><span style="display:flex;"><span>    torch.cuda.synchronize()             <span style="color:#007f7f"># 确保所有 CUDA 操作完成后再继续</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f"># globals():字典，包含了当前作用域内的所有全局变量和函数</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">for</span> _ in <span style="color:#fff;font-weight:bold">range</span>(<span style="color:#ff0;font-weight:bold">5</span>):
</span></span><span style="display:flex;"><span>        o = <span style="color:#fff;font-weight:bold">globals</span>()[func_name](q, k, v, *args, **kwargs)
</span></span><span style="display:flex;"><span>    torch.cuda.synchronize()
</span></span><span style="display:flex;"><span>    st = time.time()
</span></span><span style="display:flex;"><span>    o = <span style="color:#fff;font-weight:bold">globals</span>()[func_name](q, k, v, *args, **kwargs)
</span></span><span style="display:flex;"><span>    torch.cuda.synchronize()
</span></span><span style="display:flex;"><span>    tt = time.time() - st
</span></span><span style="display:flex;"><span>    max_memory = torch.cuda.max_memory_allocated() // <span style="color:#ff0;font-weight:bold">2</span>**<span style="color:#ff0;font-weight:bold">20</span>  <span style="color:#007f7f">#单位MB</span>
</span></span><span style="display:flex;"><span>    torch.cuda.empty_cache() <span style="color:#007f7f"># 清除未使用的内存（释放那些已被删除但未释放的内存）</span>
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">if</span> func_name in [<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, <span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>]:
</span></span><span style="display:flex;"><span>        o = rearrange(o, <span style="color:#0ff;font-weight:bold">&#34;a c b d -&gt; a b c d&#34;</span>)
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">return</span> o, tt, max_memory
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>测试<strong>不同序列长度</strong>，三种计算时间以及显存消耗情况
<ul>
<li>（1）序列的长度越长时，pytorch func与flash attention计算优势越明显</li>
<li>（2）pytorch func与flash attention的差距不太明显。</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> seqlen in [<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">1024</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;## Sequence length: </span><span style="color:#0ff;font-weight:bold">{</span>seqlen<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span>    q, k, v = sample_input(<span style="color:#ff0;font-weight:bold">32</span>, seqlen, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">64</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (1)</span>
</span></span><span style="display:flex;"><span>    o, t, m = test(<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;custom pytorch time: </span><span style="color:#0ff;font-weight:bold">{</span>t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (2)</span>
</span></span><span style="display:flex;"><span>    pf_o, pf_t, pf_m = test(<span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;pytorch func time: </span><span style="color:#0ff;font-weight:bold">{</span>pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>pf_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-pf_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, pf_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (3)</span>
</span></span><span style="display:flex;"><span>    fa_o, fa_t, fa_m = test(<span style="color:#0ff;font-weight:bold">&#34;flash_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;flash attention time: </span><span style="color:#0ff;font-weight:bold">{</span>fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>fa_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-fa_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, fa_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 256</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.000259, peak memory: 216 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000058, speedup: 4.44; peak memory: 120 MB, save: 44%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000073, speedup: 3.54; peak memory: 96 MB, save: 55%</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 512</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.001135, peak memory: 384 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000142, speedup: 7.98; peak memory: 120 MB, save: 68%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000154, speedup: 7.38; peak memory: 128 MB, save: 66%</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 1024</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.004096, peak memory: 1272 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000478, speedup: 8.58; peak memory: 233 MB, save: 81%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000493, speedup: 8.30; peak memory: 249 MB, save: 80%</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>测试<strong>不同特征长度</strong>，三种计算时间以及显存消耗情况
<ul>
<li>词元的特征维度越大时，pytorch func与flash attention计算优势会逐渐下降。</li>
</ul>
</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> seqlen in [<span style="color:#ff0;font-weight:bold">256</span>, <span style="color:#ff0;font-weight:bold">512</span>, <span style="color:#ff0;font-weight:bold">1024</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;## Sequence length: </span><span style="color:#0ff;font-weight:bold">{</span>seqlen<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span>    q, k, v = sample_input(<span style="color:#ff0;font-weight:bold">32</span>, seqlen, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">64</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (1)</span>
</span></span><span style="display:flex;"><span>    o, t, m = test(<span style="color:#0ff;font-weight:bold">&#34;custom_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;custom pytorch time: </span><span style="color:#0ff;font-weight:bold">{</span>t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (2)</span>
</span></span><span style="display:flex;"><span>    pf_o, pf_t, pf_m = test(<span style="color:#0ff;font-weight:bold">&#34;pytorch_func&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;pytorch func time: </span><span style="color:#0ff;font-weight:bold">{</span>pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/pf_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>pf_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-pf_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, pf_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007f7f">## (3)</span>
</span></span><span style="display:flex;"><span>    fa_o, fa_t, fa_m = test(<span style="color:#0ff;font-weight:bold">&#34;flash_attention&#34;</span>, q, k, v, causal=<span style="color:#fff;font-weight:bold">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;flash attention time: </span><span style="color:#0ff;font-weight:bold">{</span>fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.6f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">, speedup: </span><span style="color:#0ff;font-weight:bold">{</span>t/fa_t<span style="color:#0ff;font-weight:bold">:</span><span style="color:#0ff;font-weight:bold">.2f</span><span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">; peak memory: </span><span style="color:#0ff;font-weight:bold">{</span>fa_m<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold"> MB, save: </span><span style="color:#0ff;font-weight:bold">{</span><span style="color:#fff;font-weight:bold">int</span>((m-fa_m)/m*<span style="color:#ff0;font-weight:bold">100</span>)<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">%&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">assert</span> torch.allclose(o, fa_o, rtol=<span style="color:#ff0;font-weight:bold">1e-2</span>, atol=<span style="color:#ff0;font-weight:bold">1e-2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 32</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.003847, peak memory: 1128 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000276, speedup: 13.92; peak memory: 105 MB, save: 90%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000286, speedup: 13.45; peak memory: 121 MB, save: 89%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 64</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.004104, peak memory: 1272 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000485, speedup: 8.45; peak memory: 233 MB, save: 81%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000500, speedup: 8.21; peak memory: 249 MB, save: 80%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ## Sequence length: 128</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># custom pytorch time: 0.004594, peak memory: 1512 MB</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># pytorch func time: 0.000952, speedup: 4.82; peak memory: 457 MB, save: 69%</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># flash attention time: 0.000956, speedup: 4.81; peak memory: 489 MB, save: 67%</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="5-nnmultiheadattention及mask操作">5. nn.MultiheadAttention及mask操作<a hidden class="anchor" aria-hidden="true" href="#5-nnmultiheadattention及mask操作">#</a></h1>
<p><code>torch.nn.MultiheadAttention</code></p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></p>
<ul>
<li>torch 实现的标准多头注意力层类，包含完整输入与输出的权重参数</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> torch.nn <span style="color:#fff;font-weight:bold">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query = key = value = torch.randn(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">512</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 实例化</span>
</span></span><span style="display:flex;"><span>mha = nn.MultiheadAttention(embed_dim=<span style="color:#ff0;font-weight:bold">512</span>, num_heads=<span style="color:#ff0;font-weight:bold">8</span>, 
</span></span><span style="display:flex;"><span>                            batch_first=<span style="color:#fff;font-weight:bold">True</span>, dropout=<span style="color:#ff0;font-weight:bold">0.0</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># forward前向计算</span>
</span></span><span style="display:flex;"><span>out_put, attn_weight = mha(query, key, value, need_weights = <span style="color:#fff;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>out_put.shape  <span style="color:#007f7f"># torch.Size([4, 10, 512])</span>
</span></span><span style="display:flex;"><span>attn_weight.shape  <span style="color:#007f7f"># torch.Size([4, 10, 10])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 查看注意力层参数</span>
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> name, param in mha.named_parameters():
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;Name: </span><span style="color:#0ff;font-weight:bold">{</span>name<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">print</span>(<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;Shape: </span><span style="color:#0ff;font-weight:bold">{</span>param.shape<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: in_proj_weight</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([1536, 512])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: in_proj_bias</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([1536])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: out_proj.weight</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([512, 512])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Name: out_proj.bias</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Shape: torch.Size([512])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>默认need_weights = True，即返回多头注意力矩阵计算结果。如果设置为False，use the optimized <code>scaled_dot_product_attention</code> and achieve the best performance for MHA.</p></blockquote>
<p><strong>前向计算的mask相关参数</strong></p>
<p><strong>key_padding_mask</strong>：用于标记一个序列中的pad填充字符，使得query不会关注与其的注意力。</p>
<ul>
<li>其shape通常是 (Batch_size, Seq_len)。</li>
<li>True表示是pad填充字符</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>key_padding_mask = torch.tensor([[<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>],
</span></span><span style="display:flex;"><span>                                 [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>],
</span></span><span style="display:flex;"><span>                                 [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>],
</span></span><span style="display:flex;"><span>                                 [<span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">0</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>]]).bool() <span style="color:#007f7f">#(4, 10)</span>
</span></span><span style="display:flex;"><span>out_put, _ = mha(query, key, value, need_weights = <span style="color:#fff;font-weight:bold">False</span>,
</span></span><span style="display:flex;"><span>                 key_padding_mask=key_padding_mask)
</span></span><span style="display:flex;"><span>out_put[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.3662, -0.3266, -0.2634],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.0777, -0.1854, -0.0766],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.2575, -0.0160, -0.1000]], grad_fn=&lt;SliceBackward0&gt;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>attn_mask</strong>：注意力掩码矩阵，直接对特定的query-key组合进行掩码操作</p>
<ul>
<li>其shape通常是(Batch_size*Num_head, Seq_len, Seq_len)</li>
<li>True表示特定注意力被掩码</li>
<li>与key_padding_mask最终mask的效果是相同的。如下演示如何将key_padding_mask转换为attn_mask，得到一致的输出结果。</li>
<li><a href="https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion">https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion</a></li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attn_mask = key_padding_mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>)
</span></span><span style="display:flex;"><span>attn_mask = attn_mask.expand(<span style="color:#ff0;font-weight:bold">4</span>, <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">10</span>)
</span></span><span style="display:flex;"><span>attn_mask = attn_mask.reshape(<span style="color:#ff0;font-weight:bold">4</span> * <span style="color:#ff0;font-weight:bold">8</span>, <span style="color:#ff0;font-weight:bold">10</span>, <span style="color:#ff0;font-weight:bold">10</span>)
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># torch.Size([32, 10, 10])</span>
</span></span><span style="display:flex;"><span>out_put, _ = mha(query, key, value, need_weights = <span style="color:#fff;font-weight:bold">False</span>,
</span></span><span style="display:flex;"><span>              attn_mask=attn_mask)
</span></span><span style="display:flex;"><span>out_put[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.3662, -0.3266, -0.2634],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.0777, -0.1854, -0.0766],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.2575, -0.0160, -0.1000]], grad_fn=&lt;SliceBackward0&gt;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>关于上述key_padding_mask的转换有如下值得注意的细节。</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">20
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mask = torch.randn(<span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">4</span>)   <span style="color:#007f7f"># (Batch_size, Seq_len), num_head = 2</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">## 直接repeat不是所期望的结果</span>
</span></span><span style="display:flex;"><span>mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).repeat(<span style="color:#ff0;font-weight:bold">2</span>,<span style="color:#ff0;font-weight:bold">1</span>,<span style="color:#ff0;font-weight:bold">1</span>)  <span style="color:#007f7f"># repeat all batch as whole and cannot match the multi-head</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">## 如下两种方式均是符合预期的转换</span>
</span></span><span style="display:flex;"><span>mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).unsqueeze(<span style="color:#ff0;font-weight:bold">2</span>).expand(<span style="color:#ff0;font-weight:bold">3</span>, <span style="color:#ff0;font-weight:bold">2</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">4</span>).reshape(<span style="color:#ff0;font-weight:bold">6</span>, <span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">4</span>) <span style="color:#007f7f"># repeat each seq two multi-head (expected)</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.5375,  1.3711, -0.3028,  1.0184]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[-0.1414,  1.3067, -0.0363,  0.8482]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [[ 0.3573, -0.9005, -0.3998, -0.8608]]])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).repeat_interleave(repeats=<span style="color:#ff0;font-weight:bold">2</span>, dim=<span style="color:#ff0;font-weight:bold">0</span>)  <span style="color:#007f7f"># repeat each seq two multi-head (expected)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h1 id="6-flash-attn-v2-mask操作">6. Flash-Attn V2 mask操作<a hidden class="anchor" aria-hidden="true" href="#6-flash-attn-v2-mask操作">#</a></h1>
<ul>
<li>在 flash_attn v1中的， <code>FlashAttention</code>注意力计算是支持<code>key_padding_mask</code>参数的；同时也提供了封装好的 <code>FlashMHA</code>注意力层（包括权重可学习参数）。
<ul>
<li>！<code>False</code>代表填充字符，这与上面的MultiheadAttention相反</li>
</ul>
</li>
<li>在 flash_attn v2中，flash_attn_func本身是仅用于计算注意力过程，没有mask操作。</li>
</ul>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">from</span> flash_attn <span style="color:#fff;font-weight:bold">import</span> flash_attn_func, flash_attn_qkvpacked_func, flash_attn_kvpacked_func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>help(flash_attn_func)
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>在torch v2引入的<code>F.scaled_dot_product_attention</code>引入了Flash-Attn v2，具体参看上面第三点的介绍。值得注意是，它是支持<code>attn_mask</code>参数的。</li>
</ul>
<blockquote>
<p><strong>attn_mask</strong> (<em>optional Tensor</em>) – Attention mask; shape must be <strong>broadcastable</strong> to the shape of attention weights, which is (N,&hellip;,L,S)(<em>N</em>,&hellip;,<em>L</em>,<em>S</em>). Two types of masks are supported. A boolean mask where a value of <strong>True</strong> indicates that the element <em>should</em> take part in attention. A float mask of the same type as query, key, value that is added to the attention score.</p></blockquote>
<p>如上参数说明有两个注意点</p>
<p>（1）关于shape，不强制要求是(Batch_size*Num_head, Seq_len, Seq_len)。broadcastable也可以；</p>
<p>（2）与Flash-Attn一样，False表示填充/被掩码</p>
<div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 方式1</span>
</span></span><span style="display:flex;"><span>attn_mask = key_padding_mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>attn_mask.shape <span style="color:#007f7f"># torch.Size([4, 1, 10])</span>
</span></span><span style="display:flex;"><span>output = F.scaled_dot_product_attention(
</span></span><span style="display:flex;"><span>        query, key, value, attn_mask=attn_mask
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>output[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.7855, -0.2025, -0.5377],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.1681, -2.8067,  0.0794],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [-0.1817,  0.0185,  0.1878]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#007f7f"># 方式2</span>
</span></span><span style="display:flex;"><span>attn_mask = key_padding_mask.unsqueeze(<span style="color:#ff0;font-weight:bold">1</span>).repeat(<span style="color:#ff0;font-weight:bold">1</span>,<span style="color:#ff0;font-weight:bold">10</span>,<span style="color:#ff0;font-weight:bold">1</span>)
</span></span><span style="display:flex;"><span>attn_mask.shape <span style="color:#007f7f"># torch.Size([4, 10, 10])</span>
</span></span><span style="display:flex;"><span>output = F.scaled_dot_product_attention(
</span></span><span style="display:flex;"><span>        query, key, value, attn_mask=attn_mask
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>output[<span style="color:#ff0;font-weight:bold">0</span>,:<span style="color:#ff0;font-weight:bold">3</span>,:<span style="color:#ff0;font-weight:bold">3</span>]
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># tensor([[ 0.7855, -0.2025, -0.5377],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [ 0.1681, -2.8067,  0.0794],</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f">#         [-0.1817,  0.0185,  0.1878]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意点：</p>
<p>（1）上述仅为单头注意力计算；</p>
<p>（2）F.scaled_dot_product_attention仅提供注意力计算，不能作为完整的注意力层（缺少权重参数）</p>
<p>核心：attn_mask与key_padding_mask参数并没有本质的区别。</p></blockquote>
<p><strong>补充~</strong></p>
<hr>
<h1 id="sp1-performer">SP1. Performer<a hidden class="anchor" aria-hidden="true" href="#sp1-performer">#</a></h1>
<ul>
<li><a href="https://arxiv.org/pdf/2009.14794v4">https://arxiv.org/pdf/2009.14794v4</a> 2020 Google Research</li>
<li><a href="https://github.com/lucidrains/performer-pytorch">https://github.com/lucidrains/performer-pytorch</a> Pytorch版本实现</li>
</ul>
<p>Performer注意力：将复杂度降低到线性，使得它在处理长序列时更加高效；并从理论角度证明是可行的。</p>
<p>对于长度为<strong>L</strong>的输入序列，嵌入向量长度是<strong>d</strong></p>
<p>计算常规Transfomer自注意力时 (下图左栏)，其复杂度为O(<strong>L2</strong>) → Quadratic;</p>
<ul>
<li>(1) 注意力矩阵：Q * (K)T = (L, d) * (d, L) = (L, L) → O(L2 * d) → O(L2)</li>
<li>(2) Softmax计算归一化权重：(L, L) = (L, L) → O(L2)</li>
<li>(3) 加权和表示：Q * (K)T * V = (L, L) * (L, d) = (L, d) → O(L2 * d) → O(L2)</li>
</ul>
<p>Performer注意力计算的注意力计算 (下图右栏) 复杂度为O(<strong>L</strong>) → Linear</p>
<ul>
<li>(1) 首先对Q与K进行随机特征映射（Random Feature Mapping）
<ul>
<li>Q：(L, d) → Q&rsquo; (L, r),   K：(L, d) → K&rsquo; (L, r)</li>
</ul>
</li>
<li>(2) 然后计算 (K&rsquo;)T * V = (r, L) * (L, d) = (r, d) → O(Lrd) → O(L)</li>
<li>(3) 最后计算 Q&rsquo; * (K&rsquo;)T * V = (L, r) * (r, d) = (L, d) → O(Lrd) → O(L)</li>
</ul>
<blockquote>
<p>计算复杂度时，可忽略常数项、低阶项，以及系数。</p></blockquote>
<p><img loading="lazy" src="https://raw.githubusercontent.com/lishensuo/images2/main/img01/image-20241108151812052.png" alt="image-20241108151812052"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lishensuo.github.io/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lishensuo.github.io/en/posts/bioinfo/724%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%84config%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE/">
    <span class="title">« Prev Page</span>
    <br>
    <span>深度学习常规Config训练配置</span>
  </a>
  <a class="next" href="https://lishensuo.github.io/en/posts/bioinfo/726%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97%E6%8C%87%E6%A0%87log%E8%AE%B0%E5%BD%95/">
    <span class="title">Next Page »</span>
    <br>
    <span>机器学习日志指标log记录</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://lishensuo.github.io/en/">Li&#39;s Bioinfo-Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
		<br/>您是本站第 <span id="busuanzi_value_site_uv"></span> 位访问者，总浏览量为 <span id="busuanzi_value_site_pv"></span> 次
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script type="text/javascript"
async
src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
displayMath: [['$$','$$'], ['\[\[','\]\]']],
processEscapes: true,
processEnvironments: true,
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
TeX: { equationNumbers: { autoNumber: "AMS" },
extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});

MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>

<style>
code.has-jax {
font: inherit;
font-size: 100%;
background: inherit;
border: inherit;
color: #515151;
}
</style></body>
</html>
